so okay um well everyone welcome to uh 182. maybe I
should go to the other another slide here and it's really nice to be back in person after a long time for me um I
wasn't uh I didn't teach in spring in person so it's really great
so just starting off like what is this course and how do we feel about it I
thought I'd start with kind of acknowledging the most important thing
which is that you know we're trying to study deep learning and like we don't know very much this is a picture of my
son with um his neural networks for babies book um
you know I identify very much with sort of you know his perspective uh on this material
in terms of how I feel I understand it and how we all understand it right
so um one of the things that we're going to talk about in this class that's really
important is what do we do in the context of something that we don't
really understand that well how do we kind of muddle through anyway
so you know so like that's
you can't hear the audio of him talking um you will feel like that many times
and you know want to say bye bye to this material
um but okay so like let's get into like some
like how do I think about what the subject is
okay so I assume by now every single person in
this audience has seen lots of news articles has heard about seeing videos on YouTube
of all the cool things that people have accomplished using deep learning I'm not going to waste your time showing you
things that you can find on uh online and just looking on the news so how do we think about what it is so
fundamentally uh when I think about the subject deep learning is the use of simulated
analog circuit to do information processing where these analog circuits are defined
by lots and lots of parameters and instead of hand tuning those
parameters and figuring out what exact values you want to use you've set those parameters using an
optimization algorithm and that optimization algorithm is driven by data
and that's kind of like what you do in deep learning why do you call it d
they're called Deep because the information start when it comes from the input flows through many many many
many many Circuit elements before I get to the output this is like the circuit perspective on
what deep learning is and it kind of reflects what you do now why are we doing all of this why are
we building these circuits uh simulated and why are we you know having this optimization algorithms at various
parameters of them the reason is we want the learn circuit learn circuit is a circuit after we set
the parameters by the optimization algorithm to approximately
capture whatever are the relevant patterns underlying the data
so that we can use that circuit in interesting ways to solve problems involving new data
new data for which that pattern is presumably relevant so that's what the subject is about
and it's about so a lot of what we're going to talk about in the course is we're going to
talk about different circuit architectures that are useful for different types of problems and that
have different properties so a lot of the main part of the class will be going over then and then details about how do
you deal with that circuit architecture what's involved now there are also general principles that apply to a
different architecture and you can use different architectures for different problems all of that we'll go into later in the course but this is what the
subject is about now there are many many different hypotheses out there for why you need
depth in the circuit um there is the belief that this is for
allowing representation learning there's some evidence for this there's also a belief that this allows for different
kinds of functions to be represented more efficiently there's evidence for that here's the belief that no it's more like
a telescope that where you have lots of different lenses uh to bring the appropriate things into
Focus you can't say the intermediate lens has some appropriate representation it's like you're just trying to bring a
pattern into Focus there's some evidence for that too so we don't really understand a lot of
what's going on so I'm not going to try to claim a particular perspective here as we go through the course we will kind
of engage with these mathematically but you know that's I don't want to claim that we know more than we do
okay so what kind of class is this right
um this is a specialized bus so what I mean by that in the Berkeley
context roll
this is a chart of the Berkeley X curriculum that inikapanu put in you can find it
online on hcam's website so where is this class
this class is here okay
so the path that students are expected to sort of take to get here is
starting the freshman year 16a 16b 70.
learn probability and optimization take a general machine learning class and
then you are here so
why you know why do we do this why do you need all this right hopefully the
description has helped you understand this a little bit why this is relevant
so I'm going to talk a little bit more about the preparation kind of in some detail um so let's just think about the
different topics Sub sub topics of preparation that are needed hopefully people can see
um what I write here tell me if you can't
so as I described the way that the subject works is kind of sitting at the
heart of what's going on operationally is optimization
did people see when I wrote optimization people on that side no okay maybe we can raise the screen up
hope it'll automatically turn off where I don't care if it does or doesn't um
so why you need to know optimization because that's how you train these
things that's how you type these parameters you don't have an intuition and understanding of optimization
you're gonna get lost in the optimization details and lose track of the deep learning detail so we're going
to assume an understanding optimization where do you learn optimization in the Berkeley context now here I'm speaking
mostly to Berkeley undergrads optimization ideas show up in
a 15 B
127. and a bit in 189.
for The Graduate students in the audience who might have learned in other schools uh you know
you can ask on the 282 a uh Ed uh if you're you know what details of your
background we can we can address that what we need to know but you need to know this stuff to
understand so now the next question that comes up is you know what if you don't or what if you don't understand it well
enough for graduate students who have a lot of time to take more classes who can build
up to it who don't need necessarily this material right now I would say take 127 227a
and you'll be ready on when it comes to the optimization material if you don't have time for do that we're
not going to exercise all of the detail uh level in 127 and 227a
the most important stuff we're going to use is actually hit here but it's
so we're going to try to keep things as Elementary as we can but there's times when we're
you're we're going to do say something that you're not going to understand so you have to go fill in that Gap
so when we think about optimization what do we like kind of in that mix
we have like two components of it that are particularly important for us
because they also help us understand um modeling especially in the case of
linear algebra
so linear algebra critical critical background again what are the classes
that hit the relevant level of linear algebra
and there's review here but this is not needed linear algebra here is if for us
why do we need linear algebra it's not just because it's like the
language we use it's also because it allows us the ability to visualize certain things and to invoke geometry
so as we say the Big Challenge in the subject is we don't understand what's going on pretty much
beyond that baby level book um I mean we do but not that much Beyond relative to where we need to be
um so any two thing that helps us engage our intuition is very useful and linear
algebra with its geometry is extremely used to help us engage our intuition
the other thing you need on the optimization side is Vector calculus
foreign calculus engaged with in the Berkeley curriculum
math 53 weeks 16b
and 127.
um we're going to try to keep the vector calculus kind of in terms of how much we have to engage with it uh symbolically
kind of manageable a lot of stuff the computer will do behind the scenes for you but you need to understand
conceptually so this is kind of like in described
this as the optimization grouping of things you need to know
we need to know something else to get to understand the material and that's probability
now what's interesting is that probability didn't show up anywhere in that description that I read to you
about what kind of deep learning is right and that's really interesting right
because it's when you're doing things probability is often playing a role in
the background of helping us think about it so what do you need to know for probability ps70 is like a very basic it's not
nearly enough what you need is
probability in X 126 or
that one that's the background Reuben actually found on
so probability gives us a strategic language in which we can think about uncertainty and uncertainty that is
in particular not out to get us and also it's useful to think about
uncertainty that is out to get us but I don't know how much we'll get into that here and so we're going to need probability
in the context of abstract toy models that help us kind of distill what's trying what's going on in practice when
we're actually doing things there are times we're also going to invoke randomization and things like that but
you're not you don't really need a lot of class level stuff to understand what you learn in courses to understand the
idea of like shuffling things so operationally the probability use might be less
um it also shows up in the computation of certain important things like expectations but we're going to assume you you know
this
you need to know the basics of machine learning generally
why because we're building on it we're a specialized approach machine learning
so one of the courses that hit the relevant ideas
again in a in b
189 or that 154.
yeah 188 no
um good question but no um
me so if you mastered the material here
that plus 188 might do it this is the facility of pot that we're going to count on
but by itself it will and then finally you need to have some
sense of if the only approach to information processing you've seen is by
procedural program right where you have it then right that's kind of the basic thing
that you do you're gonna feel a little bit um lost in terms of what's going on you
have to have at least a basic exposure to analog information processing
since you can pick up pretty fast um some people don't find the challenge some people do kind of really wrestle
with kind of understanding how you can have information processed in an analog way this is definitely covered in
week 16b or many of you have had in a physics class
especially the concept of filtering the key thing is the idea of filter because in a way everything we're going to talk
about is like a giant glorified convoluted filter
right someone can make a meme right of Pokemon evolutions you start with like a
basic RC filter and like later is a deep net um kind of it definitely grows out
so this is the like background that we're gonna count so I'll talk later about how
actually intellectually diverse the field of deep learning is but any one
course can't fully teach it from the fall the diverse
perspectives that one can understand it we're going to try to take advantage of our comparative advantage of sitting in
a top research program and try to engage with it leveraging these tools
there are alternative tinkering ways to engage with the subject which are also valid and provide great Insight that's
not what this class is about we're going to try to help you this
so there's this really great XKCD comic right about machine learning which I
thought is this really really nice in describing how things feel at times so in this picture people can read it
you know it says you know what it feels like if you have a huge stack of linear algebra
which you like shove data into and you shake it until it works and
this is you know to some extent true um of how it will feel we hope the
linear algebra and things will also be supporting some kind of building of intuition but as I said at the beginning
we just don't understand the stuff that well and we're going to be muddling through um anyway
and so at times we're gonna feel
and just match and that is this engineering or is this Alchemy okay
in fact before teaching this class I actually went and I talked to lots of people in various big companies in deep
learning groups and I said you know I'm teaching the class is there any particular thing you want you know
that you would you think would be that benefit people I talked to other faculty as well and you will not believe the number of
times people brought up this the subject feels like Alchemy so let's take knowledge it
right um we're gonna try to convey stuff from a
scientific engineering point of view but I want to you know kind of let's explore this parallel a little bit a
little bit for fun but also just to understand I'm gonna erase this up
um so here I'll write Alchemy
here we'll put deep learning
let's see if we can understand some of these parallels so
Alchemy you are searching for
the elixir of life
something that will grant immortality what is the counterpart in deep learning
it's called artificial general intelligence
a search for a super intelligent artificial entity
it's kind of like occupies the same kind of elixir of life kind of quest for
motivating people what they want to work on Alchemy you search for the Philosopher's
Stone
you know a Mystic object that you can construct that will turn lead into gold in deep learning practice
people look for ways to transmute
unstructured data
into dollars where there's like build deep Learning
System question mark profit um
an Alchemy people try to build try to do things that would let them communicate with the
Dead
uh people try to do exactly this thing foreign
people want to predict the future
in deep learning people try to do exactly the same thing
in Alchemy people search for Rich patrons
to fund their experiments
while promising them various things in deep learning people do exactly the
same thing so just like a bit of humor right but I
want to say something in Praise of alchemy you might not have heard praise it all
right people usually think of alchemy as oh you know horrible stuff that used to happen and how could people like Newton
and so on get mixed up with all this right but I want to say something very much in
praiseville what Alchemy did was it had an aesthetic
the aesthetic was you draw on ideas from anywhere
and you try to make stuff by doing actual experiments
in an era when people human beings are always subject
to the same core problem and that's a group thing and fashion
that you think that what matters is making the other people happy
and Alchemy early on by its Embrace of experimentation and
empiricism actually paved the road for modern science
by telling what matters is what actually happens in the world you can say that something was inspired
by the sun had a battle with a dragon spirit and so you should mix these two
things together but then you had to actually mix the two things together and
do the experiment and deep learning shares that Spirit of open intellectual openness of drawing
ideas from everywhere and not worrying about whether they make sense or not
um which is actually quite powerful and so the question we have is that
Alchemy led to chemistry and modern science
and so what is going to happen here okay
so we're going to take the attitude that you know it's fine
and we're going to embrace a little bit of some of the out Alchemy aesthetic
um which is we'll talk about Theory and we'll talk about general principles but we're also going to tell you the basic
principle that you know if you want to make something work you probably should try what the other person did that kind of worked first and
just see how that does if you don't understand why still try it
right there's an element of um it might sometimes feel like Superstition in deep
learning practice um and it's not entirely not superstition
but when you try things out empirically that's okay
right and so one of the shocking things right just again a comment they should all be
aware of let's get a bit of humility people were building cathedrals before they understood f equals m a
and sometimes deep learning feels like and again that
so what I want to talk next is a little bit of Engagement with the history of
the subject so here uh before I do that any sort of questions or comments we can have more
of a discussion online but anyone have anything they think needs to be shared with the class on this
people literally try to make Bots where they take the full set of texts
and written information of their deceased relatives and generate a chat
bot that lets them talk to the Dead I mean not joking
this is actually what people often try to do um they also try to extend life and promise the extension of Life all that
reason Alchemy did all that was that's what it takes to get people to pay for
they're listed you know figure out poisons to kill your enemies right I mean it's but you know at some day people
have to pay for things and so that that doesn't change very much over a thousand years um what drives people
I guess love potions too I guess I don't know like I mean so I want to talk a little bit about
like the history so I'm talking taking some of these slides from Sergey just to kind of acknowledge the history
of the subject a little bit of my own twist on it so
people think about you know what is the history of learning
um people say cheering talked about in the 1950s 1950 of how you could use
learning to get to intelligence um Clyde Shannon as well and a lot of people um Norbert weiner in cybernetics
all talked about how learning could do this in 1943 you had mccullo and Pitts a
model of the neuron as a way of trying to understand learning in the brain
a lot of people will date uh machine learning and kind of neural Nets
in particular to 1957 with rosenblatt's perceptron which you might have heard
about in other other courses but it's also important to remember that at the same time you had LMS
um least mean squared algorithms developed in the context of adaptive signal processing and control and this
is actually a figure from a review survey paper by one of the authors of the original paper of the circuit kind
of perspective on the first adaptive neuron that was done and so
what what you're seeing what the subject is it sort of grew up around making circuit
um that can learn and has a long long history um of doing this
you know and it also goes through waves so there was a lot of excitement with these early circuits uh on on what on
what they can do kind of and then in 1969 Minsky and papert published a book
where they showed a counter example that certain kinds of circuits couldn't learn certain kinds of things and that caused
a crash of enthusiasm but that crash of enthusiasm was
actually a crash in kind of the popular or like one side of the field
um in ee uh control theory and
adaptive signal processing just kept going kept building on these ideas for a long time
so those ideas kept being built upon uh and then
you see the uh in 1986 you have you know back prop
uh coming up in the context of neural Nets but actually back prop
had been working in adaptive control and those areas for decades earlier
so a lot of this field uh kind of backdrop comes from the idea of dynamic programming applied to you know using
chain Rule and things like this to figure out how to compute gradients fast but you know people been working on it
for a long time at the same time period let me just my slides need to be broken
um you know people think about the Modern Wave of uh interest in deep learning
kind of starting roughly 2006 but really having a huge burst in 2012.
where you have Alex that sort of outperforming everything
but a bit of a personal story for myself back you know around this time
uh I'm dating myself actually you know um when I was a high school student and
you know the for one of the first things I was doing kind of the research lab nearby was I was sitting around figuring
how to you know do use neural Nets and back prop in the context of op-amp circuit to you know achieve certain
goals so people were kind of you know very hot on the field but then it kind of crashed
again right there was another Resurgence of neural Nets and neuromorphic Computing
and things like this and what brought it down was that other techniques uh became
on the compute of the day able to do better and those other techniques were actually
um understandable so people like understandable things
um part of this is cultural technology some of the culture cultural aspects of it that you know the areas of control
theory and adaptive signal processing which we're developing a lot of you know the underlying ideas they were also
defined by certain key applications that kind of motivated the field and in control was things like making sure
airplanes fly and that led to a culture of safety that you should understand what's going
on right and not just like oh well I don't know I don't know why I did that right that's
not okay uh and so whenever there were techniques that were had provable
guarantees and you could understand they were very attractive and kind of could supplant other ones adaptive signal
processing grew up a lot around making sure that you know computer networks were able to and phone networks were
able to communicate properly and the LMS algorithm that I mentioned was deployed to make sure that you could track
communication on tables that were changing their properties and wireless communication channels and again people
don't like it if stuff just doesn't work so but underneath
you had you know these ideas and then you know at this time stuff
deep networks and neural networks were come back why did they happen then
and here it's it's important to recognize that in a way deep learning represents a
intellectual Triumph of coming together of so many
other things that allow it to happen so and I could list this on the board but
there's like no point in sort of spending time talking about it in great detail so let's talk
you had the continuous Improvement of underlying devices in Silicon
right more what you think about when you hear Moore's Law and so you had things like finfets you
know that our own Dean right uh Professor sujay King Lou right she was one of the pioneers of you know that
we're kind of developed in the late 90s and really started to be able to drive advances in actual computers and
architecture you know kind of around this time the computers were just getting faster and faster and faster you had advances
in magnetic storage and memory technology that allowed you to store and keep all these large data sets
you had improvements in computer architecture that constantly kept moving the ball forward and you had the advance of gpus
right which are a particular kind of architecture but where why did gpus come about it wasn't the support deep
learning right it was to support Pixar and video games
right so it's computer Graphics uh that was driving many of the underlying you
know markets for these things realizing that you know what linear algebra was really really useful for lots of things
including adaptive signal processing including graphics and so you can make specialized Hardware DSP accelerators
and uh gpus you had the rise of data centers and Associated networking technologies that
made it possible to actually host these things and you had all of the advances in you
might think it's been completely unrelated but we're incredibly seminal your advances in CMOS Imaging allowing
you to make in Silicon imagers which then led to the Boom in digital cameras which then led to the boom of sharing
photos which then led to the creation of large data sets online of things like
Flickr um which then were curated to make these data sets which allowed the progress to
happen in deep learning um all along the way while this is
happening you had the underlying algorithmic advances that had been incubated and ways of thinking for
decades in control and adaptive signal processing and then you know another
Berkeley Connection in fuzzy logic which really brought ideas that were
then grew into things like softmax that we'll talk about later um you know
thinking about things in an analog way as opposed to a digital way moving beyond if that which kind of driving
digital computation and the way you thought about programs
so all of these things came together with the con with this acceleration that's coming from Moore's Law and
effectively made it possible to make bigger and deeper networks and to train them
I want to acknowledge something else here which is that I said it's actually all of eggs you know kind of has
contributed to this you had contributions critical contributions from programming language
and the idea of domain-specific languages and in domain-specific languages embedded in other languages
um that then people put in the hard work to create Frameworks like Cafe which is the Berkeley thing which then later grew
and evolved into things like torch by torch tensorflow and the light which made it possible to have
ordinary skill folks copy and paste code and get deep
learning models to work and then be able to tweak them so a course like this
uh offered before the Advent of things like Cafe and even of course like 189
offered before the Advent of things like Cafe and Pie torch we would have spent a lot of time kind of making sure that you
all knew how to program these networks from scratch take a paper start with empty code
nothing and like code it up subscribe that skill is no longer that important
because of the power of these Frameworks so we're going to take advantage of that and not emphasize that
so all of these things came together to make deep learning happen and behind it is also
you know the spirit from Alchemy of try stuff get it to work you know
it's cool don't allow people Alchemy let people who come from the trades right
Crafts People to contribute and engage with the subject as opposed to being restricted to people you know who are
very scholarly um so that's also a part of it now all that being said there's one more
intellectual component that I'm going to talk about we're going to form the focus of the of the class
is what happened around here
in 2006 2012 was there was a shift
in how people thought about the design of the circuits in
uh for deep learning before there was a lot of emphasis on thinking
about neural Nets as universal function approximate
we're going to engage with that but thinking about them is kind of being able to approximate any function
and what shifted was a focus to something that we call inductive bias
which is that we want to learn particular kinds of functions well
and that shift of starting to think more seriously about inductive bias is what
allowed people to move from continents and keep going and we'll talk a lot more about
that okay so I usually don't like talking about all this uh history stuff and why
now but a lot of people like talking about it you can also talk about the brain side
many people believe that deep learning owes a great deal of intellectual debt
and inspiration to studies of the brain
other people think not so much that studies of the brain that the brain
is pretty complicated we don't understand it very well and what we can be inspired by aspects
of it we can be inspired by lots of other things too so it's another source of expect of inspiration on par with
um the Sun Spirit how to fight with the Dragon Spirit and so you should do this
um but the important thing is we try stuff out so and there's lots that you take ideas from everyone
okay so I'm out of time for today so before I close I'll set up the story of
what we're going to do next so let me switch to my the website so you have a sense of what to expect
one second [Music]
[Music] my screens are like I lost all my screens
where did it go oh wait whatever
um so I'm just gonna I'll say it out loud next lecture what I'm going to do is I'm
going to start engaging with some of the the core doctrine of deep
learning which is take data set up a loss function optimize it
understand what goes on and I will also start talking about the core trade-offs that one is navigated
I'll try to set this up in a way that will be a brief review of material you've already seen in a machine
learning class and then we're gonna challenge that in particular there is the idea of so if you want to go and
read something go read about the bias variance trade-off understand how people think about it and
then we're gonna tell you why that's only part of the story and you need to understand more
it turns out this improved conceptual understanding will help um
after we do that basically for a week we're going to kind of go in and kind of rethink you to the core underlying ideas
in discussion however on Wednesday I hope you will actually start engaging
with neural Nets starting in the first discussion I don't have a lot of build up of math before you can even start
touching the subject hopefully you'll engage with pytorch so on your computers bring your computers a discussion make
sure you have a working python installation Anaconda install General packages we'll
post maybe some more details online and then you can start engaging with it we'll see if we can release the homework
to you on Friday not promising anything but if we can tomorrow we'll give you a homework uh if you can't it'll be the
next week it's all good um and after that we'll start talking about
the following week we'll talk about a survey of different kind of problems that deep learning can work on different
kind of give you a scope of the different kinds of architectures different kinds of problems and then we'll start going into it contents start
talking about the details in every lecture once once that starts happening I will hope to have some Maybe
um theoretical or conceptual interludes kind of break up the pace of boring stuff and with other boring stuff but
different people find different things boring um so that you know keep everything engaged
and uh we'll just keep going in the class uh online I will ask folks to
participate in if there are things that you would like to see covered you know we might be able to take some requests
of balancing the topics a bit but you know we can only teach you what we know
so you might ask for something that we don't know how to teach it okay with that I will stop
today we're going to start into the material as is typical um in courses we're going to start with
a little bit of a review and recap to bring everyone onto the same page this will also give you a sense for some of
you of kinds of ways that we look at things to see you know is this the right right
class for you um administratively I just want to announce discussion sections are starting this
week so discussion start this week
again you can attend any section there is so but you know please don't
overcrowd the rooms there's capacity constraints on every Rune so um typically what happens is the early
afternoon slots are the ones that are extremely packed so if for your
scheduled later slots in the day work then I recommend doing that you're more
likely to how you know find a find a seat and not find yourself showing up to the room and then not having a not being
able to actually attend a discussion okay we will also be releasing a
homework pretty soon and we'll give you a week for it the first homework uh we think of this as more like the zeroth
homework it's just designed to exercise begin exercising some of the ideas that you
should already be familiar with and already know how to do um there'll be the first few homeworks will have some of these flavors to kind
of bring everything back up to speed for people okay
so um recall this is the class in deep
neural Nets as I mentioned last time we don't understand everything or
that much about how deep neural Nets work and so we're going to follow the philosophy you know when we can of let's
unders show you what we can understand as well as you know different levels of understanding that we actually do have
and a part of that remember is the the joke I told you about looking where the light is so classical understandings of
ml are a bit of uh it's what we understand so it is
important that you also understand that because we will make reference to it um as uh throughout the class
um I just want to acknowledge though again that there's many Concepts and think
phenomena which occur in deep learning for which we do not have crisp classical analogs and
understanding for or ways of manifesting those phenomena In classical settings
so that is what it is but you should at least understand the stuff we can
understand so I'm going to today's it's going to be a bunch of ml review uh ask questions uh
engage so let's just start with
the standard optimization Paradigm optimization based
paradigm
or supervised learning foreign
basic ingredients
so okay this is the learning problem so we want to learn from what from data so
we have training data
which we can think of as pairs x i
y i for a supervised learning problem means we have these y eyes where
the X I's you can think of as inputs they're also referred to as covariates
um especially in the literature that is badly influenced by stat
and Y i's are labels for outputs
and we have some finite number of uh data of data we might have an infinite
number so but let's think of the finite case so this index I is going to range
from you know one to some n where n is the size of the
training set
we're talking about the optimization based Paradigm for supervised learning so this is the data there's the thing we
want to learn and so we have we call that the like the model
which is some function that takes inputs in and should be
generating things like labels so but it has parameters associated with it
these are all the things we have to set and so
that's the basic uh story of what you're faced with
you have some model you want to learn its parameters set good values from its
parameters given data uh still
just it's good to Recall why are we doing this we want to
do this so we get the underlying pattern right remember the name of the game in machine learning is to recover from data
the underlying pattern and be able to use it so how we do that in the standard
optimization Paradigm is we do training
via empirical risk minimization or something like it
so we want to choose this is why it's the optimization based Paradigm so we choose
a Theta hat that we can learn from data that minimizes something
so we have to choose something that we want to minimize
and for picking the thing that we want to minimize we actually have so you can think of it this way like what are your
design choices that's what one of the reasons we're thinking about this I'm going to keep in mind like when you're doing machine learning what are the
things that are under your control um and what are the things that are not or are less under your control
so typically you think of the data as being only a little bit under your control or not that under control you
have a selection of which data you want to choose you know how you get the data but then you just get it of course the model is under your
control right you get to pick what model you're going to use um and that's an architectural design choice that you have but then you have
to do this training and you have to choose so you're gonna you're gonna do optimization based learning so you're
going to do an argument but you have to choose what are you going to minimize
right so what you choose to minimize for example classic setting is you say well
I want to minimize the average loss foreign
the label that I actually had from my data
and what I would predict using my model on that relevant input
okay so this thing
is a loss that compares why
to some prediction of Y and returns a positive real number
returns or returns a real number
so this could be some high dimensional object this could be you know it could be a real number it could be a
vector it could be many different things right lots of choices for what these are but the loss always returns a real
number because you want this whole thing this average thing to be something real that you can take a minimum up like if
your loss returned a complex number then you'd be kind of like what does it mean
to minimize a complex number right is one
smaller or bigger than J like it's the real numbers you know right not even
less so that's like the standard part and hopefully this is very familiar to
everyone just want to make sure for those who maybe have took a machine learning class a while ago like you know bring everything back up
so this is what we're gonna do right but
what are we trying to do how do we understand what we're trying to do so
a true goal right the goal isn't to
minimize this function okay that's like very important to
remember the goal isn't minimizing the function the goal is something else the goal is generalization right the goal is
to have good performance in the real world on actual inputs x that you expect to
see okay true goal is
real world performance
so the problem is that you have
uh you need to get some way of accessing the real world right because you can do this and
you need to be able to sell how well are you actually doing so here it's important to keep in mind
two different things that are proxies for this struggle okay this is the
distinction which may or may not have been emphasized to you in your previous machine learning class but it's important to keep straight
so one is to have a mathematical proxy for this thing
or this true goal something you can measure and so the way that you have make a
mathematical proxy is you make an assumption you make an assumption that there exists
a distrib an underlying probability distribution in real life there's the real world it
is what it is right who knows how the real world works right but for the sake of our conceptual
understanding we assume a probability distribution
some p x y and we want is
can people see over right here want a low expected
you can change this we can fiddle with a fiddle with this right because it's like just the basic vanilla story expectation
over X and Y of this loss
okay
this is like the basic story um for most of you I hope
you are looking at this my statement of the basic story and you're saying wait there's stuff
missing from this basic story and if you're feeling that way good we will
start bringing in some of those things that you're hopefully feeling or missing um and for those who are not thinking
there's anything missing or unfamiliar um it's got a little bit of a warning
maybe you want to refresh um stuff okay
so any questions on this basic picture
yeah
okay I'll try to write a little bit bigger um
Okay so I'm gonna just move this board and just bring the other board
and I'll keep annotating things there okay so the first complication that you
have to deal with he's right you can think it in any order
right I'm just introducing them in a particular order uh first complication
is we have no access
to p x y in this sense we don't trust it
we made this assumption as what are you trying to do mathematically right you want to do well on average on
stuff we haven't seen you assume that there's average makes some sense and that there is uh
some underlying distribution but we don't know it so how do we how do we deal with this
problem the way we deal with this problem is we collect a set of data
that we keep from the algorithm during the time when it's being trained and we use that to see how well do we do
on that data so we wanna our goal is of course to do well in the real world
but our proxy is to hold back a set of data so solution to this standard solution is
to collect no we can just think of it as tests that
I mean there's a different held back data
if you collect this test set of held back data so let's just call it these pairs
X test comma I y test comma I
equals 1 to n test and then
we come up with a proxy for
this object or alternatively if you want to think of it this object with itself a
mathematical proxy we come up with a proxy for seeing how well we do in the real world on cnx
okay you can think of it either way and what that proxy is is you say
you look at test error
which is the average over all these test points of how well
do you do
okay any questions about this kind of thing hopefully this
is very familiar to everybody you know why do you believe this you believe this because you think like why
do you believe this makes any sense you don't tell me why do you believe this makes any sense
okay so yeah the the answer was because they believe that however we collected
this test set it is uh somewhat faithful representation of what we expect to see
in the real world and so we then expect that the we hope
that the real world follows the kinds of things that probability distributions do and so you hope that averaging and
sampling gives you some uh con uh some predictive power on what will actually happen
right so you're you're leaning that way so what's happening here is you're seeing that this mathematical proxy is
serving also as a kind of um philosophical underpinning for what we're doing
right so we say well an alternative way is that if in fact it were true that there was a distribution
then it would be that this would be a faithful representation you know if if n test was large enough of what we get so
at least it works in the case where we hope things could work and outside of that well you got to do
something right which is the standard you know well I don't know what to do I know how
to do this so I'll just do this right um approach that we use in deep learning
okay good so this is like the um the first you know basic complications
so we do this we can use this principle over and over again as you've seen in
your machine learning class use dispensable over and over again whenever you find yourself in this kind of situation
so the second complication
so we have this loss
so the loss L true I mean I'll label it right
not we care about actually care about
is incompatible
with our optimizer
Okay so example of this the classic one is
you want to do this this requires something to go around and
try to calculate what this argument is that's some algorithm they'll have to do this work
that algorithm will only work if certain things happen and it might be that the loss you care
about doesn't let it do what it needs to do so for example the classic example is
you actually care about some loss that's not differentiable because it's what's practically relevant
for your problem but your minimizer is going to be using derivatives
and so it will say can't work
right now here is where we follow like a very important aesthetic kind of principle or
like I don't know cultural principle in deep learning is that
having a problem that reflects what you care about but you cannot solve at all
is far inferior to having a problem that you can solve approximately that might
be related to the thing you care about okay something
imperfect is better than a speck of something ideal
okay because this is an engineering subject and stuff needs to work otherwise it's useless
okay so this problem is a real problem come
from that perspective of course it's much much deeper than that but this is the most basic thing
so how do we solve this problem standard solution
you say well I can't deal with this loss so I'll use a surrogate loss
I'll train
that I can work with thank you
so
we're going to do is instead of doing this with just some loss that you might care about we'll use a training loss
now yeah you have an example of like a true loss function for a specific problem and then trading loss function
sure the question was what's an example of a true loss function and a training loss function so the classic example
so classic example now I'm going to twist things a little bit
is why is discrete
so let's say
cat dog you want to classify things into cats
and dogs and the loss you care about is the you don't want to mistake a cat for
a dog or a dog for a cat okay
I'm being lost but this loss has the problem of
uh first it it's like first thing it has this type
error right that I have these y's are like from this discrete alphabet
okay it's totally fine you you know exactly what I mean by this right it means lost cat is zero lost cat dog is
one like it's there's nothing ambiguous about what I'm writing here but the problem is that you know I can't take a
derivative of cat right it's just the wrong type right so I predict something cat like
it's like what's the derivative a derivative If This Were a little different how would the answer change the doesn't make any
sense right it's discrete so this is an example plastic example and a surrogate loss in this case might
be
because many things philosophically associated this might involve changing the types also is you say well let me
move y right to be like a real number where
or training data foreign
will get mapped to -1 and a dog will get mapped to plus one
and then I'm going to actually ask the thing to my function to predict not
minus one plus one is what I want to do but to predict a score so I should change the type of the thing
I'm gonna my functional predict which will be a real valued score now I can adjust the real valued score and I'll
choose for my loss for training I'll just use squared error
okay so squared error says you know hit the
target and don't be too far away but and closer is better than further
okay that clear now most of you might not we're thinking of this example
might have chosen something different here but it's important to understand from
the perspective of solving this complication at the most basic level squared error also does the job
biggest differentiable it's nice everything works okay
any questions okay good let's keep going so there's a
general principle here that we're going to be following is we're working with computers and we're working with a
fundamentally continuous world view right in a general machine learning
class you also get exposed to some more discrete perspectives for example when
you think about uh decision trees and the like um but in a deep learning class it is
all continuous right we just only have a continuous perspective on everything so it's all
continuous math derivatives Vector spaces vectors things like this that's
all we care about so when you try to translate problems whatever they are you quickly want to
say whatever you might have started with let's quickly get the vectors real numbers things like this whatever it is
translated into this world because that's where you're you have building blocks you can put together
okay so of course you know you have many other choices of losses that you've seen
for example you know in in your background you have you should have seen the following things
you should know
so squared error should be familiar to you as a loss function other losses you should know are for
example for binary classification
you should know logistic loss should no hinge loss
for multi-class classification
cross entropy loss these are names of things that you probably should have seen
and all of these loss functions have some nice properties which
we will not be leaning on um too much we'll definitely lean on the
fact that they're nicely differentiable but in your machine learning class you probably also heard reference to leaning
on for example their convexity and how they play nice with convexity in our
class we will have less call for leaning on convexity
except at one point we will reference it for practical purposes but here
um we'll stick with this and remember what's convexity convexity is how many
nice things you have of squared error can you get for something else without it being squared error like the
philosophical way you should think about it um squared error is like the center of
our understanding everything else is like generalizations of nice properties of
squared error why because squared error plays nicely with the geometry of real numbers because the side euclidean norms and so we have intuition about it and
linear algebraic perspectives on it okay so squared error will like is the center of our intuition
now important thing again review
we have a true loss that we actually care about we have a surrogate loss L train that we can work with because the
other one just doesn't didn't work at all it's like it wouldn't move um now
it comes time to thinking about for example how you're doing for a test error
right which you can think of as here or here right which loss function do we use when we
think about test error raise your hand and answer
we have two loss functions on the table right we have true and we have train
when we're evaluating test error which one do we use
maybe I'll just take votes how many people vote for train
okay I will vote for true
how many people aren't sure Okay so
can someone who voted for True um raise your hand if you want to willing to explain to the rest of the
class while you voted for drill come on
we're going to be in this room for like a semester so you should become less shy
okay good so let me repeat that the answer was that uh your the purpose of
evaluating test error is to get a sense of how well you might do on real world
data it is an evaluation that you're doing on a specific model that's already
been optimized no optimization is going to be happening on Test air
because no it's only trying to reflect how well you expect to do in practice because there's no optimization that's
going to be done on it there's no reason to worry about the fact that the uh you
know doesn't have compatibility with our optimizer and so in this setting it's better to be
closer to what you actually care about and to the extent that you're lost true that you think you can express it
reflect what you care about you might as well evaluate that okay I think that captures your the
expansion of your answer uh that's correct right philosophically it's important to
remember what why you're doing what you're doing
okay so I'm going to write another object down and can anyone I'm gonna can
someone comment on this object so I'm here I'll write down through
aside it's an aside
number of training points foreign
what I wrote is the same as this except I'm evaluating it on the training set
okay this object is not this object
because of this word here in fact you know you'd have to like
is that clear to everybody different first ever understand this object is different than an object any
questions about that
this says true I'll write it big
right and this says train here so I've written this guy down
so anyone want to comment on
the philosophical status of that object and why you might want to look at it or
not look at it or what role it might play
so yeah um
yeah it's like trying to see but what does that mean
why would I want to look at it or what's like what's its purpose
it's the loss you think you care about optimizing in the sense it's the loss you would like to minimize in the real
world as applied to your training data after you use that training data to optimize
your other loss the approx the surrogate loss so why would you want to look at
this what context might it come up yeah um
okay good so I'm going to say that out loud because it was being recorded so I want to make sure things are recorded
um so the comment was we want to use this to understand whether or not
actually optimizing our training losses doing anything reasonable with respect
to the thing we actually care about um and see how well are we actually doing because if there was a
growth I'm gonna add some more words if there was a grotesque mismatch between what you told this thing to optimize and
how you were doing on the thing you were hoping you were kind of moving towards um then
you know maybe something's wrong okay so from that perspective the value of
this object is kind of practically speaking for everyone who's going to be working on things is debugging
right is is everything sort of sane does it make any sense at all
um any other comments that people have want to make on I saw their many hands raised on this object
yeah um
okay good so I think there's a an important Point uh that was mentioned so
I'm gonna I'll mention here so
I'm going to tweak what you said for the recording um so the there's a comment of hey maybe
we want to be careful about working with this object or looking at it uh too much
okay and the point there is that you want this to be
a faithful measurement of how things might work in practice but if you looked at this guy and said
oh wait okay I should have changed this right and then you go back and you say let me look at this again then you might
be running an optimization Loop involving you as the optimizer in which
you're actually looking at this held back data and so this data isn't being held back anymore
right and because it isn't being held back um you might not trust how well things will
work in practice so here is a good time for a joke or uh a story
so I'm going to describe to you um something which is I believe illegal
um but is an interesting scam to understand the concept of overfitting so you have this great idea
you know you want to trick people into giving you their money okay and so you say I have a really
foolproof system you can like dress it up all you want right
um and this thing is you know guaranteed to predict to you to you how well the stock market
does right and so if you said that to somebody
um mostly uh they would say uh yeah I'm not giving you my money go away right so
then you say no no no like I'm gonna actually give you evidence that my thing works and so you have this really great idea
um the scam wouldn't work today but it's the story days from an old earlier time um
you send people in the mail a note that says I know you have no reason to trust me so I'm gonna tell you
right now my prediction for the price for whether apple is going up or down tomorrow
and you write in there here's my prediction okay and then the next day send them another
letter right it says here's my next prediction the last prediction next prediction here's what actually happened
here's my next prediction and so somebody's sitting there and they're getting these letters right and they're like you know I see the prediction I
check there first they throw them away I was like oh wow this is working okay
so I'm gonna like call and you say you know but you know the more letters I send you
you know the price goes up right to get them to you know call up
the money right and then they like oh yeah here put me down they give you the money right
so can everyone see the story I've described you the story from the perspective of the person receiving the
letter um How do you how do you make this scam work if all
you have is you know a hamster and a wheel like making predictions
yeah um
yeah it's gonna go down and if it's really big eventually there's gonna be
like one person uh that you nailed every prediction did everyone hear that so basically what
you do is you send this letter different versions of it to different people right and then by dumb luck
right you're gonna get these predictions right for somebody then you happily take
their money and move to Cayman Islands you know like right this is the this is the like a scam right this scam
hopefully helps you understand why overfitting is a problem and why you
can't trust it if you can keep looking at things okay it's the same concept
okay so yes it's a very important point that was raised that one must be careful
about evaluating this thing um there's no such care on this guy though
because you're already using this data to evaluate how well you're doing um in the sense of your optimization
algorithm is looking at it all the time so whether you choose to take other views of it is cost free
uh it doesn't doesn't hurt you in any way but this one it's really looking at this data that's the problem now there's lots
of interesting research on challenging and questioning the extent to which this is true there's deep
intellectual connections to ideas in what's called differential privacy on this but we're not going to talk about
that in this class um it's really cool stuff though but but it's not really that relevant for
today's deep learning so I'm not going to do it okay so hopefully everyone is clear on
this uh this particular complication
now let's get to the
third complication
okay everyone remembers the basic story
foreign
so now you do all this you you get a nice uh
a nice loss function that's surrogate loss that you have you hold back some data and then you go ahead and run
and what happens is that
you see one of two things happening okay I'll write it in its classic form
is you run your Optimizer with your surrogate loss and
we get I'll put them in quotes crazy
values for data hat
you're on the optimizer you're on the optimizer go ahead goes ahead it doesn't argument it's reaching a minimum it's clearly
reaching a minimum and you look at the values of your parameters and they're like 10 to the 14th
10 of the 22. 7 you know like a whole bunch of values and they're like all over the map and
they're crazy okay so then you say okay
you did that and then you say well okay I got some crazy values but maybe they're still okay right
so but then and or
really bad test performance
so this is uh another
kind of perspective on the phenomenon of overfitting
that's not the scam perspective it's like complementary they're related but
it's different okay something like the conditioning perspective on overfitting
really talking more about this as we go this is a very important and
subtle thing that influences a lot of our thinking so
what do we do
foreign yeah
in the back yeah so solution a classic solution
so what that means is you do something like this
instead of picking the argument you did before so remember before this pen is going to die
see if there's another one
sampling pens to see which ones are better it's really bad right if you control
your pens then at the end all the pens die at the same time right
so punch it instead throw the pens away
exercise think about the relationship of that to regularization
um Theta hat
foreign
okay so this is what you did before and you add an explicit regularizer
on it it's like a penalty term so it's a classic example
like Ridge
so it's good to step back for a minute and see
um what you've just done here uh
you want things to work you tried it without it and it didn't work
so you say well what went wrong with this complication the problem is that I got these crazy values that were
really big and I think really big is cuckoo so I
don't want it to be really big how do I tell an Optimizer not to make something really big I say well it's going to cost you to
make things big right so please don't unless you really need to
so you add this term which says I'm going to charge you to make it big
so don't make it big and you say let's do that instead
so but remember
what you actually care about is still how well you'll do on something with no regularizer at all
good to remember we're kind of we're moving further away from the initial perspective of well I want to do well at
this so let me use the training data and try to do as well as I can on that and hope
that works in practice that doesn't work so then you do this
okay hopefully everyone has seen this right if you haven't you probably should drop the class
because we'll be leaning on this quite a bit okay
but any questions about this this idea of doing explicit
regularization yeah how how can we choose the appropriate amount
great question so notice I'll make this down notice
great we added
another parameter
in our that we have to that we have sitting around how do we choose it foreign
so I'll mark it down this way right
you ever notice that it's a great observation it's exactly the next thing I want to talk about um
so what would be the
straightforward naive way of setting that parameter that new parameter that
we added
yeah in the back okay great the naive approach is to say
hey I have this new parameter that I added so
I wish I had another color um
right is to say instead let me do theta ha equals
you'll hopefully can understand what I mean by ARG men think of it as a python argument that returns many things
right I just take the first one right stuff
just add it to the argument right just optimize over that too okay does everyone see how that's the
naive straightforward thing you might want to do I have a parameter how do I do all the parameters I have my Optimizer tell me the answer I just run
it okay so the first does everyone understand why this would be something
that it is natural to try I think it's a good idea but it's natural not nothing
that'll work but that's it's a natural thing to try anyone have a question about that why
it's natural okay so
the natural thing to try can anyone tell me what happens if you do this
right so one is I could just give you negative Infinity for this and say that's really fun right I just
make the parameters go cuckoo I pick negative infinity and I think hey reward right and I just go nuts
okay very important kind of thing so you could say okay okay not negative
Infinity it should be a loss okay like should be
greater than equal to zero what happens now it says zero it says why because like
you're penalizing me for something how can I be happier like I'll remember get rid of the penalty
that I'm happier then I can pick crazy values and I'm super happy again okay so
this naive approach doesn't work so what do we do
so Part B of the solution
is you actually have to split your parameters into two sets of
parameters
foreign normal parameters
and Hyper parameters
okay so you want to this is like a general principle actually like sometimes you
have a parameter that you think is a normal parameter but it behaves like a hyper parameter okay
um this is a little qualitative and learned by experience but
what's the hyper parameter okay the practitioner's definition of a hyper
parameter is philosophical definition of hyper parameter it's a parameter that if you
let the optimizer just work with it it would go crazy so you have to segregate it out
this is the practitioners practical definition of what a hyper parameter is
okay so
in many systems in practice other people have already listed to you these are hyper parameters and so if that someone
else tells you the desire parameter is probably hyper parameter okay so this is another very important
aesthetic aside for deep learning machine learning in general but deep learning comes from a general engineering principle
when you don't understand something and you know you don't understand something and you are pragmatic
okay you are also going to be superstitious
because you don't know what's going on you don't want bad stuff to happen so you know what you'll hit your bets
right General Life advice right for uncertain situations right consistency
isn't what you're going for all the time when you have to actually like live
right or get stuff done different from a scientific perspective where you want things to break and go oh
yeah that good right but your this contact you don't understand what's going on you know you don't understand what's going on
so what is a kind of basic level pragmatic Superstition when it comes to
deep learning that is when you're building on the work of somebody else right and they set a bunch of parameters
to something and you don't understand why they sent them that thing probably you want to try setting the
same thing right as a start feel free to change and adjust but maybe
they were onto something maybe they weren't but you don't know you know they don't understand you know you don't understand so try it first
okay practical thinking applies here too what's that parameter someone else's
probably cyber parameter might not be feel free to question but good guess to start with
so you have these you split your parameters in normal parameters and Hyper parameters and then what you do is you do uh
an optimization where you kind of use held out data to set your hyperparameter
so classic examples of hyper parameters are of course the Lambda from Ridge regression also things that might be
fundamentally discreet in nature that you can't find a good way of continuing making continuous in a proxy way like
model order uh there's many many hyper parameters there's learning rates and other things
that you have to deal with as well so what you do is
hold out additional data foreign
hold back some validation data
right and use that
to optimize
hyper parameters
foreign so what does that mean it means that
you do things in two steps you find your normal parameters by doing
this kind of optimization where you hold the hyper parameter is constant then you use the resulting Theta hat
which is tagged to those hyper parameters to check how well do you do on your held
out validation set which will be a calculation like this and then you optimize that
so you don't look at both sets of data at the same time in the optimization
this tends to work it doesn't have to work
um I in Fall 20 when I get when I taught machine learning I had examples to warn people that look you can have
counterparts of overfitting to your validation set vis-a-vis hyper parameters and there's certain situations where you you can't this
approach won't work but it's the standard approach
now when you do this should be very familiar to everybody right when you do hyper
parameter optimization using the validation set you might be using a different kind of optimizer
than you used for the argument you're doing for finding your parameters so
typically in the context of deep learning this thing is always going to be some variation of gradient descent is
what we use to do this kind of setting but for hyper parameter setting we you might be doing a Brute Force grid search
for example or uh searches based invoking ideas related to
things like multiram Bandits or other techniques of you know zeroth order
optimization algorithms that will help you do that you can also use for some hyper parameter searches gradient based
approaches when it comes to using the held out data in fact when we talk in our this class about meta
learning approaches you can think of some of them in that way okay but it's really important to understand the basics and then how to
play around with them later
so um very much related to this issue of hyper parameters
is the idea that okay I'll move to the I can't I'll close
this it's fine any questions first on how you do hyper
parameter tuning um in principle yeah also like reduce the complexity of our
model instead of doing a regular regularization ah good question so the question was if
we had this complication of getting crazy values um we can add an explicit regularizer
during training but you can also do uh complexity reduction of your model
right make your model less complex okay we will talk a lot more about this but I will like say the basic thing
the context of a standard machine learning class um making your model simpler in the face
of overfitting is good general advice
that um will often translate into good performance in practice
and so it's a part of the standard uh treatment in a machine learning class
it also has the advantage of being philosophically well grounded right you're trying to learn too much
from your data so you're getting gibberish don't do that
be appropriately humble and learn what you can don't try to learn too much
so go for a simpler model excellent important good to keep in your minds
however when it comes to deep learning practice empirically people have found that
following the strategy of trying to go for a simpler model isn't the best way to go
okay so in the context of a deep learning class
that's not necessarily going to be your go-to advice
the reasons for this are subtle and deep and things we are only beginning to fully understand but empirical practice
definitely says this empirical practice instead tends to be
oh my model isn't working well let me
adjust some other things to make it work better okay however there's versions of this
that are important so but we'll get to them later so you're
right alt solution
simplify model so that's the example of you know reduce model order
in which case model order become a hyper parameter so you still end up doing a hyper parameter search over model order
but it would be something you think about um yeah in deep learning we don't tend
to do that
other questions on this basic yeah
uh the question was what does model order mean uh by model order I'm referring to the idea of how complicated
is your model and sometimes that can be like there's a clear number so the paradigmatic example of this is from
machine learning class it says I would like to fit a polynomial to my data
um what is the degree of that polynomial the degree of that polynomial is represents the model order
um another version of it could be I would like to fit a Fourier series to my model
how many Fourier coefficients am I going to keep that's model order in general model order reflects some
number might be a vector right actually which captures how many degrees of freedom
your model is supposed to be capturing is usually tied indirectly to the number of parameters
in your model okay sometimes you have such a natural
choice of model order so in the context of deep learning um your counterparts like model order
right there's things like which are treated like model order in many times hyper parameter search for example the
depth of a model right the number of hidden layers could be viewed as a hyper parameter you could
add more more layers and you can treat that as hyper parameter which is basically of the model order flavor
okay so this is hopefully good I'm gonna
move to the next one hopefully it's all like so first
I guess maybe people might feel self-conscious about it so I won't ask but just do yourself think is everything
you're seeing good review like you're a c you've seen these ideas you might not have seen what talked about but the
review if so good you're tracking the way the class is supposed to be going if it's like I have
not seen these things I've not seen them talked about this way then warning
might not be the place for you
if you find yourself in the latter position you should be taking 180.
Okay so
that
I'll call this a version of the previous one but it's like a further complication it's important to think about
um we talked about hyper parameters before so I'm going to talk about kind of more explicitly right is the
optimizer foreign
might have its own hyper parameters
example learning rate
generally optimizers might have their own tunable knobs and in practice as a
someone trying to do deep learning you're going to have to have the discrete choice of which Optimizer to
use okay which is kind of you can think of it as a discrete hyper parameter choice
for your thing so again you might think wait this is
getting more and more kooky right like we started off saying we want to minimize this loss
um why because if I don't if I can't do well on the data I've seen philosophically if I can't do well on
the data I see why would I do with it well on data I can't see it's kind of weird right or alternatively all I have
data I see I want to do well on data I can't see well I could try to do well on the data IC you see a two subtly different
perspectives okay one is well it's a proxy and the
other is all I can do is work with this data so I'll work with this data right that's the looking where the light
is perspective both valid has ways of thinking but
you might think in this perspective this optimization oriented perspective what
the optimizer is is like a nuisance detail it's just designed to optimize this
thing for me please do a good job optimizing this thing for me why do I care about which Optimizer to use well
maybe certain optimizers like certain problems and so that's you know you need to use the
right one for the right kind of problem that's the classic perspective um what you'll see in the context of
deep learning is that we will we will soon be moving away from that perspective and start thinking about the
role of the optimizer a little bit more a little bit more differently in terms
of what the optimizer might be doing so but right now let's just think about
uh you know these issues with the optimizers so here's like a good time to review
uh you know the most basic Optimizer that we use
so most basic kind of the root
is gradient descent
foreign
so what kind of optimization is gradient descent gradient descent is a an
iterative optimization approach where you make improvements and you make
them locally so what's the idea behind gradient descent
so you're going to iterate so you're going to change the parameters you're going to change them a little bit at a time
foreign
the parameters a little bit at a time all you care about is how does your loss behave in the neighborhood of the
parameters you're in if you're going to move it a little bit you care about what's happening locally
foreign iteration step I look at Theta I for iterating Theta we
take iteration so as the maybe I'll use T for time because it's kind of like related to time so I'll call it t
Theta t and the basic idea
is you say Theta hat if you think of it as t plus one well it's called we'll drop the hats
right is just going to be the data you had before
plus some step size
also called learning rate
and what direction do you want to move in so the idea is you're trying to minimize
so you want to move look you're going to do in a greedy kind of way so you're going to move downhill
so the gradient with respect to the parameter Theta of the loss
whatever loss you're minimizing which is like hidden behind things but
the loss your trading loss you're minimizing
that tells you gradient points up so you want to move in the direction of
the negative gradient now so here
L train Theta is for example
Over N sum
I've written this before so I'll just write it sloppily
plus whatever regularizer you have regularizer is a part of this
so when I say this is like a local neighborhood of the loss what I mean is
that we're basically approximating what we have with the spherical
neighborhood you know around this parameter and we're hoping that the first order Taylor
expansion of the loss is a reasonable way to look at the loss
so we're saying let's look at this and let me put this function up on top right
so I'm going to look at this as Theta t plus some Delta
Theta and I'm going to say this thing is basically
by Taylor expansion gonna look like this
right with this um linear operator so I take the derivative of a scalar with respect to a
vector and we treat the parameters as a vector so this should give me a linear operator that takes
vectors and return scalars right it's a derivative so it should take a Delta and return to me a change
so this is like a row
this thing acts on Delta there all right this is the approximation we're using
now this thing is a row right so if I want to move in a
direction which maximizes the change of this I should match it right if I want to take a it's like
koshy Schwartz right I take a vector I want to take the inner product of that Vector with something
right such that I maximize it and I'm constraining how big the vector should be it should be exactly that size but a
line to the vector I start with right and that's where you get this idea so the transpose of this row is called
the gradient right and so this is what how gradient
descent is working people are totally familiar to people
how gradient descent works so if I look at this the question becomes
what is the role of this Ada the step size
so here it's useful to think about this to say that
there's a reason I chose T here for time to emphasize something so the operation of the optimizer
basically is a dynamical system right it's a Dynamics
it starts off somewhere there's force acting on it basically and it moves
is that clear to people so this is a discrete time dynamic system
foreign
medical system the basic considerations apply which is the sort of MSB thing you can
ask about a dynamical system is is this thing stable or unstable like as it evolves what happens
right and so this Ada
controls stability of this
system right
edit too large Dynamics become unstable
it oscillates away
now this is non-linear right your model so in your loss right is this so I'm
sorry I'll make a comment here just an F Theta sitting here right so this app can
be non-linear the loss itself can be non-linear so this entire thing is
non-linear it's first order approximation is also non-linear and so you get all the things that can happen
with nonlinear Dynamics and instability and nonlinear Dynamics so in linear Dynamics you know instability is always
Divergence by you know exploding away to Infinity exponentially fast
right that's how linear Dynamics diverge they could diverge oscillatory but still
exponentially away to Infinity in nonlinear Dynamics you get other phenomena that are also unstable right
you can also have chaotic paths you can also have where things will not converge you can also have limit Cycles where
things will like go around and follow some trajectory that has nothing to do with what you want
um lots of things can happen all of which are traditionally viewed as bad
minor comment if you use logistic loss or cross-entropy loss in many cases you
also get the dynamic system is actually on the desired path
um going to Infinity but it's going to Infinity in a controlled way on many cases because the true Optimum lies at
infinity and is interpretable that's like a minor point you may or may not have seen that in your machine
learning class probably didn't but it's sometimes relevant for us in
deep learning so we have to keep that in mind but here
it is too large Dynamics go unstable so what about just Ada is too small it's good to keep that in mind
it is too small practically speaking from the perspective of deep learning is
it takes too long foreign
so here I use T to reflect time but I one very important point I want to
make before I I close lecture we'll continue with review right which is in the context of deep learning there's two
different senses of time that you need to be aware of one sense of time is kind of an abstract
view of the operation of your algorithm which is like iteration counts it can be viewed into like we'll I haven't talked
about stochastic gradient descent but like how much data you've ingested and gone through like you know epic stuff like this
and that's definitely an important sense of time that's useful for understanding deep learning algorithm but there's also
another sense of time which is really practically significant and that's called wall clock time
okay wall clock time is how long has it taken by the clock on the wall
so when we give you homework there will be times and you'll be like okay I'm running this it's running
it's running it's running it's running sometimes we'll put a
progress bar or some kind of update sometimes he wants like it's running you go okay shooting some water come back
it's running it's running so that's wall clock time okay and in many practical
settings walk lock time is super duper important because if you're running it for example for
example on a compute cluster that's like your electricity bill right if you're using it on a cloud
system like you're racking up expenses right and so when people say things like
oh training uh the modern diffusion models for generation is expensive it means like walk walk time on processors
like that's a bill right of a thousand ten thousand a hundred
thousand a million 10 million dollars right so wall clock time is the
different but is also important so when you make these iterations too small
the step side is too small it takes a lot of iterations but also takes a lot of wall clock time
so we need to worry about these things so that's the final thing I wanted to
say there we will continue moving forward uh through this material
basic review material as we go next time
I have office hours now you can ask questions come up and ask questions um on stuff on lecture then we'll move
left off last time so just a matter of a couple of comments of things that you
should see um you should have seen homework zero 0 has gone out it's just a shorter homework designed to get you
kind of into the swing of things make sure you know how to use uh you know review some ideas
also make sure you know how to use pytorch and things like that
so that's the goal you've experienced the first discussion section so I want to make a comment about a discussion
section so remember for some of you are graduate students and you are might not be used
to the idea maybe in your original school there wasn't a discussion section so discussion sections uh in this course
are uh cover mandatory material like the material covered in discussion is
required for the course so it's basically a chance for you to understand the material in a more
interactive way with specific problems in front of you and where you can get
help from course staff and work in groups so we noticed in in the
attendance at the discussion sections from graduate students in many sections was quite low so want to encourage grad
students to you know do try to come to discussion it is important uh for undergrads the discussion sections
are very valuable we're not taking attendance but in our experience people who attend discussions regularly and
participate actively do far better in the course on exams so we do want to
encourage you to do that come to discussion it'll happen every week
um so yeah we will release a homework one soon
ideally today or tomorrow I kind of get back onto the Friday on the our regular
Friday due date cycle and homework one will also have
substantial review given that the electors are also first few lectures are
are largely review okay so we're going to continue where we left off last time so
last time we talked about the idea is amazing it's still on the board um that you know when we're doing these
optimizations to figure out how to set all the parameters in our model so that
we can generalize well uh the Optimizer might have its own
hyper parameters like a learning rate this gives us a kind of chance to talk about the idea of learning rate and what
happens so that the idea of gradient descent
optimization so here I'll just you know make Mark things as that are vectors as vectors we have lots of parameters so
think of them as a big Vector we evolved the parameters based on
training loss so we look at the training loss and we compute its gradient with respect
to the parameters and then we take a small step in the direction opposite the gradient
because we want to reduce the loss so this is uh you know basic gradient descent and
if you look at this this is coming from we mentioned last time it's coming from the perspective that we're going to look
at the training uh loss locally so we think of the training loss as a
function of where we are but we want to understand where it would be if we were to take a
small step and so by doing a Taylor expansion view of the loss we see what the dependence
is you know on the parameters and so we can then figure out what
direction to move in okay to maximum you want to make this as small as possible means we match these two together
so uh this is the you know and we commented
there that you know because this is fundamentally a dynamic system that this is evolving with time T which is an
internal computational time we have to worry about the stability of it and in general if the learning rate
is too large the Dynamics will go unstable but if the learning rate is too small it'll take too long to converge so
that's the guess we ended last time kind of in this discussion so
we closed with the idea of thinking about okay so we care about getting this learning rate right so that we converge and we mentioned the
idea of wall clock time the idea that it's really important in these problems that things actually
um work not with some abstract iterative time but with actual time on the clock right because that's dollars
um of compute and so in doing that
I want that's like an excuse to talk for us to talk about uh stochastic grains there's another review thing we have to
do so you look at this uh training laws and
the training loss has a big sum in it
right because you're summing up over all the training examples you have of this loss
so you know you can use when you compute this derivative right this derivative you know will
nicely it'll play nicely with the sum right you can just bring it inside the sum but you still have to deal with the
fact that you have a lot of terms here uh to to work with
so Computing this gradient here
uh as you know requires Computing uh this loss typically and it can become
expensive if you have n large so what can you do about this so the standard
approach uh that one has to think about uh you know
making this part more friendly with respect to wall clock
is to say excuse to cast the gradient descent
SGD and the idea of SGD is to say
instead of using n all the Turning data just randomly sample it
foreign
just use and batch typically much smaller
than n random samples
so when you use end batch what you'll use is an approximation of the training loss and hence an approximation of this
gradient where instead of where everywhere you had an N you instead use n batch so this n becomes
an end batch this becomes an end batch
and then these eyes become you know a Rand you know
x i these y i x i pairs become a randomly drawn n batch number of
training points ideally kind of our mind how we think about it
so when you do that you're replacing
something which is an average of a lot of things with an average of a smaller number of things and so you expect that
this will be an unbiased estimate of that quantity right because just the
randomly drawn a subset has been selected yeah
multiple times in the same Apple okay great so I'll talk about that in a
moment so the question was uh what is going on here so I'll say it this way
wait you have uh n training points so how do those training points get hit um
if you're randomly drawing from them so here we'll talk about that in a moment kind of at the next level of details
let's understand the basic level of detail so the basic idea here is you make a if you were to do a random
draw you get an estimate of an app so the quantity you're interested in is an average
um and if you take a random draw of points instead of the entire set of points you will get an estimate of that average
that estimate will be on itself on average if you were to go over all
possible things you could have drawn it would be the right thing um but it will have some variability associated with it because the fact
you're taking a small number of samples and not a large number so
before we talk about kind of the next level of detail let's understand philosophically and make sure we recall
for everybody philosophically why is this okay why is this reasonable to do so
there's many answers here and one answer is the standard uh deep learning answer
which is I can't afford to look at all the points I can afford to do this so
I'll do this okay this is like a totally legit answer but we can go a little bit further in
our understanding and so a little bit further comes a depth of understanding comes from the
idea that look when we're going to be moving this data
there might be different regimes of where we are and when we start off we have no reason
to believe that where exactly we start off is going to be a good place so probably in this thing there's lots
of these points that are all going to be telling us to move in a similar Direction so why do I have to exactly compute that
direction when I'm going to have to take a step in a certain direction maybe a small number of points will be definitely enough to
tell me what general direction to move it so I don't need Precision beyond what is
required for me to take my step appropriately so it's like a that's a that's a particular intuition that's a
pretty reasonable it's good to keep that intuition in mind um as you think about
um you know like we talked about this hyper parameter which is learning rate which is how you might think about how this learning rate might change during
our training so one the default answers you pick one learning rate you stick to it um which is also not that bad but
sometimes you might want to think about how that learning rate might change during training and so think of what's
called a learning rate schedule so understanding the intuition of what might what's going on is helpful to
think about help you understand what's going on what might be going on with the Learning grade schedule
so that's a you know a reason why this you might
believe uh this will work it's actually kind of interesting uh that there is alternative perspectives which most of
you might not have seen uh in a standard machine learning classes why I'm not going to dwell on it here so but for
those who are interested you can uh you might be interested in
looking up the area it's called online learning which is learning from data that is revealed to you sequentially one after
the other and there's very interesting um beautiful analysis of online learning
is so much surprising which also argues for another reason why these kind of
approaches looking at only a small set of data still work okay I'm not going to dwell on that here
uh it's not to say that online learning isn't relevant for deep learning it
actually is quite a pit but not at this current stage of uh coarseness of
description that we're doing and also it's not something that I expect everyone to have seen Yeah question
uh does that also imply to Divergence of convergence okay that's a great question so I'll
repeat it the question is if you have a step size that you know will converge
for a regular gradient descent does that mean that that same step size will also
behave well for stochastic gradient descent okay so it's a really good question so uh
the general answer is no okay um that's uh not necessarily the case uh
the reasons why are a little bit uh subtle but I can give you the base the basic idea so you can think about two
things first thing is uh you know if it is too large then the
Dynamics go unstable um when it comes to going unstable like
actually like exploding uh then yeah it because the averaging is the same size
and the expectation is the same way uh you expect that the same thing will roughly work for in terms of stopping
you from going exploding however what you actually need to have happen is you need your model to train
and converge to something that's good okay so uh when you do that and this is
actually pretty subtle and delicate uh
I'm going to give you first the conventional answer that probably most of you have seen so I'll pull you
afterwards see who has seen this but if you have so
the conventional answer goes like this look
if you need this thing to converge okay uh what does that mean what does
convergence mean it means that this thing stops moving time keeps going but this is no longer moving or not moving
by very much okay everyone that's what convergence is so what would make it move
this no that's the last thing so that's not making a move was making a move is this part
so standard kind of optimization thinking with the actual gradient says
look when you're at a stationary point which uh an Optimum definitely is at the
stationary point the derivatives are zero right there's no more variation no more little little movement won't change
anything uh substantially so the derivatives are zero and so that's why you converge because you're approaching
that point and so this thing is approaching zero and so this which is
where you were plus something which is approaching zero means that this thing is staying still
got it okay now that works for gradient descent
but let's think about stochastic gradient descent in stochastic gradient descent you are not using all the
training points you're using a subset of training points so the classics picture you should have
for this is something like this you want to at least squares fit
a straight line to a bunch of points that's what your least squares fit looks
like right like a straight line but notice something your losses are a
bunch of sums of squares right every individual term is a bunch of sums of squares sums are squares and so at the optimum value
this although the overall gradient of the
loss is zero the corresponding component that corresponds to this point is not zero
does everyone see that because this is not this point isn't fully happy
right this point would prefer this line to move up the overall gradient is zero because of
a balance between these points so some points are tug of war like that that's like the key
thing in deep learning and all neural net uh in all gradient-based learning generally
um what you have is a tug of war between points some points are pulling parameters one way and other points are
pulling parameters the other way classical perspective and so the final answer you get is the
balance point when everyone is balanced so if you were to have you're actually to
be at this Optimum thing and you were to run not gradient descent
but stochastic gradient descent what would happen what would happen is that a small number
of points whichever ones you randomly picked will likely have some imbalance in their uh pulling in their tug of war
does ever understand that like because everyone together balances so a subset only kind of balances
and so what will happen is that for SGD this term
will not at Optimum point
under the classic perspective will not go to zero and so what will happen is that if this
learning rate is anything which is itself not decaying this thing will only kind of converge
it'll like converge something sort of look like it's converging something and then it'll do a little dance Wiggle
wiggle wiggle wiggle wiggle wiggle wiggle wiggle wiggle wiggle wiggle keep wiggling right because of this stochastic variation that's happening
from this GD right
did everyone see that and so under a classical treatment um which I hope people have heard about
um of SGD and its convergence uh you would have for SGD to have actual
convergence it's not sufficient for Ada to Simply Be small enough it actually at the end has to be diminishing
to actually get convergence okay question yeah
okay that's a good question the question was do we typically have end batch the
number of points we pull uh change with every iteration or we keep it the same
um
okay good good so let me let me answer that question that's a good question so
uh first does n batch typically change uh during training and the answer is no
you usually pick something and stick with it why do you pick the thing you pick typically you pick the thing that
is the biggest batch you can sustain given the parallelism and memory of your
architecture so you want to make sure everything fits in memory and you don't get cache misses and other kind of crazy nonsense
happening so you pick a batch size that'll fit and then you use that because that will be the fastest because
what you care about again from the deep learning motivation which is not a theoretical motivation it's like I want
my wall clock time to not be too big okay so you go for the batch sizes that
will Max will make your wallet clock time the best okay now your question
your question was if you just picked a batch size let's say seven
okay seven things
okay great that's good that's okay good I wonder if that's exactly the distinction I want to make so um you pick this batch size but then every time
you take a step you draw a fresh samples so it's always that's why it's shaking
around is because you get a differently imbalanced tug of war in a small group
small groups will never balance they'll always be imbalanced
did that answer all your questions okay good um
so this is the classic story and I want to ask um kind of a poll how many people have
seen this classic description of why SGD requires uh diminishing step size
for convergence raise your hand small number how many people have not
seen this description of why SGD requires a diminishing step size for convergence
so for people who have not seen STD requires a diminishing step size for convergence they wouldn't want to
volunteer for weather so first of all how many people have heard that you should have a
diminishing step size for SGD okay so there's an overlap between these categories so for people who have heard
that you should have a diminishing step size for SGD anyone want to volunteer for what
explanation you have heard for for this I want to know where my class is
for the explanations
if we give the same Stepside a very random
Fighters never go to the the optimum point if you want to be
and you will never be there so it's unclear to me so I'm not going to repeat that but it's unclear to me
why the fact that the gradient becomes small will make you do a Wandering what was the what's that what's that bridge
because if you go through large steps and you will
cross over the table so that's why you're not too big so the
question is whether you've talked talk about why you would want this to go keep because the steps has to keep becoming small smaller and smaller and smaller
that's what I want to know who's heard an explanation for that just because I don't understand where my classes yeah
okay foreign
some people have heard is when you're far away uh you can afford to take a big
step but when you're close you don't want to miss it or go unstable so you go
for a smaller step that is that kind of what you're saying okay uh have other people heard that explanation
okay anyone with a different explanation for what is going on that you want to yeah
it's these acts towards the center and um the explanation for the diminishing uh
is that as we go through like you can you interpret the error you you basically interpret the stochastic
Matrix as adding noise or adding some error that people said and so you want to diminish that so that that error
switched to zero to approach okay so that explanation is the same as the one I said
right that the stochasticity of the small batch is what's causing you to wiggle and so you have to suppress that
by making this get small if you want this thing to actually converge so good good I want to make sure that people also understand it um
yeah once you once you make a distinction it's good for people to remember make a distinction between oscillatory Behavior
of bouncing back and forth and random walk behavior of being shaken about these are actually different
both occur in the context of under the analysis of gradient descent both are combated with things that might look
similar but they're different behaviors
uh yeah okay so the question was is there a relationship between the understanding of the step size and some
appropriate eigenvalues and the answer is yes in the analysis of classic standard gradient descent this uh this
this threshold between stability and stability is exactly tied to eigenvalue analysis but in the context of
stochastic grade descent it is more subtle okay because you have to think about eigenvalues a certain random matrices
and how they behave but there's still a relationship um so there's a second explanation which
I was thinking maybe some of you have seen of kind of another like analogy which is
also good to know uh suppose I wanted you to compute the average of 10 000 numbers
okay but you would be showing these numbers one at a time okay
how would you compute the average of 10 000 numbers being shown them one at a
time
yeah the answer is you can update the average right so you keep the you keep this is like the you know you have very
little limited amount of memory you keep the average and then you get a new point and you know what point it was like
which like this is the 37 000th point you're told that um then you can use that to update the
average okay and if you do that if you write out those equations which is a good exercise if you haven't done it already and
sometimes done in some classes you'll see that the amount that you move the average with every new Point becomes
smaller and smaller and smaller that when you have two points there's a second point it moves the average quite
a bit but when you've already seen 999 points the thousandth Point moves the
average only a little bit so that's also a reason kind of
inspirationally for why this uh step size of getting small so this is a
useful side thing to make an aside comment that is useful in the context of deep learning
whenever you find yourself feeling lost or confused or
like wondering what is going on okay what you should do in the context of
machine learning disclose your eyes it's like you know like on a on a ride or
something close your eyes and say repeat to yourself it's all just fancy averaging it's all
just fancy averaging okay because actually that's kind of what's
going on all the time so everything that happens that whenever learning works it's all just fancy
averaging and low pass filtering okay that's literally it's some like
disguised version of this with all kinds of other sophistication so
if you find yourself confused about anything you can say okay let me see if
I can see which way averaging is some familiar Landmark that I understand very well and like make the connection
between what's going on here and some kind of averaging and that might help you Orient yourself
for what's going on okay so
sdd is like fancy averaging um but you know it nicely works so I
told you that I was first going to talk about this in the um classical point of view
so now I'm going to give you a preview of something that can happen
we'll talk about it we'll keep coming back to it in the context of deep learning
so in deep learning sometimes what you see isn't like this
picture at all um
instead what can happen is that your gradients will go down because
every single one of your points will start to become happy
it won't be that your training points uh are
all unhappy and in a visible tug of war where some are okay I'm this unhappy but
you're that unhappy so okay we can't move the parameter it's like everyone's like I'm good I'm good I'm good I'm good
okay which is a behavior which is different than what happens in classical settings
but it will it will happen in deep learning settings all the time and so in those settings what will
happen and I can't draw a good picture of it for you yet I mean here right we understand this a little bit but not
sufficiently for me to draw it in a very short term what will happen is that
your end batch points will all be reasonably happy as you keep going in
training and then this gradient will become small and in those cases you will
see that even SGD with a constant step size will converge
so in practice for many deep learning applications despite the fact that you've been taught and we talked a
little bit about the classical story of making sure your step size is diminishing the learning rate schedules
that we will sometimes use will end with a constant learning rate
we will not have a diminishing learning rate and it'll still work so I wanted to warn you of this
discrepancy between what you've seen in your machine learning classes and your optimization classes and what you might
start seeing here okay with all that now I can come back
to that next level of detail that was asked for earlier so the way I described it was the kind
of conceptually easier way which is every time you take a step you draw n
batch random training points and use them however remember what we care about is
wall clock time and this can become painful sometimes
with respect to wall clock time because your data set might not fit even in you
know not your GPU memory but they might not even fit in your regular memory and so doing a random draw and then doing
disk seeks to different points to get your your
data is too painful okay so what is often done is a different
strategy where you say you know what sampling with replacement and sampling
without replacement for a large number of points is kind of the same thing
so let's just sample without without replacement so what you do practically speaking is
you take all your data points and you shuffle them
Shuffle is kind of really really important shuffle them randomly and then just look at them and batch at
a time so when this kind of approach is done there
are some specialized terminology uh in the Deep Learning Community um used to refer to things
so here people at the beginning were very very concerned about whether or not
you've actually seen all the data and so they would refer to the times that you've actually gone through this
entire batch of data into all the data you have as an epic so you go through the entire data
you know in the shuffled way that's a netbook and then you do it again now here uh
you can in principle uh and if you want to avoid certain kinds of subtle
bugs that sometimes happen you might want to do this you might want to reshuffle your data between epics
foreign just to prevent having some pattern
repeat and bother you um but many people are like that's too
annoying and so these won't just shuffle your data once and just keep keep using it again and again
the balance between avoiding certain subtle weird bugs that can happen and wall clock time and implementation
complexity Yeah question
um will sampling with replacement yield better results uh the answer is no typically not because what will happen
is relative to each so if n batch is sufficiently small
then for every one of these steps that you're taking the difference between sampling with
replacement and without replacement is almost nothing
so it turns out not to make a big difference at all
again this is just to give you the bias in this course and in a lot of
deep learning research unless we State otherwise we'll be uh two cases where
you have a lot of data like lots you know thousands Plus
when your data sets are very small then you know different levels of care might
be required because then you really then you do see differences between these things
ah good question the question is when I write n batch is much smaller than n what what should this be the answer is
nbat should be as big as works for you in deep learning practice and batch should be as big as works for you with
your Hardware um fast
okay and that will depend on the problem it'll depend on your Hardware um
that's what it is so n will usually be like much bigger but you have no hope of getting there so this is something that
you can hope to get to when you go to get there now there's an entire other part of the subject that we are not
gonna do more than maybe touch on here and there I'm hint at which is
when but you can think of as the classical CS
point of view on deep learning which would be look
you have massive quantities of data you have to Marshal not just you have to
deal with the details of the computer architecture that you have and the memory and networking systems that you
have and the distributed storage that you have in Real World Systems to actually get these things to work in an
accelerated way when they're very big okay so this is like a something that is usually referred to as ml systems
extremely important in practice um to deal with wall clock time and when
you the large companies that exist uh all have teams of very highly paid
people who figure out how to manage those systems and to get them to work
and the algorithmic design sometimes takes the considerations of those
systems into account because if you can't scale it and run it on large enough amounts of data it won't
give you the right performance one wall clock time and dollars so you have to choose an architecture that does that
entire level of consideration we are not going to be talking that much about for one simple reason we have no way
right now that we understand how to give you homework about it
to figure out ways of giving you homework but hints in that direction we might start putting some of it in but we
don't know how to do it so we're not going to spend time on it okay so this is like a longer than I
expected um review of SGD any any questions on that people have
any remaining questions on basics of SGD
Okay so now you understand the
what you think is the basic perspective on why things work uh kind of the way that
people normally understand it um we are going to challenge it a little bit um oh in in a little while but for now
let's just be happy that we've kind of caught up on where we are uh where you were from your
um ML Class on the big picture what I want to do now is to like just
remind people a little bit about just neural Nets themselves since this is a
class on deep neural Nets and you haven't really talked about neural Nets yet um
so what's a neural net foreign
perspectives and they're people who come at it from a biologically inspired point of view we already said before it's some
kind of analog circuit that data we put feed data in it processes an analog way you get analog outputs and you interpret
them that's for sure true but the question is like how do we think about that and what's the point
um so I like to think of it not in a
biologically inspired way but instead um keeping with the general Spirit of
deep learning of you do what you know how to do it says look
we know how to optimize things using gradient descent and stochastic gradient descent
what kind of uh computational thing makes it easy for us to use gradient
descent and stochastic gradient descent whatever that is let's use that
okay so
you know so I'm I'm going to assume that everyone has seen neural Nets before this
continues to be review okay I'm just going to give you a slightly different perspective on it so
you know that you can compute derivatives using
gradient descent music Sorry using the chain rule and you know that computers can do the
chain rule for you and so computers can do automatic differentiation
so for us a neural net is anything that plays nice with automatic differentiation
Okay so it's uh computation graph
start okay analog circuit
plays nice with automatic differentiation
so automatic differentiation is sometimes referred to in this context as backdrop
which I'm assuming everyone has seen it's a required Topic in the machine learning class at Berkeley
um I believe it's a required Topic in most machine learning classes uh around the world if you haven't seen this
and you're in uh 282a make a post on the Ed for 282a and we
can point you to resources for it so
why is this great because you can express your computation whatever you want to do as
an analog circuit you can write it in a programming language as a description of that circuit and then automatic
differentiation will happily compute all the derivatives you ever want or need and so you can then execute this
approach to doing optimization and that's what you're going to do so this is kind of very high level
so let's think about what do we want from the circuit
so here the traditional perspective on these circuits
um which is well I think most people have probably seen but I will emphasize here
is that the what you want is expressivity
right it should be rich enough with the parameters that you have in the circuit to express the kinds of patterns you
want to learn
foreign
ly when people thought about this expressivity and they thought about the
patterns of Interest uh people said wait I don't actually
know what the pattern of interest is I can't hand craft a circuit to do what I want
the whole point is I'm trying to learn it so how do I know that this something is going to be expressive enough to
express the patterns of Interest and so one of the early approaches that people had to think about this was to in
a way borrow from uh are already existing Rich tradition
in both mathematics and the early theory of computation
which is to say you know how I know something is expect can express the patterns of interest if it can express
any pattern then definitely the one of interest is also expressible
everyone see that logic there so it's the same thing that makes people appreciate Universal computation it's
like how do I know this computer is powerful enough to do the thing that I want when I don't yet know how to give
those instructions well it can do anything that's expressible in you know certain
kinds of computational you know classes so great
if it can express any decidable thing hey that's probably going to explain capture the class I care about so good
okay similarly and much more apropos
for uh deep learning you have the idea of approximation Theory which has been
around it's like okay I want to solve these differential equations well I
don't have necessarily a closed form solution for these differential equations what can I do well I want to
express them in some form by which I can do the computation to compute coefficients that will give me approximations to solution to these
differential equations so let me use Fourier series why is Fourier series okay because it's to give me enough
terms it's a universal approximator for functions again there's lots of you know what kind of functions did you know
so that style or you know if I have a polynomial
right give me a high enough degree polynomial I can express any function right with a high enough degree
polynomial so this kind of style of thinking is what's driving traditional views of uh the expressivity that's
needed here um so patterns of Interest so for example
quote Universal function approximation
so for Universal function approximators you're used to thinking about presumably
everyone has seen things like Fourier series and uh polynomials as a way of generating
any function that you care about approximately any function you care about however to understand
neural Nets there's actually a simpler to understand category
um that is kind of conveys the idea more crisply and instantly
so let me think about a specific kind of domain to visualize so let's visualize
functions from R to R 1D functions
that's the homework a discussion had you do okay okay and in the discussion you saw
that look if I have a 1D function I can draw it on the board it's like the only kind
of function I can draw on the board right because one is the independent and one is the dependent axis and then I run
out of axes on the board okay so asym function
any function how can I approximate this so if you ask if you if I asked you okay
give me a polynomial of some appropriate order degree that will approximate this you'll be like yes I believe this thing
exists um give me a computer and I can grind it out I asked you okay give me Fourier series with some number of terms
a Fourier series that will approximate this function you'll be like yes I know this exists give me a computer and I'll
grind it out right but for one category you can just draw it and that is piecewise linear
right piecewise linear functions approximate all functions right
you know that so you can say uh if I have another color
let's see if it works
right I can
I can just put line segments down
oops
and approximate this function and you know that
piecewise linear functions
given enough pieces
foreign approximate arbitrarily well
okay any questions on this hopefully this is very familiar this is like way back when you studied things
so the question becomes how can we think about piecewise linear
functions uh in a way that we can express them using a circuit
because if you can express a piecewise linear function using a circuit
that we can then have automatic differentiation play nice with that
means we can opt optimize piecewise linear functions so on discussion right so let's just make sure that we we
all understand it so
first before we do that just a comment I'm using piecewise linear here
you could say that wait actually I don't even need piecewise linear to approximate I can approximate using uh
piecewise constant right yeah you know like I'll I'll lose
something I'll put a lot more that's why I do like Riemann sums right like piecewise constant right so
I can use piecewise constant um it's an interesting exercise for you to
think why are we think talking about piecewise linear and not piecewise constant constant simpler
um and as we kind of go through the exercise of making a a circuit for for this we'll see
you know maybe why because remember we want to express the functions of
Interest in a way that we can express using a circuit
that plays nice with automatic differentiation so that we can use gradient descent
style optimization to set those parameters right it does us no good if we break
anything along the way okay because we need stuff to work
Okay so I will cover this
so one way of thinking about how can you get uh piecewise
linear functions to be expressed is to say hey
what can I think about piecewise linear functions as being made up well they're made of
here it's like a little bit of a Twist of how you have to think about it so one way of thinking about piecewise linear
functions that you might have is that they're made of little pieces okay little segments that are linear
that have a beginning and an end uh but there's another way of thinking
about piecewise the neurofunctions so you can think about piecewise linear functions
are can be constructed
from elbows with Moon quotes elbow functions
and what's an elbow function something that looks like this
it just goes and bends right so it looks like this or it looks
like this or it looks like just like one
example here's a second example here's a third example
right elbow functions and a constant
thank you you want to move the whole thing up or down so does everyone see that
because you can think of a piecewise linear function you can think in one day I think generalizes in higher Dimensions
too you can think of it this way is start somewhere
and then you change your slope keep going change your slope how do you change your slope changing your slope is
the same as adding an elbow that reflects the change in the slope right and you keep going change the
slope again add another elbow change change so piecewise linear
functions are constructed out of these elbows so now the question is how can you
express this l so great this means that
if you have an input X and you want an output
F Theta of x right it means as a computation graph as
a circuit I can put some which has a constant
okay and a bunch of things that are elbows
where this is thumb circuit that reflects an elbow
does everyone see that this is a way of thinking about it so now how do I make an elbow
well I can make an elbow by saying
go look at one of these boxes
so turns out you can make an elbow by taking
uh an input multiplying it by some w
adding some B and then take the result
and put it into a non-linear box which is just
maxed of 0 and whatever its input is
this thing is called a relu
rectified linear unit which is inspired by diodes
that's two Inspirations actually
it's inspired by a diode because it looks like
zero for below zero and then one
so they align a line of slope one after zero so that's the kind of diode inspiration
and if you want you can think of this piecewise linear stuff is also inspired by the classical analysis of Bodhi plots
right piecewise linear understandings of things
so either way it was amply present in the intellectual space for
you know how do you build these guys Okay so
first thing that's really important for everyone to understand is
why is it that this combination can actually make
uh these elbows okay so let's just quickly make sure
everyone's on the same page so first let me just kind of touch base and see if everyone's like you saw this in
discussion we already were doing this in discussion using the computer um
and hopefully you'll find the discussion worksheet quite informative in terms of the um the notebook part we can watch
how as you train the these piecewise linear things move around and the elbows
move and and adjust the first element is check
how many people in their ML Class have seen
this kind of a description of why you get uh basically a linear approximator
linear piecewise linear approximation out of this non-linearity value has anyone seen this
okay so let me a couple people uh
in your ml classes that you saw neural Nets what non-linearities did you see
raise your hand and say something so I can I can't make out the different things
someone raise your hand yeah sigmoid okay what other nonlinearities
you see tanh hyperbolic tangent um what other
just those you didn't see this one it's not this one too
okay what you saw sigmoid first and then 10h you say there's also this guy yeah yeah yeah yeah that's often how it's
taught um different reasons um sociological cultural historical
um but uh I personally think this is the easiest way to understand what's going
on so I will like stick with this because why should a sigmoid result in a universal function approximator you can
grind it out right here it's like obvious ly near like it's it's all good right
um derivatives are here are easier too um so okay so you look at this and you say
why does this work so let's look at what this function looks like
so maybe I'll think of this entire thing because it's parameterized as two parameters right so I'll think of this
as f w b of x
two parameters so this is the circuit View
we can write it out as like math symbolic math
S zero comma B plus
w x
which is the same if you don't like the max can use IFS
okay
so what is this this is the same as
w x is greater than equal to
minus B and now
you have to think about if W is greater than or equal to zero okay
equals if W is greater than zero
right why because I need to by bringing the W or the other side I gotta check which way the inequality goes right
and if W is less than zero
right then this whole thing is
um e
X is less than equal to
minus B over
w okay so first thing we notice is that
actually and this you'll often see in the context of deep learning which is why I'm doing
it this way is that we kind of got it a little bit
like we can't the way that we've got it set up right now
we can't get this guy number two does everyone see that
because we can't get anything to Output that's less than zero okay so I need to get this flexibility
back of being able to do this so I can get this flexibility back by looking at my circuit architecture and
doing a modification and saying so notice does everyone see that like you try to understand something you see
can I do everything I can do yes no not quite what am I missing I'm missing the
ability to get negative things out how do I bring them back well I can
multiply by a bunch of numbers here
if I allow myself a bunch of parameters to multiply by at the end then I can multiply by a negative
thing and come come down does everyone see that okay good so now you have this and you
have this and you see that look the point of the elbow that you get like
where do you transition this elbow point is
minus B over W for that particular one
it's the slope you get on the other uh on the other side well
this is the case where you have a slope that's less than zero
right so this is w and this is w
two different cases so if you have a negative W you'll get this kind of behavior positive W you get this kind of
behavior either way the elbow will be at minus B over w
wonderful so now you understand this basic case and you understand
um what's happening here in terms of you add something on the outside so
this is kind of the prototypical uh multi-layer perceptron architecture
is kind of grows out of this okay because what do you have here is
the way this is viewed is using neural net language you say okay
this thing is the it's called The Hidden layer
that's the output the things that are coming out of these boxes are the output of the Hidden layer
and this is the output layer
and this is just pure there's no non-linearity here it's just athyne right there's a shift and a bunch of
multiplications so this is called the linear unit or linear layer
is the hidden layer with relus
yeah
why the number of them will the product make any functions yeah so the question
is that what do we know so you know for 1D functions that if you add more and
more pieces you can approximate functions as well as you want um if those functions are for example on
a compact set so the inputs are always in a certain domain if the inputs are always in a
certain domain a large enough number of these will approximate as well as you want which means for the perspective of neural Nets that when you call when you
increase this it's called increasing the width of the network this is usually called the width of the network
so it says for by making a one hidden layer as wide as you want you can
achieve Universal function approximation
okay that's a great question so I'll repeat that question for everyone the question is is the class on deep
networks if you get Universal function approximation by making one thing wider
why do you need to make things deeper that question is like the subject and
topic of the entire course right is what like why do you need to
think about the architecture and in fact historically
um so you know you might have heard about this uh
intellectual disciplines um go through as engineering disciplines often go through Cycles
of uh enthusiasm try to make things work some kind of things start working and
then like people get super excited by them and then things Crash and Burn
right and then like they say okay maybe we should go do something else for a while and then they comes back
so I've lost count but the historically
the first golden age of neural Nets was kind of in the 50s and 60s
and people were really interested I mean kind of grew out of an existing base of analog computation analog circuits
they're like oh cool we can do these and we can make patterns and lots of work happened and then there was a paper that
so before they tried to do things without the hidden layer for reasons of computation Simplicity
and then they showed that without the hidden layer you couldn't achieve certain things like it's not not
possible you didn't have Universal function approximator and then people crashed and burned like okay stop this
is like too hard let's like go do other things for a while then um arguably in the
um late 70s 80s there was another Resurgence um in uh
in neural Nets in the interest in neural Nets um and in that Resurgence the idea of
universal function approximation really captivated people they're like oh wow this is amazing we can make it work
using wide networks people tried playing around with depth too but you know didn't really know how to do it and it
was a lot of fixation on the universal functional approximation part and things worked and you got in
advances and some people didn't really care about the function approximation part just care about making things work
um but then again uh other Technologies became better using
at giving the given the technological capabilities
computationally at the time and then neural Nets again fell out of favor by
the I'd say mid 90s um because other things were much better
at being able to work and also had provable guarantees associated with them and would work better
then you had the Resurgence of interest in neural Nets which occurred
um I mean definitely you know was a complete breakthrough in 2012. right in
the modern era of interest in deep networks kind of follows from that and one of the things that happened was
actually the shift of the fixation on
expressivity towards something else and so people shifted from really caring
about Universal function approximation to also caring about the fact that
um so the other goal goal number two
right besides expressivity is learnability
can you train it
or the patterns of interests
does it actually learn the things you wanted to learn so it's not enough for it to be able to express what you want
to express it has to learn the things you wanted to learn and that's different
just because something can universally Express anything doesn't mean it'll learn the things you wanted to learn given data
given especially given the data that you have okay so but before that I want to make
sure we're on the same page for certain terminology I'm going to like Express that terminology so when we talk about this hidden layer
written out this way I will just edit this in place okay for Simplicity
when you combine this with this the usual way we think about it is
as follows we replicate the pattern I just we had over here that was very natural in terms
of how you draw it so you say Okay each of these is
getting multiplied by some weight
so we're going to choose a way of doing it so usually we'll use two indices
um I forgot what exact order we use in
I'll just for now use the order where I'll describe the layer number first and then the particular entry the
w21 w22 w2d
b 2 0 or 1. right is it only one of them
so this would be w11 W one two
W One D multiplication and then we add stuff
for reasons that come from circuits these are called biases
B1 two the
d and then
we run it into the non-linearity which has no parameters you want to keep
our parameters visible
and by convention our non-linearity has no parameters
the output of the nonlinearity defines the output of the player in this case this is the convention so these guys are
referred to as weights if they multiply things and if they're added on they're referred
to as biases okay the weight makes sense right bias
is a circuit term for biasing circuits
okay so far so good any questions on this
picture besides the big question of like why do we do things that are deeper which Ruger will build towards
it's for activity like uh comparative engineer model because there's not
there's no parameters okay excellent question let me repeat
the question for everyone the question was okay if you think about expressivity as
coming from I'm going to re-paraphrase your question if you think about expressivity is coming from knobs to
twiddle um it seems that expressivity should be tied to parameters
and sure we have all these weights which are knobs to Total we have all these
biases which are not object twiddle we have more weights and more biases to not to twiddle but without these relus here
without this non-linearity here the addition if I just replace these with identity Maps then all I would have is
basically all the biases would just multiplied through these weights and add
to a universal bias and all these weights would just be like would go together and everything I would
get is just a single affine function I get one universal weight and one universal bias that's it and I would
only be able to express redundantly using all these parameters
um the stat of affine functions on one Dimensions but with the addition of the
relus I get all piecewise linear functions notice even in this most
natural representation there is some redundancy like this W here
and this B are choosing where that elbow is right does everyone see that
these two are choosing the elbow but this W and this W together
are choosing what the final slope is that gets added in here we did that originally because we wanted
to make sure we got the negative ones in right but it is redundant you see that
because between the three of them there's only two actual degrees of freedom that are being manipulated
where's the elbow and what's the slope like where's the corner right
so just counting parameters here isn't telling you what the true flexibility is
of what you're getting so the question of you know like why do you need the relus well
you can see from first principles why you need the relatives it turns out that
generically the addition of an appropriate non-linearity here
anything that's like sufficiently non-linear injected here will still give you Universal function approximation
so it's you need the non-linearity to get the ability to express anything
um so yeah again there's theoretical papers that talk about this about exactly what
this is how it connects to like Concepts like bandwidth and so on but for us it's just from first principles we can see it
but the important thing is you can't just naively view the parameter count
the overt parameter count as the actual degrees of freedom even in this example
there's redundancy yeah
okay great question whenever we use the word something is a universal function approximator we're always mean the same
way you think about Fourier series as being Universal function approximator which is as you take the limit of number
of terms in the Fourier series to Infinity you can approach anything as close as you'd like right same thing applies for neural Nets
when everyone says that this neural architecture is a universal function approximator of this means as you take the width depth other things to Infinity
it can approximate any function as close as you like
great question the question is if it turns out that there's theories saying that you can use other non-linearities
and still have Universal function approximation why do you use a particular non-linearities in practice
does it matter and the answer is it matters for different reasons okay first
reason is what makes your life easy
relu is extraordinarily easy to differentiate
right because you just check what's the derivative right on this side it's zero
on this side it's w with respect to X One respect to B like you can just take these derivatives super easily right
so it's really really easy to run derivatives through relus so that's like a reason to have relatives
that's like a practical perspective um as we go we will talk about other
perspectives of desirable properties of non-linearities for which value will
also be nice so relu has an another just since you asked the question relu has another very
nice property which is often useful to us is that it's non-saturating
right if you think about a sigmoid or a tan H or something they like flatten out
a relu doesn't just keeps going so the fact that it it doesn't flatten
Out means that when its derivative will be zero is just in one place other place
it looks like is linear so that is often useful um uh in the context of larger systems
now everything here is about not expressivity it's about this other part
learnability because that's what we're going to talk about we spend a lot of time thinking about this
how can these things actually learn the things you want and one of the preconditions of learning the things you
want is that it should be able to converge to something useful in a reasonable amount of time
so the fact that things are non-saturating is in part related to the fact that the gradients won't just go to
zero and not move very much
okay so um with that I'm actually out of time for today
um we'll pick up where we left off and kind of hopefully you understand this I can just do one more thing sorry I
should draw one picture since I said it this guy right this is the same as this guy
circuit wise right so it's just a super simple
the end we out relationship
okay now we have questions about stuff on the board right here and then we'll walk over Cory 258. Let's go to Cory 258
Cory 258 for Office Hours. Cory 258 OH. 
homework zero is essentially all review stuff um for should be for most people
homework one is a mixture of some things which are review and some things which
will be new for most if not all of you um so uh just yeah keep that in mind
so to understand what's going on in homework one this lecture is pretty important we'll be introducing some ideas and ways of thinking that are
interfeatured heavily in homework one the discussion uh tomorrow and um the
you know a little bit of discussion we have on Wednesday um that will also help you kind of get
understand the material we talked about in lecture today and you know get ready for what you need to do for the homework
so you remember last time we introduced uh these relu networks uh
you know with a single fully connected layer and we said okay let's build this
thing up to understand piecewise uh linear functions right as a universal approximator from the traditional
perspective of hey what do we need from uh a model that we want to fit to data we
have to build Express the patterns of their interest so one of the ways of thinking about it is can you express all patterns and piecewise linear is kind of
an approximation to all functions so great and one of the things that we commented on
at the end was if you looked at the parameterization we had I'm not going to write it up again maybe I'll get to it
near the end of the class but if you look at the parameterization we had we had a set of weights at the beginning
and then we had another set of weights after the first hidden layer and we saw that in terms of what is being expressed
those two weights are kind of not exact not actually redundant but they're sort
of redundant right that both of them adjust the slope of the line of this of the line segment
if you remember that so this issue of you should be in the
back of your mind should be thinking of well okay when you in genetic in general when you're going to parametrize the
network you might get these kinds of little bits of redundancy um how does that show up in terms of
what you actually learn how does that influence learning because again if you remember from last
time we talked about how what really defines the current age of deep learning the kind of
Advance was the withdrawal of focus from you know being able to approximate all
functions and starting to think about well does this network trained on actual
data using the learning algorithms the training algorithms that we have does it actually learn patterns of Interest
right so this can it actually learn the thing that you care about
and so what we're going to be talking about today is we're going to start introducing some tools and some ways of
thinking that are building towards uh an understanding of what actually about an
architecture in the Co in the context of things like gradient descent what actually makes it favor certain kinds of
things in terms of what it actually learns okay so that's the the topic uh
of today so if you think about it
we're going to do is we're going to use regularization as the way to understand
that because regular regularization is you add a penalty term the traditional approach you add a penalty term and
the goal the penalty term is pragmatically I didn't use the penalty term it learned some gibberish or
something I didn't like or I think let me make better so I'm going to add a penalty term to shape the optimization
so it learns something more to my liking the pragmatic view of regularization make it work better on the kinds of
problems that I actually have so let's think about so what regularization is doing is shaping what does something actually learn so first thing I want to
make sure everyone understands this everyone understands how adding a regular explicit regularizer a cost term
in the optimization doesn't change the expressive power of your network does everyone understand this
okay it's really important like it doesn't change whatever functions you could represent before you can still represent them all
you're changing is how the Optima with the optimization is favoring okay so that's why we're going to use
regularization to kind of try to get at this this concept so let's think about it we have a
a loss function
for your parameters data which is this we wrote this last time
this is some overall the training data
sub training loss
some label associated with the is data point and your parametric function that you're
learning F Theta evaluated at the trading input x i
so this is the standard loss without the regularizer and you can add
to all of this a regularizer if you want
so to understand all these things uh it's all very general and abstract and
hard to wrap your head around it's really important that you
understand at least one such model well at an
intuitive level and the standard one we're going to use because the one we understand the best
is you know least squares with Ridge regularization
I'll put paradomatic
in large part because it's the only one we understand that well um is least squares
the ridge so what's least squares with Ridge these squares the problem looks like this
I have some Matrix X I have some weights W that I want to learn and I want to
approximately have this equal to a set of labels why so remember think if this is a big Matrix
n rows corresponding to the end data points
D columns corresponding to features in your training data this is a
D dimensional Vector because you have to learn weights for all of the features and this is a
column that has n
uh labels in it for the end data points okay okay so least squares means I want to
approximately solve this equation like I have I want to understand what
are the weights that generate those y's which I do by optimizing the cost
of the ridge right optimize cost
and I'm going to drop uh one over n just for not having to track
that thing I'll bring it back when I need it for when it's important but right now I'm going to just for Simplicity eliminate it
so I look at the
the error of my prediction squared
this is an N Vector all right look at this length squared usual two Norm
and I have a penalty term that says please don't make these weights go get too big
also tuner so far should be completely familiar to
everybody and this has a solution in closed form
which is I'll write it in its classic form
X transpose X Plus Lambda times the identity
inverse X transpose y
so just to understand this Matrix that we're inverting here this is a d by D Matrix
is this good for everyone good so
we're going to make sure we understand this better and we're going to understand this in some different ways
those ways are represented on the homework so everything I'm going to do kind of fast in lecture you will have to
do kind of dotting the eyes and crossing the t's in the homework to make sure that you understand it but
like let's make sure that everyone understands this the basic story what's going on
okay so to understand um what's going on here
let's first uh there's like different perspectives we can take to understand what's going on
I'm going to start with uh just how does gradient descent work on this
because we're going to use gradient descent as our Optimizer so let's think about this is the closed form solution great one shot
you get this how would gradient descent solve this problem
so first let's think about just for straight ordinary least squares consider
this the OLS loss
this is the ridge term for us right
let's break it up into two terms so let's start with for OLS
what's how far do we move it's what's the gradient so
again this is a straightforward calculation
we move ETA
ingredient so I didn't put a one-half sometimes we put a one-half here so I didn't put a one-half so I'm going to keep the two
um I'm going to take the gradient to Ada
X transpose y minus
x w so whatever the iterate is so the teeth iterate
foreign descent right how do you do that again
this is that you've already been playing around this stuff in the homework you take this thing you can take its gradient you write it out as a quadratic
form take the derivative this will pop out um but it's good to understand what it
means just to we're all on the same page this is the residual
right this is how much is left like we have a current way of doing the prediction that gets us this we want to get all the
way to Y we aren't there so there's something left this is the residual and this is like a
an adjustment a bending of the residual to go in the right direction
any questions on this make sure everyone's on the same page it all should be review
um now let's do it
or Ridge foreign
if you look at this loss we have the same loss we have before except we added another term
so we're going to move in this direction
with another thing is that clear to everybody right just
linearity of derivatives so you're going to have
so I'm just going to group the terms together I'll pull the Ada out because I don't have it
yeah um I think I can pull the 28 out two Ada
and now I'll get this guy
which is the contribution of the residual
and I'll get from the derivative of this
so what do I get from the derivative of this I'll get a minus
Lambda
WT okay
so I want to make a comment here on this thing just uh because it's a bunch of
stuff to review so we might as well get this comment done right here
if you look at this is the kind of how how you move right so
you you have two things you move you move in the direction here of the gradient
you know kind of negative gradient move this direction and then you have this other thing
which basically what how do we interpret this thing what this thing says is hey
you know what however you are whatever Direction you're pointing it currently
like move closer to the origin
right like come in does everyone see that so let's think about it actually iterate right so let's
look at the iteration so for for this one let's write it as uh the iteration so WT
plus one the next weight I'm going to take this term so what is
WT plus one WT plus one is the old WT
Plus the thing that comes from the uh gradient descent right but now there's a
WT sitting here with a minus Lambda there so I can pull that
term over 1 minus 2
ETA Lambda and then here I get the gradient term that's just the OLS standard gradient
term
okay just like a comment on how gradient descent works for the context of rig is that clear to everybody what I did if
it's not ask just algebra but I'm skipping steps
the Richter I mean the regulation term are we subtracting ah okay so
what I'm writing here is the direction we're moving okay it's not the gradient and the
direction we're moving is what I'm writing okay so this is the negative gradient so if you look at the negative gradient
for Ridge you have a minus Lambda w
so when you combine that with the full iteration that you get a 2 ETA because I didn't
have the one half right two Ada Lambda and instead of just having the WT from
last time you have 1 minus two Ada Lambda so you should think about when the
learning rate is small enough right and the regularization term is small enough which it better be for
things to work well this thing
this thing here this is basically uh bringing the weight
it's a stable thing pulling the weight in you see that it's like pulling the weight towards the origin
so if you didn't have any more stuff coming from this
okay what this would do if this was Zero this would as time iterates would make
the weights Decay to zero exponentially fast I think as a geometric does
everyone see that so this term is called weight decay
so in deep learning context sometimes you'll see as an optim instead of talking about things in terms of oh I
have a rich term or something like that they'll say I'm going to use this Optimizer and I'll have this for my
weight Decay parameter and what they're talking about is basically controlling this thing in the
optimization it was an important piece of terminology you should know connect to something
it's not mysterious in any way it's just um something which comes out of very
naturally inspired by gradient descent for Ridge
okay yeah
it's a good question right so if you make your um Lambda become too big relative to
your learning rate right it's kind of interesting right because if you think about it
when Lambda is really big you're saying please please make these weights really
small right but just grinding through the math
right you see that well there might be a negative interaction with the learning
rate if the learning rate is too big um with a Lambda that's too large in the
context of weight decay I mean if this is where you're coming from in many in many
um libraries you actually have direct control over this in This this term
but as a comment um many people have gotten confused
about why something is working the way it is in practice because what they've done is they've added a
a regularizer explicitly that's a ridge type to their loss
and also enabled weight decay and sometimes you on the computer you
can get it to say to have more of this effect than you might have intended because you've got an aspart twice
so it's important to keep this straight and one of the interesting things we'll talk about later um
when we when you get we start talking in the direction of Transformers is we'll talk about optimizers that try to like be careful
about how they're doing this effect okay this is like just a bit of excuse
to review and to bring this up as a piece of notation
okay so this is a perspective uh that we can have
on just classic Ridge I want to do another perspective because
we're going to use this all the time as a way of thinking and in your machine learning class this may or may not have
been brought up so I want to make sure everyone is on the same page with this
so whenever you see a problem that is largely defined using euclidean
Norms two norms and the neuralgia break things that are going on
then uh one tool very powerful tool in Your
Arsenal is to say can I understand better what's going on
by looking in some different coordinate systems this is always a good idea generally but in the case of two Norms
you have a rich arsenal of ways of getting coordinate systems so it's always good to see can I understand
what's happening in a different coordinate system to see more clearly what's going on so in two Norm your go-to tool is the
SVD
and we have homework to make sure you review this um
okay so the singular value decomposition is super useful so we're going to do is we're going to look at what we're doing here and we're
going to look at it with the lens of the SVD so here
there's a perspective that's important to keep in mind what we're going to be doing is taking the perspective of an
outside Observer nothing we're doing is going to be impacting what the algorithm is doing or
anything we're just going to look at it from the perspective of an outside Observer watching what's happening who
has access to the SVD the outside Observer has the SBD
so the outside Observer says hey you know what I know that X
this n rows D columns Matrix
is actually equal to U Sigma
V transpose SPD so again let's make sure we understand the types of everything what size is X
remember I'll write label right here x n D and so for our purposes we're going to
use the full SVD foreign
because it'll help us keep some things more straight so in the full SVD
what is the size of you how big is you
someone just like raise your hand and try that out or just shout it out and buy it right yeah exactly this is n
by n what's the
size of the V transpose d by D
and what's the size and shape of uh Sigma how many how many how many columns
D columns right and and
wrote same shape as X in the philosophy so we do this and now remember SVD has a
cool property that these guys
are unitary right orthonormal columns
sometimes called orthogonal matrices also you just use when you might have complex
and rows so they're really useful because they
represent coordinate systems that are like rotations that don't change any angles
or any lengths so if what you're talking about doesn't care about rotations rotation type
things relation reflection type things you're free to switch to those coordinate systems and nothing will change so that's really cool because here you
have two norms this is a two Norm on what size vector
right an n-dimensional vector and this is a two Norm on a d dimensional Vector
both of these you know will play nicely with this
so if you use the SVD to understand what's going on a little bit better then
you know what happens so we have many different ways of doing
this I will take a particular perspective that's going to be like in shorthand
so I take the problem that I have right in terms of what's approximately equal to what so I can see you know what's
going on on as I change coordinate so I take this perspective and I say okay so I have U Sigma
V transpose w is approximately equal to y
this is the same problem because I'm looking when I say approximately equal to I mean hey in two
Norm the difference should be you know good so I can free I can rotate everything
it's the same problem right so I can take this and I can say
hey I can change coordinates on W
um to be such that I look at a new W which is V transpose w and that's fine because all I care about
is penalizing its length and this doesn't change its length so this is also cool
so I do this and then I get Sigma W transpose and hey you know what
this is just a y this is the rotation of the Y I'll just call this a new y
tilde and now I have the
approximate problem which is sigma
W tilde is approximately equal to y but this is a really nice problem right
because let me expand out Sigma
let's think of the situation in which um for now for illustration purposes
let's think of n as being bigger than d so I can draw things into some right
shape right so this is going to be
Matrix like that okay
so now I can ask what are the w
what are the W's I'm going to get okay so for the case
so now it depends on how I actually how I solve this problem if I solve this problem using OLS
right in which I'm not going to have a rich term a penalty term at all so the
OLS solution is just going to say
I'm going to use Python indexing to look at the ith component of w tilde
this is just going to be 1 over Sigma I
for the ith component of Y tilde
I is less than or equal to n and zero
otherwise
did everyone see that
important to understand now so here I'm going to pause to survey the audience um
for how many people is looking at OLS through SVD coordinates something that
you have seen before if you've seen it before raise your hand it's okay to raise your hand if you've
seen it before yeah okay if you haven't seen this before raise your hand
okay it's like a little over half I haven't seen it so it's super useful and
cool to you you think of it this way because what happened at a high level is
by looking at things in SVD coordinates what was a complicated Vector problem
as essentially decomposed into a sequence of scalar problems
right and each of these scalar problems is easy to understand what's going on
okay so this is very useful because complicated what's happening in some vectors how things might be rotating or
moving around you can't wrap your head around I mean yeah
ah so I'm looking at the case where okay when I said I less than equal to n
but N I meant like how are many of these are not zero the instant you get zeros it's just you say zero because think
about it if you have a zero what's going to happen you have a a row that says 0
times a bunch of stuff is approximately equal to something please adjust the stuff you're like well
I can't right there's nothing I can do about that stuff so right it's like it's not there
so the N is bigger then it's a tall Matrix and so the sigmas run down the diagonal
and so you'll run out of places to go before you run out of rows hence this shape
does that answer your question yeah
ah and here refers to the original problem how many data points do I have okay
so this is the OLS solution what's the red solution
so for this it's useful I'm going to talk fast you
need to work this out for yourself if it is not clear to you okay
so so for Ridge
you have an optimization that looks like this okay which means after you have made this
change of coordinates you have an optimization where everything that is replaced in the following way for Ridge
this becomes y becomes a white tilde X becomes a
sigma W becomes a w tilde okay
foreign problem
every time you have a euclidean norm squared it's just a sum does everyone
see that's just a sum so this is just a bunch of sums
now let's think about it you want to minimize this you're penalizing the length of of w
tilde are you gonna change this part at all
no obviously not right these these W's out here are doing no work for you at
all they don't help you reduce your residual in any way and they cost you lent
right so you're not going to set them to anything other than zero so this part is just unchanged
okay now you have n terms in which
you have it's you have n different optimization problems they're just summed up
so you might as well consider them and decoupled optimization problems like there's no interaction between them
okay because this is diagonal so it's like solving a scalar Ridge
problem because you're solving a scalar Ridge problem the solution you can just grind
out the algebra it looks like this
let's write out the answer
so this is just like scalar optimization right it gives you this
so this is uh so again you can see it
here
so maybe this is a good if you haven't done this before I strongly encourage you to do this exercise
in the interest of time I don't want to do it because I have more things I want to talk about but um
remember we said the entire problem just transformed to having a sigma anywhere you had an x a w tilde anywhere you had
a w and a y tilde anywhere you had a y so this optimization answer also
transforms in the same way W tilde equals Sigma
transpose Sigma plus Lambda times the identity inverse
Sigma transpose y tilde right just reading off the the
transformation Sigma transpose Sigma is just a diagonal matrix filled with
sigmai squared plus Lambda times the identity is
plus Lambda inverse of a diagonal matrix is a
diagonal matrix with the reciprocals all the way down
Sigma transpose is just going to pick out Sigma I times y i tilt
right so you can also do it just by formula it's good to be able to go back and forth between these different perspectives yeah
I'm not sure metrics
if you see the entries of the stigma large metrics the last term should be Sigma d
okay so this is shorthand okay
uh so you're right sorry my mistake my mistake sorry sorry sorry thank you
thank you you've attempt to point out before I did not catch it when you did
sorry is that way if you have this is this is what it looks like if
if n is less than d then it looks like this
if n is greater than D then it looks like this
oh segment D you're right you're right
thank you yeah so one of these two so I was talking
about this case so this is not n but d thank you for correcting me
good sorry ever understand
what's going on yeah
oh yeah yeah there's no otherwise case all right this case doesn't happen thank you I'm just doing this so that
you can understand um the reason I'm doing it writing it this way is I really also very much care
about the case which is uh this case
so I want to acknowledge the otherwise case just forget to get it on the board thanks
Okay so the SVD perspective is interesting and nice so I want to dwell a little bit
about what's Happening Here okay
so let's write out this aside
sorry like what it is sorry let me just write it this way
foreign correctly
there you go okay or basically yeah it's the idea here is that your training data
is some dimensional okay
that so whatever that dimensionality is is going to cause you to have some
number of singular values so whatever that number is is this
right that I that you care about here okay so it's important to understand what
this is doing right now if you look at it this way
right a perspective on just this closed form solution says look if x transpose X
is too close to being invertible not invertible is this close to a
singular Matrix then inverting it would be bad adding a Lambda I pushes it away from
being singular and prevents this from causing giant things to happen that's kind of at the high level of
looking at the level of matrices but when you look at it in at this kind of more refined level you can actually
understand this object better right so you say you use the classic
um kind of analysis that one does for example in analyzing body plots in circuits for responses where you say
look Lambda is going to be something
different than Sigma I squared it's either going to be intuitively much
smaller than Sigma I squared or much bigger than Sigma I squared so
understand what happens in those two regimes okay so this thing
so let me get an aside here right if you add two numbers together
and one of them is much bigger than the other how does addition behave
right think about it you take your numbers and you add them what is one billion plus 0.02
a billion right right when you take two numbers they're
very different in size and you add them bigger the bigger one survives and the other one's gone
right this is like just how addition works so if
Lambda is much bigger
than Sigma I squared right then
this thing is approximately equal to Sigma I
divided by Lambda if Lambda is much bigger than Sigma I squared
alternatively you can think of it as if Lambda is much bigger than Sigma I
squared then Lambda divided by Sigma I is much bigger
than sigmai you can think of it that way too either way you get this
if on the other hand Lambda is much smaller than Sigma I squared then this
sum is basically just Sigma squared in that case Sigma squared will cancel
with the sigma and you'll just get a 1 over Sigma I if Lambda is much smaller than Sigma I
squared and smoothly going in between so looking back here
what it says is Rich regression for those directions
that are well represented in the training data meaning what meaning that their singular
value that you have is substantially large relative to the ridge regularizing
parameter Lambda for those directions effectively you get the OLS solution
so if a direction is well represented in the training data so um for those who come from a background in controller
system ID if the input the trading data is sufficiently exciting those modes
the regularization will do nothing it'll just allow it to preserve straight through yeah
uh have like the smallest simple value
okay good that is an excellent question I will um repeat it uh with a Twist to make it
parse a little bit better so the question is Hey look
if I look at this interpretation of what's going on with the ridge regularization what it's
saying is that I'm going to set those weights to be in the transform remember this is all happening from the
perspective of the Observer right gradient descent is grinding away doing its thing it can't see this The Observer
outside says I'm looking at this Matrix in the in the right
coordinate system I'm going to look at what's happening in this other coordinate system so from that other coordinate systems perspective the
observer's perspective that outside Observer is saying look wages but basically said
uh some of the weights in the core transform
coordinates to what they would have been for ordinary least squares and is going to put others
close to zero right because Lambda is much uh bigger
so this is going to become those are going to shrunk down to zero by contrast those weights it's for
example Sigma a was Tiny OLS would be blowing them up so everywhere OLS would be blowing those
weights up to be big giant numbers with an appropriate Lambda Ridge is going to set them closer to
zero this is the perspective of the outside Observer and there's some smooth thing happening in between
okay for the intermediate weights that are closed up um there are so whose Sigma squared is close to Lambda
now did you answering your question in practice it is being found in many many many
kinds of applications that when you have natural data of any kind giving you some kind of Matrix uh of features or data
that those things tend to have a wide dynamic range of singular values right that you don't tend to have
singular values that are all kind of hanging around in one place you tend to have a in many problems you didn't have
a wide dynamic range of singular values so this becomes useful in dealing with this wide dynamic range of singular
values okay now you could say dance to raise your question you could
say if that's the perspective of the outside Observer why can't we take advantage of that
outside Observer perspective algorithmically and algorithmically say for example let
me take the SVD explicitly algorithmically before I tried solving anything
and let me use the SVD to do dimensional reduction where I will dimensionally reduce by
projecting my data into a low dimensional space precisely that low
dimensional space which corresponds to the high singular values in X so I only take those directions where you actually
have sufficient representation of you know signal in the input to actually
excite any mode of a linear model and I'll only fit that that's explicit
dimensional reduction by using the SPD what this is saying is that in fact the
two solutions will behave somewhat close to each other but
here for Ridge you never had to take an SVD
to get that benefit no computational cost was required to like compute an SVD of a giant thing it
wasn't needed what happens is it just happens for you automatically by doing gradient descent with weight decay
you're already getting that benefit okay so algorithmically in terms of
practice it's important to know when you need to do something and when you don't need to do it so
it's kind of interesting that way that you might have started off saying look I better do dimensional reduction you say
well let me first just try Ridge and ridge is worked it's like oh that was cool right you
could save some work in terms of doing it um but yeah it's good to sometimes explicitly in deep learning we will do
things that are counterparts of dimensional reduction followed by uh you know fitting but here
I'm just trying to show you you don't necessarily have to do that yeah
okay great so this is now now you're getting you're getting into where we want to be right
so the question is have any of my commentary changed the
expressivity of the model okay and
the model is still what the model was right so in our case that we have a linear model for which says you're
allowed to use any linear combination of the features that you see right
these features might be non-linear whatever there's some features right any linear combination of them is fair for
you to use that's the expressivity of the model however what this is saying is sure your
model has that expressivity right you could do that but if you choose to fit the data using
gradient descent with a weight Decay term you're not actually going to use that full expressivity
if they're Dimensions which are not well represented in the training data you're not going to set them to anything
and you're basically going to take the default whatever the default is in this case
it's linear model default is zero in other models the defaulters might not be zero you'll get whatever the default is
in terms of how you predict along variations in that direction is that clear
yeah in the back
so ah good question the question is if you did explicit dimensionality reduction versus doing this uh soft one
using Ridge which gives you more accurate results the answer is you can construct examples that go either way
but the point is a little bit close to each other you know they're not going to be hugely different
Yeah question in the front
great yes how can you under I can interpret this result so we interpret it this way so interpreting this
I'll just move this over it's fine everyone remembers what's on that board
Okay so I'm going to draw some functions
should think about this axis as being on a log scale okay
I drew two functions there okay one was a step it went straight
and then step down and went somewhere and the other is like a smoothed version of the step
you see that these two functions so what this is saying when I talk about this
difference whenever you say something is much bigger than or much smaller than in terms of addition
you should think of that as being bigger and smaller on log scale
because log is like order of magnitude scale so when I said think of a billion plus 0.2 right these two are numbers
that are different on a log scale so what it's saying is that uh
this function is this smooth one is behaving roughly like this step
right on a when viewed on log scale so it's going to keep for this and this is determined by
log scale sigmai squared and this is Lambda
okay six selecting which are the singular values which are going to get to contribute to the
Learned parameter weights and which are the ones that are not
does that help you understand the what I'm saying in terms of what this means
yeah log scale Sigma I squared same as log scale Sigma
log doesn't matter
okay great question so the question is um I'm talking about everything in the
context of two Norm what if we were thinking about one Norm or zero Norm or you know fourth Norm or something like
that would the results be different so remember what I said about the joke about the drunk man looking for his keys
at the very beginning we understand the geometry in two Norm much better linear algebraically so
we're going to use that to build our basic understanding there is when hopes
and believes that there should be parallel stories that occur for other Norms one Norm has some other
interesting things and zero Norm has other interesting things that it does that
aren't captured by what's happening in two Norm it is people's belief that if you look at some other Norms similar type story
should be happening but we don't fully understand that and it certainly wouldn't be as easy to understand and
describe as here but the key thing is that
so understand what regular how regularization is functioning that's what I'm trying to help you understand here is that you can think about
regularization as um okay you think about deep learning as
cooking right and you can think about you have no understanding of cooking you just have a set of spices and things that you
add okay and you're like it doesn't taste good right let me look on the
internet it says add I don't understand what this thing is okay I'll take them and add it okay
and I have no idea why I'm adding this right I don't even know what this thing is really except that you know some
parameter in Python right and I don't know how much of it to add or what it means to add more of it or less
of it right it says teaspoon tablespoon if any of you cooked like and mistaken like a teaspoon for a tablespoon you're
like this can happen if you don't know what you're doing right and sure you can
approach things that way um and sometimes you'll find yourself having no choice but to approach things
that way but what I'm trying to do is to give for some of the cases a little bit of intuition for what might actually be
happening so you can understand and then you're going to try to take that intuition to other domains and sometimes
it'll carry over successfully and sometimes it won't sometimes we understand how to mathematically make
that shift and sometimes they don't um for those who want to understand
these things a very powerful set of tools for which you can kind of build on your optimization understanding is to
say look we're talking about gradient descent in two Norm um and doing things this way if you want
to go to other norms and to understand what's happening there's like techniques called Mirror descent and so on that try to bring some of the geometric ideas
um over into other Norms but again for deep learning practice that tends not to
be that important so we're going to stick to two and arm style intuitions
it's a good question though great question okay so you understand now
what's happening we have to so behind this thing we had great descent written out which I should
bring back
okay
and we described as one perspective as look at gradient descent another perspective is look at things in terms
of the SVD you can combine these perspectives and look at gradient descent in terms of the SVD
and if you do that you'll notice something
what's the transformation right wherever you have an X it becomes a sigma Matrix
y becomes a white Hilda X becomes a sigma this becomes a w tilde again remember
this is the perspective of the Observer so let's write this out just this term
out I'll write it here just a little section
which I have some space just to keep everything in one place before I shift to something different
so I'm going to write this out after this transformation okay
that's the only key term that matters to Ada
Sigma transpose y tilde
minus Sigma W delta T
right so when you look at it at first it just seems like okay how is that any more insightful than what you're seeing
here right it does you know this is some Matrix but
remember these are diagonal matrices okay so this is a diagonal matrix
this is the diagonal matrix so you under you see something else interesting happening
which is that gradient descent in SVD coordinates is just operating coordinate by
coordinate there's no interaction between these coordinates at all just like the optimization problem
decoupled into a bunch of separate optimization problems even gradient descent in SVD coordinates is just
moving along these individual coordinates by themselves there's no interactions does everyone
see that so
what's happening is that
look at this Sigma transpose out here
this is thing so let's look at in terms of the ith coordinate okay so the ith coordinate is moving like
this to Ada Sigma I
y tilde I minus Sigma I
w t i
so that's what's happening
so what's happening is that for those Sigma I's
that are big in your data you're going to move more
do you see that this is like
an important point we're going to build on okay this is your step that you take in
the greatest remember you have two parts you have a wake Decay part the weight Decay part doesn't care
about what the sigmas are you ever see that just 1 minus 2 ETA Lambda like whatever right has to do with the Lambda
it's how fast Lambda is basically telling how fast am I going to try to bring these weights to zero collapse
them to zero it's like the gravitational pull of zero is what this is doing okay
this is the thing trying to get to be a good fit forgetting to be a good fit
what's happening is it's gonna move more in directions with large Sigma
directions with small Sigma move very little because they get multiplied by these sigmas times you know Sigma
squared what you kind of expect to see here
right the final solution basically doesn't go very far in directions which have small sigmas but here you can see
it kind of in the Dynamics itself the Dynamics move in directions of large Sigma first and foremost small Sigma
directions move a little bit and the gravitational pull s them to zero and doesn't let them
Escape which is what makes this thing happen and gravity basically has very little
effect here and so you just get a good fit because they move very fast
okay so
this fact is super important
because what it says is that the actual evolution of gradient descent when run
on machine learning problems will move first and foremost in directions that
have large singular values that's where it's going to try to fit
which comes back this back to this kind of dimensional reduction right in a way that it's not just the case that the
Subspace represented by large singular values is where you're going to fit
it's also that within that nested sequence of subspaces all which are relevant you're going to first fit
in those directions which are uh the biggest singular values and then
you'll fit the next one and the next one so the next ones yeah
ah you're saying that if this thing is small it might counteract that
sure but when this thing is small it means you've already fit that direction right so interesting what happens when
you haven't fit it yet when you haven't fitted yet this Sigma is going to say move fit to it fast it
is going to tell you hey go slow right that's your learning rate and the sigma is going to say no keep moving you
know like and so you can see also that your Ada the learning rate is controlled
by how big these sigmas are you can't let any of these Dimensions go
unstable because like intuitions that you get
from the map okay so with that there's two things I want
to do one I'm gonna just do um yeah so let's see
so now I'm going to do two little perspectives so there's regularization that's explicit
right explicit regularization like I add a ridge term something like this but there's also other ways to get
regularizing effects and what I what I'm telling you already so is
uh from what you're seeing here for example even if
you did not have the ridge term at all so no more weight decay you have no
weight decay this effect is still present okay does everyone see this
gradient descent just gradient descent running on squared loss no regularizer
will Thor and try to fit the directions that have
large singular values first the directions that have small singular values
it will creep along okay you see that suppose one of those singular values was
very tiny right like 0.01 you're afraid that OLS will give you one
divided by 0.01 a giant number that's not well represented do you see that
that we're afraid of because right but gradient descent we'll say okay I'm
going to take teeny tiny steps in that direction right so there's something really big
it's trying to get to and it will converge there eventually if there wasn't a regularizer
but it's gonna move very slowly in that direction
meanwhile another direction it'll move very fast so this is something which is also
interesting so you could have a problem and if you run gradient descent for 500 steps you
might never see the effect of this threatened blow up
like it'll blow up eventually in 10 million steps model
uh excellent yes in fact this is true more steps of gradient descent can
sometimes give you a worse model and you might think wait what okay I started off saying that we're going to
try to solve problems by doing optimization gradient descent was a way to do optimization and optimization was
the thing we cared about but now looking at the details we say that wait a minute
by not running that many steps of gradient descent for example we can avoid having something bad
actually bite us okay and so this is called early
stopping we'll talk more about early stopping later but I want to mention it here as part of this General story
but the important thing is that your mind should now be we're trying to expand your mind okay so what it means is that
there's many ways to get a regularizing effect so in that case you're getting the
regularizing effect of of saying I don't want to have these directions that have
small singular values explode in my learned weights but you're getting that effect without ever using an actual
regularizer we're just getting it from gradient descent itself and using it for a small finite number of steps
now I will show you two more ways to get this get this kind of effect
both of which are representatives of things that are very important oh man okay
so I'm like running slow so I'll just give you these they're in the homework um so you can grind them out
cool trick one okay
so one of the ways
that you can that's very very important in deep learning is something called
Data augmentation
so data augmentation says that you have some actual data and you run the actual data through your
optimization to learn something and you don't get an answer you like so what you do is instead of using the
data you have you augment it in some way to get it to behave the end result behaves better
so there's many real world ways of doing data augmentation but this is the easiest one so here's the thing I have a least
squares problem
w foreign
I get a bad answer right like I get things blowing up like OLS oh you're an OLS and it blows up
I would like to get something smaller the weights become smaller how can I do it without actually adding
a ridge regularizer one trick is I can say I'll add a bunch of fake data points
so I'll add a bunch of fake data points in which the covariates the inputs just look like square root of Lambda times
the identity and the output is just a bunch of zeros
so these are all thick fake points
but all these fake points are doing intuitively is they're saying please set the first parameter to be
something close to zero please set the second parameter to be something close to zero please set the third parameter
to be something close to zero you see how that that's what they're doing
so a bunch of fake equations exercise for you
you can just use the OLS Formula
on this problem which is X transpose what happens when you take X transpose
act here and invert it you will see the answer is the ridge solution
foreign regularization the regularizing effect
without adding any explicit regularizer running the optimization to completion
and instead just having big data points this is an example of something which is
very very useful in practice okay what's the other
cool trick maybe I'll
so this trick is well recognized the following trick is less well recognized
so how can you discover this trick first you can discover this trick by saying
whenever you're thinking about linear problems you know think about the SVD
so one thing you know about the SVD is if you take a matrix or take its transpose it tends to behave the same
way vis-a-vis the SPD right so it's kind of duality
so other trick is let's add fake features
so I have my problem X
w approximately equal to
y I will add a bunch of fake features
so a bunch of fake features were effectively every single uh data point
gets its own feature
you see that like every data point gets its own feature so you're the nth feature you're the second feature like
so on if I do this
what'll happen if I give every data point its own little knob that can be
used to tweak it I can definitely fit why do you see that even if originally I
couldn't now I have a wide Matrix a wide Matrix
times some Vector equaling a short thing is always solvable
so now I can solve this I can solve this and then I can just take the W that comes out
how do I solve it I solve it use infinitely many solutions so I'll pick the Min Norm solution
exercise again this is on your homework Min Norm
is rich solution
so
this is something data augmentation is something you will do all the time explicitly and you're you'll know you're
doing it in the context of a deep learning problem because you'll have to add these fake
data points yourself like you'll figure out a way of adding them
big features is a perspective that
uh people have only come to understand very recently in that sometimes when you make your
model bigger and bigger you might think you're doing something very intelligent of adding cool features
but you might just be doing this because think about it all that you care
about it's like a little bit fast all you care about here I put a square root Lambda times the identity all that you
need here is this thing behaves in second moment sense like an identity
right what does that mean it means these things are approximately orthogonal to each other or basically orthogical to each other
but in a high enough dimensional space random junk will be approximately orthogonal to itself
and that can do it too so again regularization by features
also regularizes
ah that is a question which has a longer answer than the four minutes I have left in lecture okay
um right now I see you understand it mathematically the intuitions for this are like there's different levels of
intuition we'll talk about some of it next time but I want to make sure I get to one more thing
okay so one more thing lastly
we've talked about this linear model in some detail using the SVD and so on
how does this relate to non-linear models and the understanding of gradient descent solve the Deep networks and the
like like how do you make the connection
so the way you connect to the neural Nets is to use the perspective which
comes from your choice of where you get it from uh there's people who've studied analog
circuits know this as small signal models for analog circuits right a small signal
model for analog circuit says look at an analog circuit and to try to understand
what it's doing understand what it does to a small perturbation of
something else okay take a very small signal and put it in
how do you understand a small signal model in analog circuits you take the original model and you linearize it
around an operating point and you see how does this model behave
original thing is non-linear there's non-linear circuit elements but in the neighborhood of a certain point it's
going to behave locally linearly because it has derivatives now
from the perspective neural Nets your complicated neural net model
F Theta right of input X
is basically you remember during gradient descent what you're doing is you're trying
you're setting Theta you're going to set data by making small changes to the weights
so the small signal is at the level of the weights the weights are going to
change in small ways so you can take the entire model and you can think of it this way
F theta plus Delta Theta is approximately equal to
f as I'll call it f Theta not F Theta naught of x Plus
the derivative of that function with respect to Theta some big grow
evaluated at Theta naught acting on Delta Theta
see that just depending A first order approximation to the function that we're learning
why is this okay because I'm just going to be taking gradient Steps From the small steps so I only care what happens
in the neighborhood if I take this model then this
namely the derivative of the function with respect to all of my parameters
this is like a generalized linear model
there are a bunch of features derivative features here I can evaluate these for all of my data
points and what's going to happen is that we
can try to understand what's going to happen optimization by looking at how this thing behaves linearly because this
is something where it happened to start I can take this and just take this linear part this
this cons this constant part that doesn't change as I change Delta Theta and just fold it into the Y's
it affects residuals not directions so what happens is that the singular
value decomposition of this object not the features you see
but they're derivatives the derivative of all the with respect to all the parameters
this guy and it's singular values determine the actual directions that your model will
want to move it in gradient descent so if this thing has a high singular
value in some Direction that's what you're going to try to fit at least locally for one step or a few
steps of gradient descent until this thing stops being representative anymore then you have to go to a new new
operating point so
this object turns out to be really interesting and so in the homework you have a problem which asks you to work
this guy out and compute its singular values and compute the so just as you have singular
vectors what's a singular Vector singular Vector represents the direction
that you would move it right so
in the same way the singular vectors corresponding to the derivative of this guy on the training data represents
synthetic features that represent the directions you'll move in you'll move in the direction of
that synthetic feature those synthetic features can be visualized and the ones corresponding to
large singular values are the directions you're going to fit first that's this is the best tool we know
right now to understand a little bit about what's called the inductive bias of a neural net what are the directions
it'll actually learn in so you need to understand this
the okay I'm now out of time by a few couple minutes over so I will stop next
time I'll pick up I'll talk more about it but this should give you enough to start on the homework
um on this part so you should be able to I I can answer
more questions about this online in Ed since we didn't get as far as I would have liked would you guys you guys had a
good question so um that's why we didn't get as far as liked okay I have office hours now but they're
not going to be here they're going to be in 258 Cory uh because there's another
class in here immediately after us
okay let's start so uh I want to start with an
announcement we posted this on Ed but I want to make sure everyone hears it because of the class size shift the
campus rooms we got for the midterm are on Tuesday March 21st
from 7 to 9 pm initially we thought we were going to get Wednesday March 22nd 8
to 10 but this is the time we actually have so this is the time we're going to have the midterm exam
so sorry about the slight change there the other announcement is you've gone
through homework zero and you're probably working hard on homework one uh
so we noticed that that the
attendance at the homework party was not as high as we expected so either people
are you know finding it perfectly plausible and easy to do in groups that
you already have which is great um or there might be some issue with the
timings of the homework parties or something so what we're going to try to do is um we're gonna we're gonna pull we'll announce it but
we're adding a might shift add slash change
a homework party to not be in the evening so we'll have
one that the time we could get it's probably like 9 to 11 A.M on Tuesday on
Thursdays so announcement will come out for this to hopefully give people if they can't
make the um seven to nine PM slot that we were able to get earlier this is a more of a
daytime slot so we'll try to have another one for you guys we might remove
one of the others to keep things efficient um so that's one thing we noticed
um the other thing we noticed is that the
uh the activity that we're seeing uh
on Ed on some of the questions is uh I mean it's it's all good but it's a
little lower than we had expected for some of them in terms of the kinds of questions that people are asking so we
want to make sure that you know if you read a question and you try with your friends you know your study group to to
work on it to do it and you find yourself you know stuck
on something or you don't understand something it is okay to ask we don't want people to do the I glance at the
question I don't understand something that I ask and then five minutes later I say oh I already got it like that's kind of annoying for everyone else you know
but like just and if you really have a question and you want to ask something please do ask you know we we are we can
support the questions that we get um final kind of uh announcement is uh I
mentioned this last time I'll say it again in case some people are missed last lecture uh
the discussions are meant to be quite useful to you in terms of understanding what's going on so if you haven't gone
to a discussion you know please do it's it's important uh for understanding
um we have staff there to support you and your understanding of the material so do take advantage of those
um okay so that's the kind of logistics type announcements that I wanted to make
uh did get some questions and feedback from people that like and I felt the
last lecture I kind of went really fast at the end on things so I'm gonna like slow down and kind of cover it so as a
result I'm Gonna Change what I'm gonna do today a little bit instead of kind of what I initially announced that we might
launch into kind of a broader survey of like architectures and things we'll push that to Monday and today we're going to
kind of Zoom keep going kind of at the level of preliminaries to make sure you understand those
um I'm by in doing this I'm counting on the idea that most of you the idea that
neural Nets are useful and can be used for lots of different problems is not something I need to tell you
however you you're one of the people who really hasn't looked at this at all and doesn't know and you're really really
curious you do come to office hours and I can you know say something about it but otherwise we're going to kind of talk about the survey of architectures
and things like that on on Monday instead of instead of today
today my goal is to recap the story and also introduce some language I realized I didn't say in lecture on inductive
bias I want to then talk about the thing I just I mentioned if I posted something
on the Ed for the lecture kind of talk about this two views of learn features so engage with what you're doing in the
homework um one of the views we'll engage with doing the homework but the other one will engage with something that you've seen in discussion
and then I want to go in to talk about initializations a little bit and talk
about optimizers Beyond stochastic gradient descent which is the one that we've talked about so far I'm just going
to think it's like preliminaries that are good for you to know when engaging with the rest of the class
um okay good and so uh at the beginning here you know
definitely I want to make sure everyone understands everything so
broad I want everyone to understand everything but at the beginning I want to make sure that these things are familiar to people
foreign [Applause]
nice so what's our big picture strategy to
understand neural Nets right the one that we're going to follow and people generally follow is as I mentioned
before we look where the light is so we understand something and then we try to leverage that to understand something we
don't understand so when understand simple linear models
foreign
think about nonlinear models
and the tool that I mentioned last time to do that at the end was
how
we try to think about local linearization
that's how you bridge between linear and non-linear things um why do we do this do we believe this
will explain everything the answer is no we have evidence to say that there are things that are missing in a picture of
what happens in nonlinear models by only looking at local linear linear models this much has been established there are
phenomena we cannot explain this way but we know how to do this and we don't know how to understand some of those things
except empirically so you should know how we can Leverage The
intuitions we that we can back up and then as we go we'll introduce others
so when I say simple linear models in the context of machine learning the simplest one that anchors most of
our understanding is just least squares so I mentioned this before let's talk
about it again I'll use notation that looks like this there's some weights you want to learn there's
some data points and you want to match something so again let's just Mark the types of
everything this is all hopefully review there's D features
there's D weights that you need to learn we have n data points I arrange them in rows for the actual covariates and this
is n long and so we mentioned last time that we
have ordinarily squares
we try to learn these weights by Computing the ARG min
of Y minus X W squared
and for Ridge
we add a regularizing term
so we have the original loss term and we add a regularizing term
so in discussion you saw uh
so in homework you are asked to kind of grind through and think about lots of different perspectives on this
um but in discussion you saw also that this perspective can and versus this
perspective can be understood as from a probabilistic backstory as this is a
maximum likelihood estimator and this is a map estimator
right where the difference between the two is that in the maximum likelihood estimator you have no prior on the rates
W and here you put a gaussian prior on the wage w okay that you saw in discussion
and so you remember how that works is you look at when I say in the get in the
probabilistic backstory you say the reason these two aren't exactly the same is that there's some noise in my
observations that noise is distributed IID gaussian for
and then I want when I say a maximum likely estimator I said I want to pick the estimate for w which corresponds to
the most likely noise estimate that would have given rise to my observations right and then because the gaussian has
these densities that are you know spherically symmetric you get this because y minus x w is the hypothesized
noise right so just again to connect the discussion a little bit between what we said in lecture
and the reason you get this additional term when you have math is that if you
look at Bayes rule you have to multiply the conditional by the prior right in the numerator and you get a denominator
term which doesn't matter for the arc Max and this is coming from the prior right How likely is the W ah what's the
probability of the w okay so this is hopefully you know um both review
connecting to discussion I should understand it this the probabilistic interpretations of machine learning I
believe are standard in most machine learning courses so this should be review
um now last time this is where I'm gonna like slow down because I think some
people didn't understand what I was doing um you have formulas to solve these
so for this right you have the OLS Formula which is
so you can have W hat I'll mark it OLS to tell you
ordinary least squares can just be computed explicitly as
transpose X inverse X transpose y
now for Ridge and the homework has you also engaging with this so it's fine I'll take a
different tack to you know hopefully exercise some other skills for you
Bridge
the formula that we talked about was
X transpose X Plus Lambda I inverse
X transpose y but um you thaw in I think this wasn't in
discussion but it's on the homework um there's a second formula
which goes like this
foreign
formulas for the solution to Rich Okay so
equivalently these two are equivalent formulas
so uh now I'm going to ask people
this question so you this shows up on the homework right uh
how many people I'm gonna restrict attention to people who took um 109 at Berkeley
how many people who took 189 at Berkeley saw both formulas
okay how many people who took where United Berkeley did not see both formulas okay so probably if you didn't see both
formulas um and running at Berkeley it was like you saw this one and you didn't
see this one um okay so how people now I'm going to restrict attention to people who had a
machine learning course elsewhere uh did a machine learning course elsewhere
how many of you have seen both formulas
a couple of you and how many of you yeah elsewhere only saw this one
okay so it's a sizable fraction of people who did haven't seen on this
formula so it's probably worth thinking about it a little bit um
so first let's just um get the types of everything
um we talked about it last time but I'm gonna like do it for both cases again um this Matrix
here what's its size how big is it someone just call it out
client this is n by n d d by D right
it has to be because this is D dimensional right so it better have a d different rows and
so because it has an inverse it better have D columns okay um Y is n-dimensional and so X transpose
Bridges between those two Dimensions here okay here
what's the size of XX transpose plus Lambda I n by n
Okay so
this is sometimes we call the normal form the standard normal formula for
Ridge normal also because this is these are called sometimes the solutions of normal equations
right and this is like in that same style is alternatively this has a few
different names uh in terms of How It's called one is called the Dual perspective on the solution to Ridge
um the other perspective on this is this is called kernel Ridge regression
okay so this is also called kernel Ridge
okay so at this point I want to do so yeah so
it can be thought of as coming from that perspective it's a solution that
it's still giving you a solution to this right but this can be viewed as how what
solution you'd get you took a dual point of view to solving it
yeah so uh if you look at but let's just I
want to make sure that again just because it's good to review some things um
so again I'll ask again for a survey how many people have seen this thing called kernel rate regression
raise your hand okay um
so maybe I'll step back and ask a question uh
I believe that kernel approaches to doing machine
learning are standard in all basic ml courses
um so for those of you who haven't seen this can you raise your hand and tell me how you saw
um kernel approaches introduced to you or yeah
good yeah I
okay so you thought via the kernel approach to the svm now here elsewhere yeah it's totally
fine I mean that's often it was the kernel approaches to the svm that really um transformed the field in terms of
people's um use of uh of Kernel approaches um in practice
um and so it's from a practical perspective it's very natural to think of uh to teach kernel svm gross is a lot
more complicated than this in terms of the derivations um but
it's harder to teach that but practically speaking it's more useful
um anyone else uh everyone else who saw kernel methods
um without seeing this see it acronym methods introduced directly in the
context of the svm yeah okay
um in my experience people people who've gone through that treatment often um I'd say with 50
probability or higher um get lost in the weeds of uh what's going on and
sometimes the big picture gets a little bit lost so I think it is worth kind of for your general knowledge to talk a
little bit about this to make sure that everyone's on the same page um
Okay so why is this called the kernel Ridge
formula why does it have this um like why is this what's the what's the connection
so for this I want to First make sure
that you can see how these formulas reveal something different about what's
going on so they're equivalent in this formula
you see that look what I see here is the modification of
OLS the connection to OLS is very tight here right you can see it's basically OLS
except you have this Lambda I which is like changing how you um weight different things right
so what does this formula reveal about the nature of the ridge solution that is
not visible here so
I want you to observe I want I want to ask you a question when you do this multiplication
what is the type of the object that you get back
a vector right it's a column Vector with uh how big is it
to end right it's an n-dimensional column okay
Matrix Vector multiplication gives a linear combination of The Columns of
this object right that's what Matrix Vector multiplication means
so what are The Columns of X transpose
the data points okay so this tells you something
reveal something that isn't obvious when you look at this which is that the final weight that you
learn is a linear combination of the data points
right do you see that okay
so that's interesting and something which should not be surprising to you and we'll talk in a
moment about thinking about how gradient descent works and how stochastic gradient descent works
okay but it's useful to you to have the ability to understand what's going on
from different perspectives okay it's very important to understand from
things from different perspectives so okay it's clear this now reveals to you that
the rich weight you learn is a linear combination of data points
okay good so why is it called kernel Ridge
so to understand why it's called kernel Ridge we can interpret different objects so first
so we know the interesting thing about this okay good
this guy XX transpose let's just start going over here
what is XX transpose what's its size
n by n yeah good
it's a matrix what does it have in the ijth position
what what's inside this thing yeah it has inner products between
inputs right it's the Matrix of inner products
so it's this aspect of it which is why it's called kernel Ridge
from the perspective of practical use right because when you learned about kernel approaches to machine learning
the general philosophy you learned is anywhere you have a matrix of inner
products measuring you think of that as a similarity measure between inputs
and you say instead of dropping in a matrix of inner products euclidean inner products drop in any other similarity
measure that you want that satisfies positive definitions whatever right um and just use that
right so by having the formula in the form that makes this fact that this
inner products are at the heart of what's going on surf you know kind of pull to the surface you can see exactly
how to do that right here x transpose X is not a matrix
of inner products so this formula does not show you how this might have anything to do with similarities between points whereas this
one does okay so that's one connection to Kernel methods is that clear to everybody
next thing why is this called a why is the screener called kernel Ridge
it's like a five minute review so it's because on test points
the thing you need to compute is of course W hat
Ridge transpose
right this is the thing you have to compute right or alternatively this is the same as
saying X transpose W hat Ridge
right because it doesn't matter the order which I do it so when you do this on test points and
you substitute in the formula this one what you see is you get
foreign so
first what is this object what kind of a thing is it
right what kind of vector throw a vector so this is the row Vector
what are its entries
what's this first entry yeah so X transplus remember consists of
all the different data points so this is the dot product with all the training points
foreign
that's another reason it's connected to the idea of kernels right how do you use kernel methods you take a test point you
say how is this related to all of my training points how close is it how similar is it to all of my training
points I will use that to wait something right
so that's why that's why this is called
kernel Ridge
excellent excellent question right the question is does the same argument go
through for the mid-norm solution the answer is yes that effect so maybe we'll just jump immediately to thinking about
that in a way this thing's spirit is actually like the Min Norm solution this looks
like the more Pandoras to do inverse in this shape right so everything we're saying is yeah is connected
so now in the homework we ask you to show that this and like implicitly that
this and this are the same thing right it's one one part of the implicitly in the homework
so I'm gonna ask you guys to spend a little bit of
time on a piece of paper I would like you to show
that these two are the same except I'm going to tell you a particular tool to
do it right that's probably not the one that you thought of using on the homework
so I'm going to say exercise in class exercise
use the SVD of x to show that
thank you
okay everyone understand the question this thing
and this thing are the same thing they give the same answer
use the SVD to see it now why am I telling you to do this because actually when you
look at these formulas you should uh you should be troubled or potentially
troubled by some things right so let me first tell you what you might be troubled by
if these two formulas are equivalent they behave the same for all Lambda
right which means they behave the same for Lambda going to infinity or zero also
is everyone's see that okay now either
in this case you're thinking of D because the least squares problem D you're thinking of as being smaller than
n right so n by n is bigger than d by D
everyone see that okay so hopefully people are beginning
to feel a little not in your stomach mathematically okay so this D by D
object is like happy and probably full rank right adding a Lambda I makes it more so
all good this n by n object is probably not full rank
right does everyone see that and so adding a Lambda I makes it
happily full rank right but when Lambda goes to zero
this guy is approaching a singular Matrix and you're inverting it
so why is it that this thing blowing up
doesn't cause these two to become different you see that not numerically but kind of
just even mathematically do you see that okay I want to make sure everyone unders like
as you navigate deep learning and machine learning you're gonna need all of your different
intuitions kind of to be in play because you don't understand what's going on so you need all of your senses
to be active and one of those senses that should be developed is your mathematical sense
right your mathematical Intuition or you know Spider Sense like ah I sense danger
you know right and then you have to think you know is this Danger real or
not or something and that will help you understand what's going on so in this case
like if you do it this way you will not get as lost in a sea of
algebra and you'll understand why this is safe okay so like yeah take a couple take
three minutes and try to do it
foreign
foreign
foreign
foreign
foreign
go check in how are people doing so raise your hand if you're done
okay um raise your hand if you're making progress but not done
um raise your hand if you're stuck okay so I'll give you a couple on one one minute one more minute to people
foreign
foreign
I'll just bring everyone back together okay so hopefully you uh are anyone
still working people are mostly done okay good
um what you've seen right is that
as you kind of just use this fact and use this what these
objects are like right so what happens is that when you apply the SVD
you just change the SVD coordinates so all the coordinate Transformations just fly out of the
inverses and everything together and then you get everything just all the things will cancel out right and you get
a story that's essentially entirely in terms of Sigma and you look at inside here you have a
sigma transpose Sigma and here you'll have you know a sigma transpose and you
look here and you have a sigma Sigma transpose and then you have a sigma transpose over here
but Sigma transpose Sigma and sigma Sigma transpose essentially are just different size things that look
identical inside you have however whatever the rank of X is K you have
that many things on the diagonal and you have zeros everywhere else and so what ends up happening is that
when you add a Lambda I to it you just get a Lambda I Lambda added to all the
diagonal and we take its inverse you get identical things for both of them just different sizes
right then the key is that
the only thing that ever comes out of this
right is what happens to why hitting
those uh singular directions that are actually represented in x
right so X so
the singular Vector is V are the ones that correspond to non-zero singular values are the ones that span
the column space of X transpose or the row space of x
and that's where your data points are and that's the only thing that ever actually moves through this thing so
anything that's happening elsewhere doesn't matter so even if this thing is hypothetically blowing up
as Lambda is going to zero the directions in which it's blowing up never make it through X transpose
which is why that's safe and why these equivalent sold okay
so hopefully you got um something out of doing this exercise to
understand why these are equivalent also this should confirm to you that when in
doubt use the SVD to understand what's going on this should be a lesson that
has been hammered into you from earlier classes but you know that's why because
we don't know we know how to understand it it makes things we can understand what's going on so it's useful here too
any questions on this little exercise and why this is equivalent and so on
okay so I think in the next homework we will put something in to further deepen
your understanding of this object um
so in the Galaxy I'll just mention it briefly
in the gaussian setting if you look at this this thing right this is like map estimation
for things that are gaussian with a gaussian prior on W and gaussian noise in your observation and X something just
some data points wherever they are um it's an interesting fact that for
gaussians the map estimate is also where the mean is
so you can look at a conditional mean this is hopefully a fact so if you look at a conditional gaussian a gaussian is
maximized at its mean right so gaussian density is maxed at the mean
which means that map is the same as estimated mean to same as mean squared error estimation
so it turns out and you'll be asked to do it on the homework is you can view this as actually just an
MSC estimate using a standard MSC estimate formula for gaussians again different
perspectives are good to have to understand what's going on
okay so hopefully all this is a little bit clearer to people understanding
Ridge understanding what's going on so just one comment here
is if you do this you can also view the
Ridge formula in sort of outer product form so if you just grind through this
calculation right I don't know if everyone did this uh
Ridge
foreign
product form I don't know how many people when you were cheering this is equal went all the way to something like
this for both sides yeah one person okay it's a couple people yeah you can just this
is like another nice way to see what's going on shows you why this is a linear combination of the things expressed in
the Subspace of this and how the singular values show up in doing
this good so now
I want to say a little bit of what I was saying
before except it'll be a little slower so
there's a term that we use which is inductive bias this is a term from
philosophy and what it means is that we're doing learning and we want to see what kinds
of things will be actually learned like what is our bias what kind of what kind of patterns do we actually learn and
what I told you before is that uh is not enough to think about you can
learn any pattern you can express any pattern using your neural net you want to know what are the actual patterns
you'll actually learn from data so that's where the idea of index that's where the term inductive bias comes so
last time we talked about how gradient descent has an inductive bias
let me just close that off by itself gradient descent has an
implicit inductive bias that is not
misaligned to Ridge so if you just look at gradient descent for OLS we did this last time but I want
to make sure when I went fast so I want to make sure that everyone understands it let's look at OLS right what you have is
the update the next time for your weights that you're learning is the
weights you learned last time plus I'll just use two because I didn't
put a half to Ada X transpose
y minus X w t the residual
so remember we said that one of the nice things about the ridge formula is that it shows
you that the Learned weight is a linear combination of the inputs
looky here the update to the weight is a linear
combination of the inputs so if you start at 0 for example and
always add only linear combinations of your data points where are you going to be some linear
combination of your data points right so that's nice to observe I didn't talk
about it last time but you can hopefully see that here
so we said that if you look at this in SVD coordinates
so just this is gradient descent for OLS if you do stochastic gradient descent where you
only use a batch what changes what changes is this
and this right your back is smaller so you look at the residuals only on a smaller set
of data points and you only look at those data points inputs but otherwise the formula is identical
so useful fact SGD also only moves in the Subspace
not just gradient descent SGD also only Moves In This Subspace
okay so but going back to gradient descent because there we can think about the SVD but we said in SVD coordinates
and we said it last time but I want to let you guys ask questions about it so let's look at the ith position
in SVD coordinates this is equal to
the scalar is a scalar now I look at the ith coordinate
in SVD in SVD coordinates um to Ada and now everywhere you have an X
transpose you're going to have a sigma transpose this diagonal which means you can just pull out the relevant Sigma
Sigma I and then the scalar residual in this coordinate
so this will be talked about in the discussion because it's super important I make sure everyone understands it
and you get this kind of thing which
I can also combine these terms
one minus two Ada
Sigma I squared
foreign
that formula down and I wrote it out as a recursion in a in a form that made the relevant eigenvalue of the Dynamics very
clear okay so last time I talked about this but I want to make sure that everyone
understands it because I think some people didn't get it um
in SVD coordinates you can see that what's happening is uh every single singular direction is
doing its own thing there's no there's no coupling at all um if you're doing full gradient descent
everything is converging to where it should go and all of them have their own evolution is that clear to everybody first of what
the implications of this are we wrote that last time I want to make sure everyone gets it clear
okay now our learning rate Ada step size is some
number so look at this
suppose Ada is small some small thing
for what kind of singular value is this quantity
close to one one minus two ETA times Sigma I squared
when is this close to one when Sigma I squared is close to zero is
that good everyone's good with that okay if so what's the other interesting thing
this could be like so this thing has to be stable so it's somewhere in the you know unit circle
everything is real so you don't have to worry about real numbers okay so there's two interesting places
the boundary close to one that's when Sigma I squared is small
um what's the other interesting thing this could be like zero right because zero is super fast
right this is this thing is like something which will Decay as you exponentiate it so it discovers the
speed of convergence cleared everybody like if you look at this the evolution of this kind of thing
there's some mathematotic value you'll tend to
and then you'll go to it like that right and how fast you go
the time constant associated with this is tied to this guy right so you go like this
and you'll go faster and faster the closer this thing is to the whole thing is to one what does that mean it means
that large Sigma I squareds will converge faster
and so hopefully you know this other fact that if you have exponentials
uh from the perspective of different exponentials
one of them has already completely converged by the time the other is like moving
right it can use it asymptotically right exponentials like converge much faster than each other if they're going to
converge different rates so what this tells you is that when you do gradient descent the inductive bias
of gradient descent itself is during the training path to First fit the largest singular
directions so you first fit to the Subspace of the largest then the next then the next then
the next then the next so it's as though you are smoothly doing
an ordered fit to subspaces
you see that so gradient descent as it goes is selecting so if you think about the
Subspace the Subspace is telling you the implicit complexity of that model how many degrees of freedom are there
like how many how complicated is that model if you're fitting a Subspace with one vector you're fitting like a single
Direction so gradient descent has an implicit bias
right an induct implicit inductive bias that says the path of gradient descent
will walk through a sequence of subspaces a simple model then in a more complicated model then
the next more complicated model then the next more complicated model then the next more complicated model and it'll
keep doing this as you go this is if you just do OLS
now notice here in the ridge formula
what Lambda is doing is also effectively cutting off subspaces
so it's also doing that so you adjust the Lambda you're smoothly saying well I want a model of this complexity and not
larger is that clear to everybody
this is important to understand so gradient descent because of this
behavior of exponentials and time constants also does something like uh
offering youth as gradient descent is running a choice of models of increasing complexity as you go
an alternate view of this is to say when Sigma I is very very small
right this thing is very very close to one and when this thing is very very close
to one you can think about the whole thing as being like an integrator
and just integrating this guy adding it up at a very small slope
and so why very why very small slope because here's the sigma I in here so Sigma is small this thing has a very
small slope and it's as though this is just one so you're just moving very very slowly
like this at some point you'll see the exponential curving and asymptoting
but after a very long time if you're moving very slowly so this is why
great so we give you a homework problem where we say gradient descent doesn't go nuts
right and there we wanted you to do it using coarser tools but this is like a more refined view that shows you
gradient descent doesn't go nuts directions that are not very well represented as the
learning rate is small enough we'll basically not even show up in your model
okay any questions on this yeah
excellent questions the question was when in practice will you ever use the closed form Solutions and when will you
use gradient descent and you know just track and stop early whenever you need it depends on context
to zeroth order like the basic thing in when using deep learning models you will
not use the closed form solution of Ridge you'll always train everything using
gradient descent okay that's like the zeroth order View but
it turns out in some settings you want to use the
formula because it's a heck of a lot faster if things aren't too big to use it
the number of steps if you if you actually want to get the whole solution and you're you're cool with that getting
the actual solution and you don't want to cut it off if you know where to cut it off you know what the Lambda to use
is then sometimes this can be a lot faster if D is small
to do this so sometimes you want to do that
here's the other cool thing we'll talk about it later this formula
can be written in pytorch this formula is itself differentiable
so sometimes you can use the formula itself as a layer
and say okay go ahead and take derivatives through it
and accelerate certain aspects of what's going on and bring just the focus of
your model so this is another aspect another analogy which is useful to think about
we think about inductive bias we want to think about what kind of stuff does our model learn
and the classical perspective on this um sort of before the modern era which I'm
going to date as being 2012 onward um okay what kind of things can you
represent but when you when the focus shifted to understanding well what kind of things do we actually learn you need an analogy
so one analogy is sometimes thinking about how you take pictures
and you can think about the learning the model and the optimization as being like
a lens or Optical apparatus and what they do is given data they
bring different patterns into Focus and basically think about your analogy
of lenses right as you adjust the spacing of the lenses in your Optical system different planes will be brought
into Focus on the thing you want to capture in that same way the adjustment of the
model the optimization algorithm and things you do are bringing different
patterns into focus and that's where that's the name of the game of DPL so we have all these architectures why we
adjust all these things it isn't about what the model can express it's about what that model brings into Focus
this is one perspective another perspective related is the kind
of musical instrument perspective which is closer to what's going on mathematically
so a musical instrument resonates in certain ways
and so you have a bell you hit the bell with an impulse but then it rings
at a particular tone right so
remember Spectra are called Spectra it's called the SVD
right and related you call you Call these things like related spectrum because they are like frequencies
and so the fact that these that gradient descent will sort of for a model will
fit the high the high energy Parts first is like saying the model resonates
at those things and the data pattern that comes out comes into it excites the
model but what you learn is the thing the combination of the bad thing that comes in and where the model resonates
so that's another perspective on what's going on in deep learning is you're crafting a kind of musical instrument
and what you wanted to do is to bring out a certain quality of Sound by
resonating that way this is different from the
um classical view of what matters is constraining capacity of models this is not about the capacity
of models it's about their inductive bias okay so these are additional analogies
that are useful um to have in your repertoire of how to think
so usually like in many classes you're told this is the right way to think about X unfortunately we don't know the
right way to think about everything so I'm going to tell you lots of different ways to think about things because you have to navigate between them
okay so important thing is that kind of closing out this part about gradient descent in OLS is because the gradient
descent will go through effectively a sequence of models that are different
early stopping looking at checkpoints of your model as it's trained and saying hey how good
was that one and deciding to stop and use one of those instead of the final converged model the reason that's useful is because it
has the same regularizing effect this is a very very important thing in Practical deep learning
you'll often do this you won't just train a model to convergence then use the final thing you'll look at
checkpoints along the way okay so this is another kind of regularizer
okay any questions on that foreign
now we spent a lot of energy and time kind of thinking about
these linear models and we did this to sort of hopefully help you understand
uh that there's something about these linear models and the inductive bias
which is really important the MSB thing the most significant thing
that then seems to happen is what is the singular value structure
of in this story so far of this thing X
what is X x is your Matrix of inputs
of your features right so it's how your features like show up in terms of their singular
value structure that determines what these kinds of algorithms will learn okay so that's the lesson we've learned
from so now we need to have a way to bridge to non-linear models
deep models and for that we're going to use this local linearization point of view which
I introduced at the end of last lecture I wrote something in the ad for but I want to make sure everyone understands it and we'll talk about it in the
context of example I think you did a little bit in discussion also
related to this so let's move this over
then I want to get into initial that will then help us understand something that we want in initializations
okay so
a general model it could be deep this is like following what I wrote
is I have a bunch of parameters Theta I act on inputs and the model emits
something I want to learn these
okay this is like our story I'm going to do gradient style
algorithms so the important thing is this General model is deep and non-linear
foreign
or non-athlines we mean it's non-linear in what it's
non-linear in x the input
y so you can have expressive power and actually kind of capture the patterns you're interested in relative to the
inputs right you don't want to just be a non-linear function of the actual input
but it's also non-linear in Theta
it's not just a linear function of the Theta it's a nonlinear function of the Theta non-linearity in Theta or
non-alphinedness and in Theta is inevitable whenever you have a deep
Network because in a deep Network the sum layers are inputs to the next layer
and so if the layer is doing something non-linear even if the weights were acting in an
affine way before now they're going to be non-linear coming out so generally you'll get
non-linearity so now you want to learn Theta using
training data and when you talk about a local linearization right we're talking about a local
linearization we say let me look at f at Theta naught plus some small variation
around it on generic inputs X
and we'll say this thing is basically the function at
wherever you're thinking of the Anchor Place being acting on x
plus this derivative
evaluated at beta not acting on
Delta Theta this is local perspective this thing
is par f Theta 1 you know or Theta one first
first first parameter
all the way out to however many total parameters you have in your model
right it's a big thing filled with the partial derivatives
so if you look at this approximation this
approximation is has only learnable things like as you do a
step of gradient to center a few steps of gradient descent the only thing you're actually going to be changing and
learning is this these are the learnable parameters here
locally learnable parameters
this is just where you're starting right there's nothing locally that's
changing here of course you're changing Theta as a whole but you've decomposed
it this way to have this local versus uh like where you're anchored split
so in this after you do this approximation what you get is a generalized linear model where these
guys are the features
foreign X because each of them takes an argument X right because
it's a partial derivative of this um and but you use them
by taking a linear combination so now the model is a generalized linear
model locally it's not deep anymore at all it's flat
it just has a width of d different things that you're learning
weights on okay I posted this uh on as I mentioned it last time but I
want to make sure everyone understands it any questions about this picture
ah sorry so I'm just saying this thing is these derivatives
these are all what these are all functions of x right that's what it means to take this
partial derivative so they're functions of x
L functions of X we call features if we're going to use them for learning right that's what it means what's a
Feature Feature is some function of your input it could be the seventh coordinate that's like a trivial kind of feature or
it could be like the add all the energy or could be any kind of weird nonlinear combination of things so it's like a
lifting it's a lifting of x so you can view these as being features
so this points to something really important which is that when you have a
deep model and someone asks you what are the features your deep model has learned
there's actually two different points of view on what that means
one point of view is the point of view expressed by this expansion
it says well however it is I've trained my model I've learned some Theta not if
I'm going to view this model as something which acts as an initial condition as a starting point for fine
tuning which is how we use this modern pre-trained models today
if this is going to be used for optimization and I'm going to allow myself to tune all those parameters
then the features I've learned are actually these guys
because that's what the optimization sees the optimization just sees these guys because optimization is acting locally
and taking derivatives so it doesn't see anything else it just sees these
and so it's going to be this is D of them right
it's going to be the singular values and the singular vectors associated with
this lifting on your training set that determines what kinds of stuff are going
to be brought into focus by gradient descent or not
do you see that okay so it's this thing that determines
these derivatives so these derivatives interestingly are also what pytorch
calculates right Pi torch calculates what is the
sensitivity to you know your model with respect to a certain parameter by doing
back prop so these are things that pytorch is also calculating for you all the time
because that's how it does the optimization so this is one view of the features
so these are let's call them the local features
for learning
now there is another sense which is not unrelated to this sense so I'm going to
draw a picture for that to make sure you understand that one too so in a deep Network
right a deep Network looks like this I'm just going to draw it I have some input X
there's some layer right it has some weights it has some biases it has lots of stuff inside of it
and it has a bunch of outputs
those go into the next layer which also has some weights some biases
and some maybe different number of outputs right and then it goes into another layer
right which has its own weights
and his own biases and so on until finally
at the end game of this the end of your neural net how do you like cap or neural net at the end what happens is I have
some set of some weights some biases I have some set of outputs that come out
and then I add a linear layer
that generates something it's usually a structure of these networks a bunch of stuff happens
non-linearities red loose whatever stuff are happening in here then have a linear layer and then something comes out then I
attach this to a loss in some way right this gets attached to a loss
for training is that cleared everybody
okay so this linear layer has its own features its own weights associated with
it let's call them weight linear
one perspective a different perspective on the features learned by the neural
net is to say that what the neural net has learned is actually these different features that correspond
to these outputs right before the linear layer so you can call this the last layer and this is like the penultimate
layer so the outputs of the penultimate layer the last place where non-linearity
showed up right those are also viewed as some as the features that
the network has learned because after this point is just a
linear model you see that there's like two different perspectives this is saying that when I look at the
entire thing I have a bunch of non-linear stuff which is some kind of learned lifting and then I have a linear
model at the end his other perspective
says actually I have a local linear model
where I have D different features nonlinear features corresponding to all the weights and
biases so there's lots of different sizes these two different views
so this view is also a view that you will see represented and so when you use a
deep model and you freeze a bunch of weights and you don't train them at all and you just
train a new linear layer at the end that's this that corresponds to one perspective
and if you tune all the weights in your model that corresponds to this other perspective
so there's two different perspectives right one is this local features for learning perspective
and the second one is penultimate layer
the outputs of your penultimate layer are the Learned features and this says the learn features are these local derivatives
does everyone see how these are two different things they're like different numbers of things very important like
they're completely different like not they're not just like D things but they're different it's like
this could be like 27 and this could be like a hundred thousand right so it's like two different size
objects okay yeah question
ing a model that's been thing right there so when people say they're using a model
standard deep learning terminology is they say I'm using a pre-trained model
if they say I'm using a pre-trained model and they don't use the word fine tuning then they typically will mean I'm just
adjusting this linear layer they say I'm using a pre-trained model if they say I'm fine tuning a linear a pre-trained
model then they're often adjusting everything we'll talk a lot more about this later that's a major topic for the
class that's very useful but now this helps you see like what the difference is
but I want to connect the two together to show you that they're not entirely disparate
so associated with this linear layer are a set of weights
right what is the derivative of this with respect to these weights remember
linearly means that a bunch of weights and I add them up so what are these local features
corresponding to that linear layer
so let's write it out this function f at the end is a sum over
I equals 1 to 27 however many things there are here right of w i final layer
times GI
of x so what is the partial derivative of
this with respect to W3
gi3 so in this set is included these outputs
so this is the subset this perspective is a subset of features
in this strict subset
so these are also derivatives because the derivative of something
linear is just the thing that multiplies it right that way which is actually the output
here
okay is this hopefully is much clearer now uh to people what's going on here
so now for the case of a relu network you can actually compute what these guys
look like did you guys do this in discussion
right did you plop them right and you see that they have interesting shapes right they're not just like
you know Corners elbows like some are some are discontinuous right they're
like constant zero and there's something that corresponds to bias right and then
some are interesting where you have uh you know you move then you jump and then
you go so these are these are much more interesting shapes than just what you
get with the railers and the homework has you also doing this and the next homework will have you doing more of this
so this way you can get a sense for what's going on here but uh yeah so
the if you want to understand fine tuning you need to know what are the singular value structures on your
training data of these guys if you want to understand simply using a pre-trained model and what it will learn you look at
the singular value structure of these guys and see what I learned would be great in
descent okay so I have gone slower than I
expected but that's fine because I have Monday to catch up um before discussion
um so yeah what I want to say next is that once you have this picture
you can now begin to understand if you're going to look at initialization so one thing is to look
at this after you've trained it but the other thing is to make sure you can train it to anything at all interesting so that requires to understand how does
this thing behave at initialization so it's very important to understand at initialization what this guy looks like
and behaves like and that's the thing that drives the choices of how you randomly initialize
your network you want to make sure this thing will behave nicely
it won't go cuckoo on you and that we'll talk about next time and
we'll talk about optimizers next time okay I have office hours now any
questions on the stuff on the board I can answer right now and then we can move over of
okay let's I guess started stuff is already on the board so that's good they can reuse it
um so uh
I want to make sure I repeat the announcement very important announcement the midterm is Tuesday March 21st 7 9 pm
so that's the room that we got for everybody that fits so the goal today
is to you know try to catch up some more um to talk about initializations and optimizers and
then to move into a little bit of a high level survey
of the actual neuron that things we're going to talk about in the class
I want to talk about this first um
so uh remember our goal in you know working with deep neural
Nets is to achieve a certain performance we want to do well on a certain task
want to learn certain kinds of patterns from data to learn those patterns we
have to have the combination of so you need to need to design
in a sense a triple right we have to have a network
a circuit that we have two has tunable parameters that we're gonna hopefully be
able to in encode the pattern into that it'll be able to take data and encode the pattern into this neural network so
that can generalize but in addition because we have to use an Optimizer we do some optimizer
design slash choose some Optimizer that's going to actually
be able to take the data and tune the different parameters so that
you know you got a good fit why do we do this is because fitting is what we know how to do
and but you have to be able to start your Optimizer somewhere so you need to
have initialization
so the combination of these three you expect you hope will work well
so you want the combination of these three to bring the right kinds of
patterns into focus is the way I described it last time so it's not enough for the neural
network or the circuit to be able in principle to express the pattern that doesn't do you
what you need you definitely that's a necessary condition right if you can't possibly Express the pattern at the
Fidelity you need well then you're hopeless but it's not a sufficient condition
Because the actual initialization and actual Optimizer when faced with the data has to actually be able to do it so
I will augment this a little bit to say Optimizer right and training strategy we'll talk a
little bit about some stuff for the training strategy later
right this is like the big picture which you have um so uh just a preview of stuff that's
going to come later is sometimes our training strategy uh could involve uh doing things
um differently we already know it has some things like choosing the loss function for example but also includes
things like doing data augmentations during training it also includes things like
having parts of your network that exist only during training that don't exist
when you're actually going to use it they're just scaffolding to help you build so think about a building right a
building isn't put up there's lots of construction going around Berkeley a building isn't just put up
um just by itself right you also have stuff around the building scaffolding that helps you build the building so the
same thing exists in New Orleans we'll talk more about that later but big picture is this the stuff you have
to do um we talked about it before so it's already on the board that what we understand is simple linear models and
now we have a way of thinking about non-linear deep models by view of a local linearization
so at the last lecture we talked about how basically you can view the model as
having for the purposes of optimization that it has this local view of what the
features are and it's acting like a generalized linear model in terms of what's where the where the optimizer
will be focusing its efforts you can say for the next part
so you know and for simple linear model these things like you know least squares
and again a part of the training strategy
um aspect of running the optimizer as we talked about last time about early stopping that we can run the for free
we can run the optimizer and take checkpoints as we go of the network that's learned and then
at the end instead of just choosing where we converge to at the end we could choose any of the checkpoints we saved
and that can often be useful because we said last time that what's happening is fundamentally when you think about how
gradient descent at least fits models is it fits the
if it's the largest singular values vectors first
then the next ones and the next ones the next ones and so implicitly it's going through a sequence of more and more
complex models that's fundamental to how gradient descent itself works so because of that you get to just by writing the
optimizer and taking checkpoints you get a kind of view at many different models
instead of just one trained one and you can pick it the advantage of this what I said for free is that if you think about
instead using something like a ridge parameter or some explicit dimensional reduction that would involve a hyper
parameter search you have to retrain the network using different values of this Piper parameter and that costs a lot of
wall clock type so whereas this approach of early stop being just uses one run
and different checkpoints so this is the stuff we talked about last time so now
what I want to do is I want to start thinking about initialization
so for initialization I'm going to start with the model we had last time
that we talked about about the simple one-dimensional value model let's draw it here
so what are things you might want to do for initialization how do we think about it so consider an example
or motivation so the real example we had was very simple it was you take a scalar input
you multiply it by weight you add to it a bias
you put that into a relative unit which is
Max zero what comes in and this is another linearity and then you look at
the output and we saw before that this thing as a function of x if I
think of this as y it behaves like this
either you have a corner like this
or sorry it's a straight line
straight line or you have a corner like this
and in either case the location of this corner is
minus B over w okay so that's where the essence of the
non-linearity exists for this particular little setup so you know notice on this side it's
looking linear on this side it's looking linear that's where you kind of feel the effect of the nonlinearity and we mentioned
before you know you need this non-linearity to be able to gain the expressive power to express interesting
things without it you know you just have linear functions only
so to understand initializations let's first understand uh what would happen if
we initialize this weight and this bias to just be normal 0 1.
insight
foreign
ly so if you do this uh the interesting thing to look at is
where is this corner right does everyone see that because that initialization in the corner you
ran these you know you played it in the homework you've you've seen what happens you have these little you know corners
and then you know like the sides this side changes during training and the corner moves around but you know the
initial placement of the corner is kind of interesting to know right so
how is minus B over W distributed
and we won't do the math to calculate this distribution it's a standard distribution so this turns out
kaushi
so the PDF I'll just write it here
of this corner corner
turns out to be so you know where is the placement of
this corner it's called C pi 1 plus
c squared what does that look like
well it's peaked at zero and it
goes down like that
so this particular PDF tells us where that corner randomly is going to get thrown down and where it's going to show
up so it's useful and interesting um to
think about well like let's recall some properties of the kaushi distribution that might be interesting
so if you look at the koshy distribution and you say well what's the properties
you see what it looks like it's definitely symmetric about zero so you could ask what's the mean
right and it turns out the expectation of this thing's mean is undefined
right so the question is actually you know undefined it doesn't converge
so if you've never done this as a side
exercise I would recommend that you take something like the Koshi and simulate it
and track the running mean right just like you would say what does
it mean for the expectation to be undefined because you've never seen this before like what does that mean I take
any I this is a distribution it generates numbers that are between you know minus infinity and plus infinity is
never actually Infinity so I can always take the average of you
know 100 of them or a thousand of them or ten thousand of them like the computation of these averages will
already exist so what does it mean for the expectation to be uh undefined well
you should see it for yourself uh I think Wikipedia article on because she
actually has an animation of this but you should if you haven't done HBO might want to do it yourself just to believe it um yeah
probability we're very close because they're on the highest yeah yeah so we'll talk about that so
the probability is that this thing is going to be somewhere near zero but if you compute the expectation you know you
won't you won't get anything and the reason you won't get anything is you'll end up having to do an integral
that looks like this
great are you taking expectation like that now if you look at this integral and you
looked at it for like one of the halves you might look at this integral and say
wait actually I know how to do this integral right you can do this in a real close form
right I'll leave that to you uh but if you look at this what you'll see
is that asymptotically the denominator behaves like B squared
right and the numerator behaves like C so asymptotically when values are big
this thing is behaving like 1 over C but one over C the integral diverges
so the integral diverges on both sides so this thing tends to Infinity slowly
but it does tend to Infinity okay so this thing doesn't have a mean
which means that you know it's heavy tailed right you can see it's a polynomial tail um so it's heavy tail so in this case
doesn't even have a mean so it definitely doesn't have a variance right it doesn't have any moments
um higher than or equal to one so you can still ask well sure it's still a
distribution though so it still puts its probability Mass somewhere right it still has a CDF and everything
so it turns out for looking at this that if you go out I picked a couple of
numbers at random to -10 and plus 10.
in between that right this contains
at least 94 percent of the
corners right so although this thing has a diverging
uh you know expectation it still means that actually the
action you know the the corner is going to be between minus Ten to Ten you can go you could pull in much closer right and look at minus three to three
oops just as a way to you know get some
intuition and between minus three and three this is 80 percent
foreign
look at where these nonlinearities are so this actually instantly tells you some Concepts you know we'll also will
come up come up in deep learning that's good to know so first is hey actually
whenever you have a relu network that as well as in it and you have random initialization
um it could be that all of your data that you have is all living like here
right like your data is whatever it is and we'll talk about this in a moment your data is wherever it is and
that means that some of the relus will be like have a corner that's out
here so they'll be like initialization do you see that because random right
where these things fall and you have you know tens of thousands of them or more you know 94 fine 94 but six
percent is still six percent of it's thousands of units right are all
they're looking like this so during gradient descent and during sarcastic greeting descent they always
just give zero did everyone see that so these are what are called Dead relues
right so when that happens they're called dead so you could be like this or it could also be you know the other side
so a relative that on training data always shows up as zero is dead it will have no gradients it will never
undie so
okay that happens for other networks but even the ones that are the counterpart of this right that look like
they might as well be dead right I mean they're okay they're not dead they'll have a
gradient but to actually have them do anything interesting you would need to have the corner actually move such that
it's actually different for that someday training points are on different sides of it but it might never do that
so in that case this kind of value will just always behave like just some linear thing
but it won't contribute any interesting non-linearity and so all the relatives that are in that kind of state will be
you know their own uh you know different kind of uh object
where there's just a linear thing that's happening okay so these are all things that can happen in threadlets
now one of the well you know why do we care about this
we care about this because we definitely want it to be the case that from a
pragmatic perspective if you're going to have a local gradient descent type Optimizer learn an interesting pattern
that we don't want all of the inputs all of the training data to be out here
right at the you know past minus 10. do you see that because if you do that you're going to start with almost
everything being dead or effectively that not interesting patterns
so what you want a desirable kind of quality kind of intuitively is to have
the input that's going into one of these units kind of be centered around zero
and have most of its probability kind of in the near neighborhood of zero
does everyone see that ask a question if you don't understand
that yeah
yeah so what you want to do is you want to think about your your input to like
so remember we're trying to understand what's going on in a deep neural net but that's our goal so in the Deep neural
net you have many many layers but all the time you'll have these non-linearities and stuff like right in front of them weights and biases in
front of them so we want to make sure that as we're going
that if we need to do a mean shift or something that at initialization things
should be at a reasonable place so there's two parts of it there's what your actual training data is
and we'll talk about the normalization or standardization of your actual trading data later like it's fine exactly so you could for example take
your actual training data the things you actually get from you know observed from nature or something and change units
so that there's zero mean and you know for example you know standardize the variance to be one across your data
um yes you could do that that's a very reasonable thing but the problem is that in a deep neural net so I'll draw a
little picture of it recall what this class is about
if you have layers upon layers of stuff
right
and so even if you have standardized the X coming in
from the perspective of grading descent and learning this thing is the input to this guy
and so if you don't initialize properly right what will happen is that this is nicely standardized but then this isn't
everyone see that and so the same problem that we talked about will happen not here but here
like everything here will be a dead relu or we'll be hitting like a linear thing in which case maybe everything passed
here all this depth that you were adding to get certain things into Focus or to do something interesting all that depth
is just like one big linear layer effectively right if everything is sort
of you know dying over here
great question the question is can you standardize at every layer we will be talking about that and we do do things
like this um as an alternative thing we'll talk about that we talk about contents because required to make modern contents
work is stuff like that so we will we will do stuff like that but another thing we could also do is to say that
look at initialization stuff should behave reasonably that the main work shouldn't be coming from the standardization
are those about that random absolutely a bad thing because I think uh they would
be like you know the dropout so the good question the question is are the dead resus bad or good
right it's always good to ask if you see a phenomena you have to ask you know is this thing a good thing or a bad thing so uh from the perspective of
implementation and you know wall clock time and
um you know how many meals you can cook on your server while it's running um the dead redues are a bad thing
because their computations are being executed to compute for them like a
whole bunch of numbers are being multiplied by weights advice is being added and then it's compared to zero and
it always is less than zero so a whole bunch of compute is wasted
from that perspective dead values are typically viewed as being bad things but you could ask is does the presence
of a lot of dead red loose act in a good way for your network like in some regularizing way and
the jury is kind of out on that um in terms of people aren't really sure
they mostly to think it doesn't help um dead railers have another since we're talking about dead Brothers
I'll mention another thing about dead red about dead Rattlers is sometimes stochastic gradient descent
so you have this homework problem right we worked out how does gradient descent work on this right in terms of like
moving this corner but sometimes stochastic gradient descent type algorithms can because of local
randomness um in the batch that was selected
could generate a gradient and if the learning rate is high enough it could kick the relu
in such a way that it becomes dead okay like I said sitting there isn't a
reasonable place but some weird thing happens with the batch this thing gets kicked you know especially if you're
using an Adaptive Optimizer it gets kicked in some way and Bam now it's dead
problem is when that happens it's going to stay dead
so there is this problem that people have observed in real neural net training when you train for a very very
long time is that slowly Ray Lewis will start dying
right it's randomly they'll die they'll die more will die more will die more will die more will die and some of that
might be a good thing like it might be actually removing degrees of freedom you don't need but people have observed that
sometimes performance gets worse because now you've too many of them have died and uh
so it's a really good question to think about what happens with dead values um
but yeah this is you have to be aware of the phenomena so notice that I'm talking
about the relu non-linearity because it is in some sense the easiest to understand
um intuitively because it has the non-linearity constant at one point but a similar kind of concern of wanting
to keep things in some appropriate Center applies to all the other non-linearities too
right so if your non-linearity is something like has a sigmoidal shape
right then something that saturates then the problem is if you're too far out
where it's saturating you're going to get extremely tiny gradients and you're basically not going to move it might as
well be dead okay so this kind of concern exists um
in all uh non-linearities that you have that you work with that you want to keep
things kind of so we want the action here
and so one of the approaches that we do if you want to think about
some kind of object which a random object which is
hangs around zero and is kind of of some reasonable um width our default thing to think
about is normal zero one right so we think about hey maybe we want to have stuff have mean zero and
variance one I want to give myself more space you all
can remember that
I'll just draw the picture again here I'll draw it here and we keep
at initialization
now we don't really care about the fact that it's normal we just want it to be kind of contained
the only thing we have easy abilities to work with is to think about you know
scaling and things like that because we have a network that's built out of you know weights and biases so the picture
I'm going to draw so I'll just repurpose this into a boundary
um is remember we have a deep Network
okay so I'm going to zoom in you know somewhere in this story
like here
so here we have the preceding layer
in the preceding layer has some stuff okay
followed by some nonlinearity
it could be a relu it could be uh you know something that's a sigmoid tan H
whatever nonlinearity you have there's something back here and the out by convention the output of the
non-linearity is deemed the output um in for standard multi-layer perceptrons now you just have a big
graph a big circus you have to decide what's the layer you say the output of the non-linearity defines the boundary of the layer okay so this is the output
output of previous layer
and you have many such things right so many such outputs
and then we have the next layer okay this layer
and this layer we start with a bunch of weights so I'm going to look at this the top one
so this gets multiplied by a particular weight this also gets multiplied by another
weight and so on
all of them get added up and to this a bias is added
some bias and then this enters the nonlinearity
right that's how these networks are put together right and so on it repeats for all the others right so this is the next output of the next layer
everyone good with the picture zooming in so what I want is I will make the
Assumption I'll say for starting off I'll make the assumption that suppose
the output of the previous layer is the input to the next suppose I've already achieved the output
of the previous layer is already like normal zero once a bunch of normal zero ones
and I want to make sure that what I actually see in the neighborhood of this non-linearity
is also like normal zero ones okay
what do I need to do so I focus on this point
and I say look all these weights are going to be multiplying a bunch of numbers that are all kind of in the
neighborhood of you know zero in a nice neighbor to zeros I said there total
uh variance is something that's like you know one
so I want to multiply them by things add them up so that I get something which is
also in the neighborhood of zero right so what can I do how can I
initialize these weights so I'm going to make the Assumption for
initialization that you know I'm going to assume that all of these
things are uh you know whatever they are right these things I'm initialize is
going to be independent of each other so I would like to have some this thing
are all normal zero ones so they all as a one way of viewing it is as a giant
vector this Vector has a norm which is about the fan in right so the norm of this
vector is roughly the fan in of this unit so think about the number of things here so
say there's D of them D such edges
so from a circus I heard the term fan in but the number of things connecting to a gate right so
there's D things the norm of this is typically going to be D
and so I want to have these W's be initialized some way independent such
that the output comes out to look like it's you know around one you know at you know
zero normal zero one so the way to do this right so I'll write it here
it's called Xavier initialization foreign
it says initialize the weights
IID normal instead of zero make them one over
the fan in for the variance if I do this
then when I multiply it by something right
like I have there so sorry I I said if this thing where if these things okay
people precise if this because these are all normal zero one the norm here of
this Vector is at is can align to be about D if everything
was actually aligned right but if all these things were random it's actually going to have a Norm
that's like square root of d if you're adding a bunch of independent things right when you add a bunch of when you add D independent things
together and you look at the you know what happens you'll get something that's
order square root of D because of the central limit theorem so
if you think that this in initialization all these things are as though they were independent now
notice they're not independent right because you have a single X whatever it is going through all these
things there's some dependency structure that's induced by all this but you want to say it's as though they're independent
vis-a-vis at least second order properties right in terms of correlation and you just hope there's literally a
hope okay let's hope that's true then I can use this initialization to
get the desired effect any questions on Xavier initialization
so their initialization will result in the thing that comes in before the bias being roughly you know variance one
excellent question so initially I said look we just have to be in the neighborhood of zero and I
immediately jumped to variance one okay so the question is what's so
special about one right is one does one matter and here's why one matters one's a very
special number right because there's two numbers
such that in the reals there's two numbers such that if you multiply by them repeatedly
you will stay in the same place one times one times one times one is one
and zero times zero times zero is zero everything else
if you do it repeatedly will either blow up to Infinity or Decay down to zero does that hurt
everybody it's a really great question like why one so we're thinking about a deep Network right there's many many
layers here so we're going to do the same thing over and over again
so if we said you know what I'm fine with the kind of variance coming out of
here being you know like three and then I'm using some nonlinearity
that's not so saturating but there's assumption here it's not so saturating that it'll manage to squash
it down then I'll get something coming out that's also going to be variance that's you know bigger than one
at that point what will happen is I'll repeat this process and they'll get
bigger bigger bigger bigger bigger and that we don't want
why don't we want that we can go all the way back to see the gradients right the gradients will be too big yeah
initialization the the variance of the output maybe
then why because there will be some um
yeah so the great question so I'm talking about Xavier initialization exactly let's do the next thing right
not quite right for red loose
exactly right problem is that if the non-linearity in the previous layer was
a relu then you expect half of these outputs to be zero
by dumb luck right they didn't put land on some side of it so half of them are zero so
although it looks like you have D of them you actually only have D over two
right so for not quite right for relu and so that's why there's her initialization
It's the final initialization applies when the output of the previous layer is reluts like when the previous thing was
relused you use high initialization initialization says you don't have D things you have
D over two things so you take the reciprocal of D over two Yeah question
um
ah yes so great this is d which is Fan in
per unit so all the weights are connecting into something so all the weights look like
this right so if your layer has a asymmetric fan in it's like whatever the
fan is for this unit use the deeper that
so same thing here step out in for the unit for and now what matters here is
use when previous layer
has relu yeah
ah so great question the question is when the previous when this previous non-linearity was a relu
um the fan in is still in the graph d but you expect at initialized remember
everything you expect is you want the inputs to look like they're you know kind of zero mean and on either side
right balanced around zero so that the linearity that relu non-linearity
looks like this so you expect at least that
initialization that ah whatever that input was the previous layer half of the relus are here
and the other half are on this part of the value so for half the values you expect the
output at the previous layer to actually be zero
um draw a gaussian draw something from a gaussian let's say
and what is the probability that the sign for the relu center is zero what's
the probability the sign is greater than you know is is positive one half
okay so you say wait but the relatives are at different places sure they are but as many of them that are on this
side are on that side so on balance you still expect about half of them to be for that particular
input to be dead now they might be they're not actually dead resus they're just zero relus for that input a dead
relu is dead for is zero for everything but
it still matters right because the next layer actually sees for any particular input something
so you can't have it blowing up for a particular input
okay so that's why you do high initialization any questions on how initialization
so so it's a great question what I meant is that typically we'll have a symmetric fan in
for the entire layer but you might just abstractly it might be that at the next layer some particular
your non-linearities are connected to a bias and a set of Weights that touches everything in the previous layer but
some might have weights that only multiply a subset of the previous layer this because of how it's built
right so you can't just use the size of the previous layer you have to look at the fan in for that unit like how many
things does it act does it actually have non-zero weights multiply that's what it's meant
so in the going looking ahead right in a standard multi-layer perceptron you know
a deep neural net the D will just be the number of outputs in the previous layer but when we think
about convolutional neural Nets and the like then everything won't be connected to everything in the previous layer
and so we'll have to think about what that actual fan is Bannon is for doing the initializations
which is why I'm talking about in terms of pattern
sorry ah great question the question is what
is the gain in terms of dead Rattlers when we do this um the game is that
that if you have regulars in the previous layer then about half of them will be
zero so when you do this when you make this a little bit bigger
like square root of two bigger effectively because of the you know um what happens is the input to the
before the bias term will actually tend to be you know something that's like normal zero one as opposed to normal
zero one-half and so what will happen is that as you keep going through deeper and deeper
you'll prevent everything dying out to zero is trying to fight uh things as dying
out to zero as you go through on particular instances yeah
great question we'll talk about Dropout later um you know as another wreck like well as we go through we'll introduce more
and more building blocks as we need them I want to give you like a basic picture first but yeah so but I'll answer your
question so Dropout is another thing that one does during training it's a training strategy for some
regularization um it's something that operates when you're actually training we're talking about here is
initialization from scratch so when you're going to do initialization from scratch you you
pretend you're going to use the entire network it's a good question uh to ask which says if you're actually going to
be training with Dropout and your every time you're going to be running your training you're going to only have half
of your units actually be giving outputs and the other halfs will be set to zero then do you need to adjust your
initialization great question however the way we solve that problem is not by adjusting the
initialization we solve that problem by adjusting during training what we do
okay so there's like uh something you can do like there now you could also say okay it is a choice of where you put it
right there's training time there's initialization time and there's inference time and so you can think
about how you do the adjustments but yeah it's a good question um and it's it's kind of a choice of
what you do yeah
um
so when we change the distribution do they have do they still follows
excellent question great question question was okay we started off by saying we have something that's normal
zero one just to tell you why we care about the neighborhood of zero because we get the culture right but now we're
talking about initialization to different things and so we have to say with these new initializations do we still care about
the neighborhood of zero right that's kind of a really good question so
it's good to step back and look at this thing again
and ask hey why is this thing kaushi
now you could grind out integrals okay to do it but that doesn't help you
because now you change things so you need to understand more deeply spiritually why is this thing kaushi
meaning why does it have heavy Tails like this right why does it look like this so the
fact that it'll be centered around zero is coming from the biased term
does everyone see that like clearly if the bias term is around zero like this thing will be centered at zero everyone
good with that okay so that's the bias term even if the biased
term was somewhere other than zero the fact that this thing in the
denominator is symmetric would also say who cares about that sign right so the centering at zero is pretty
solid everyone see that as long as I don't make the bias term like some big number
right good so the other interesting quality we care about is the fact that
this thing is heavy tails right there actually doesn't have a mean
right good so does that need both of these to
be gaussian no it does not what's making this tail extend like what
would make what would throw an elbow out far we'll throw an elbow out far as if this
weight became small right to move out far it doesn't matter like
yeah sure the bias could be big but we already said we don't have that so it's got to be the case that the W is
small so all that matters to make this thing spirit Spirit like Akashi is that
whatever this distribution is should look flat near zero you see that
because if it has a density around zero that's flat that'll induce a heavy tail in the ratio
so everything we're doing is still making the distribution flat near zero
and so we still have this qualitative Behavior okay great question and why you have to
like be able to reason about these things more closely you start with a simple example that you can kind of analyze
that you can think about and kind of draw results for but you have to understand why it's actually true from
an intuitive point of view what I'm talking about here is math intuition because that helps you deal with okay we've changed things is it's still going
to happen so yeah great question yeah
great question if I made this W have more and more mass near zero
what happens to where the corner goes
when you divide by something that's closer to zero does it get bigger or smaller
bigger so that will throw these things out further which is bad right it'll make everything dead which
is corresponding to the or or linear right everything's gonna be dead or linear which corresponds to the loss of
expressive power that we're afraid of at initialization good question
um I want to be able to talk about Optimizer so like unless it's particularly urgent I want to be able to move to the
next thing okay so hopefully this gives you a sense for the initialization so
um final comment on initialization is
what about biases
so I I've told you how to initialize the weights so that things don't blow up and the thing stays kind of nice hopefully at initialization all the way through
um what about the biases so it turns out for biases there are many choices of what to do all of which
are tried in the literature and that remember what's the standard default rule in deep learning it's look at the
papers that you're building on do exactly what they did right so as a starting point so the same
thing applies even for these initializations right but um what about biases so it turns out
there's like four choices that are all used depending on the paper before so just be aware of them
one is Pete Fannin
as D plus one and use Xavier
so just instead of doing it instead of thinking about this sum followed by us of all the stuff
that's been weighted followed by a second sum as the bias think about the bias as connecting to a unit that always
says one and just being another weight never understand that just changes the
pattern just like Xavier of course this one will never die so you can do the appropriate adjustment perhap
right so that's like one approach just treat it like the everyone else
there's the second one that'll strike you as being weird but it actually works in many cases which is
just use bias equals zero this relies to zero
so you might say wait if I initialize to zero then all the corners will be at zero to start with
um doesn't that reduce a lot of redundancy the answer is yes it does but remember everything before is being
multiplied by random weights so the first few steps of stochastic gradient descent will kick that bias
somewhere so let's let SGD initialize the bias for you in the first few steps they don't see the kind of spirit of
this it's like why should I have to do the work let us do the work related to this
is um maybe zero is not so great so pick a small number
or some other small number um
again people have empirically seen that sometimes this does better than just using zero despite the fact that stochastic still
shaking everything at the beginning four says what's so special about .01 or
any other constant use a small random number
so people use uniforms and things like this again you look at papers you'll see what they do you should follow that for
initialization um all this stuff for random initialization is becoming less and less
practically relevant today because as we said before that in many cases today
when using deep Networks you're gonna initialize not something random but to
something that has been trained in some other way but you still have to initialize that in some random way so this is still like
in the background you need to know okay good so now hopefully you understand a bit about how this
initialization Works in deep networks what it is that we're trying to do we're trying to avoid things exploding or
decaying away in the network ad initialization which is required to make sure that ad initialization the features
that the optimizer sees are interesting they don't just have like everything being zero or everything being one
um I want to comment a little bit about why it's bad if everything gets really
big you can understand why it's bad if everything gets really small because it just goes to zero but what's wrong with
everything being really big so what's wrong with everything getting really big is that the gradients will
also get really big and if the gradients get really big it means that there are certain directions
that you're moving in in terms of the giant space of weights and biases which will have enormous
singular values in certain directions it'll be huge singularized there still might be other
directions that have very small singular values or even moderately reasonable ones
and what will happen is that the giant singular values will force you to use a learning rate in gradient descent that's
small to avoid Divergence but having used a small learning rate
for those you're using the same small learning rate for the other directions and those other directions will
basically not be able to move enough during wall clock time and so you'll just not get to be able to converge to
anything interesting if just initialization is causing you to have wildly huge singular values yeah
let's say okay if it gets big enough or small enough would it cause issues such as integer
overflow or uh or like floating points to be undefined when you're actually
doing calculating yeah great great comment I'll repeat it to make sure it's been recorded
um I've focused on things as though we were doing arithmetic all the arithmetic using perfect real numbers and just
talking about what happens singular values and the like in real systems you're using some kind of floating Point
representation and there's increasing call for reasons of wall clock time and
computer architecture of trying to use smaller floating Point representations
um and so the danger of overflow and underflow is is present
and in fact yeah if you have if you have anything which threatens exploding gradients or completely dying gradients
um you'll just overflow or underflow your um finite Precision arithmetic and then
you will get like complete gibberish and so yeah in real life that's another
consideration which is important but thankfully we can talk about this consideration without
having to at this level we can talk about the issue without having to reference finite precision
right you can just think about even if we had perfect Precision uh real real arithmetic
Okay so this is good is I'm going to
switch to talking about optimizers and I want to make sure that I at least
get to introduce the key ideas behind momentum
one thing I think there is
thank you okay so let's understand the problem
first so for understanding the problem uh
if you don't understand what I'm saying right now discussion tomorrow will help you make sure you completely understand
what I'm saying right now okay but it's kind of the prereq to everything else so
the problem is if I look at the singular values again I'm going to go to the linear model because that's what we know
how to understand but look at the singular values um I'm going to count the different singular values and I look at what they
are right I might have the first one be big and then another big one another big
one this might be on a log scale right I have some big ones
and then I have some that are getting small but it might be that these small ones
still represent uh directions in terms of singular vectors that are
actually relevant to the pattern you want to capture okay so your pattern is actually
think about it it's locally you know 20 dimensional and you
you have these 20 singular values that are all relevant and then you have a bunch that are much smaller
okay like tiny tiny ones
what you would like to do is to get good convergence in walk lock time on all of
these so you care about these
these ones down here that are hopefully much smaller are
represent directions that you don't really care about converging in in fact if you did converge in them maybe you
would get uh you know if you were doing Ridge regularization you'd regularize these out
right but uh you could do early stopping and you don't want to necessarily have these guys get too big
okay so we have a goal which is we want to make sure that we can optimize as
fast as we can by taking steps on the directions we
care about without causing us lots of trouble on the directions we don't care about
okay things like a high level vague uh introduction of the problem but that's really what we Face uh in in real
life so if you're using gradient descent
Sigma Max is limiting our learning rate
because if you make the learning rate too big this will go unstable
let's think about what happens qualitatively here's learning rate
call it data using the step size standard notation and I think qualitatively what happens
so here's zero so four
the direction associated with Sigma Max how does convergence behave
when you're close to zero it's you know slow exponential
which might as well be linear right it's too slow
then you have various call it very slow expansion then you have slow exponential
then you have medium slash nice exponential
then you have a magic value which is instant
right there's like a particular for a particular Sigma Max there's a particular learning rate so I said in
one step you'll converge right that's just like a magic value that's when the Rel when the eigenvalue goes to zero
basically instant uh conversions and then you have
learning rates that are bigger and the ones that are bigger are like mirror images of what's Happening here except
kind of so you know like unit circle
these ones are slow here you're moving Closer Closer this is the
zero and then you're going over on this side so
on the other side you are oscillating
you know exponential nice
and then your
oscillating ominously but not quite dangerous yet and then you
diverge by oscillation
increasing like bigger and bigger oscillations
so when we say the sigma Max is limiting our learning rate it's actually it's limiting this puts a
hard limit here you can't make the learning rate bigger than this or you'll oscillate away
um but actually it doesn't particularly like um what's happening here either
okay so this is something you need to understand if you don't understand it from the very quick descriptions that
we've talked about so far discussion tomorrow will definitely help you do this as will defer and we it's so
important we repeated the question will be on discussion tomorrow on the homework you have to know how you have to know
this um anyway so you have this problem of limiting the learning rate
so what happens is the learning rate exists kind of on Sigma Max scale
so um for the smallest singular value of Interest
small but interesting
for that one so that this is this is this one and this is this one
what does this picture look like on the same scale what happens is that
everything just kind of expands out right
into larger windows so the problem is that what you're really worried about is
actually not this case where in this case if you take the sigma Max and you
put it into a nice place like here right that's oscillatory
down here this is still you know maybe kind of good slow exponential
right and you'll still get things to convert you can increase the learning rate and
have things work the problem is that it could be that this is like even more
spread out in terms of where the regions are and even if you took this Sigma Max to the
kind of nicest place the the limit of oscillation that you can still converge reasonably you're still super slow
on this value and so you're just going to converge slowly there's no choice you balance the
two out and you converge slowly don't understand this concern with gradient descent
this is like the core issue with gradient descent is yes it has this implicit
regularization effect of not favoring directions that are small but maybe some
of those small directions or directions you want to favor a little bit but you don't know where
they are a priori so what can you do how can you make the learning rate bigger
foreign
while avoiding
instability foreign
ER ones
so there's all the optimization approaches we have at some high level are trying to attack this problem
right like what can you do so uh the first approach we're going to
talk about is called momentum
and momentum does is momentum says hey
foreign I actually have a warning sign
in spirit of when I'm making the learning rate too big which is that
I'll see oscillation everyone see that can you make the learning rate big on
the large Sigma C oscillation a particular kind of oscillation this is discrete time remember the learning
algorithms run in discrete time so it's an oscillation that goes
like that you know positive negative positive negative positive negative like
that so you ask hey
do I know anything that will allow me to suppress oscillations
but not suppress something which is steady remember this side is steady here
the small uh eigenvalue the small singular value Direction the
gradients are evolving steadily right they're smooth on the large one they're becoming
oscillatory so there's a question right momentum says
can I reject oscillations
while preserving
study things and the answer is yes we know how to do
this we can use a low pass filter idea
low pass filter gradients
if I low pass filtered the gradients what would happen is that where I get the oscillations I would get
attenuation of the gradient right it would get smaller
which is exactly what I want the what I want is I want the gradients on this
side effectively for the large Sigma Max to become smaller so that I could use a larger learning rate
right so I want to be able to have the oscillation get attenuated while doing
nothing to the almost DC looking behavior of the gradients for the small
and so now the question you have an idea how can I low pass filter the gradients like what can I do so you just think of
what's the simplest low-pass filter you have right what is low pass filtering it's a kind of averaging
so make the simplest averager you can so average the gradients low pass filter just means average
foreign
and I'll show you a analogy for those who uh can understand it
Okay so
low pass filter all right I'm looking for something that
looks like this circuit wise
great circuit analogy
this will low pass filter voltages right if something is steady
the capacitor will charge to that level stay there and it'll steadily go through but if this is oscillating
you'll be pushing charge in this capacitor this will stay small on the outside right so yeah I want this
so the nice thing is this has a first order differential equation
and it corresponds to the following approach to averaging right so I can make the falling averager
this is interval this is State okay to discrete time
so on average numbers I can say the average at time t plus 1 is the 1
minus beta times the average at time t Plus
beta times the thing you're averaging
so the time sequence foreign
number which is
in zero one okay so first even if you don't remember
this that this is the first drawer differential equation has low pass filter Behavior if you look at the discrete time formula
you should see how this is a kind of lazy averaging it says how do I average I maintain an
average and I multiply it by something that's you know pretty close to one
and then I multiply the new thing by something that's small and by doing this I'm effectively averaging that sequence
out do you see that because this isn't a constant coming in
this will just be the same constant but
if this thing is attempting to bounce back and forth right this will be a much smaller steady
more steady thing right because this thing's attempt to bounce up and down will be a small
fluctuation on top of this and so it'll get it'll get dampened down a lot
so this is actually a classic trick um that we use all the time uh
whenever you have the intuition of wanting to an average and you want to make it implementable in a way that you
could analyze as well as make it easier to implement averages naively implemented are really annoying because
if you want to take a moving average of 10 things you have to store 10 things in memory and take their average
but this requires you to only have one internal state that you keep track of and it basically
does the same kind of averaging you can adjust how long you average by choosing beta
something is called exponentially smoothing things out um but this is a really useful trick and
it's basically coming from the this analogy about the behavior of this thing so
what you do is gradient descent
says I take my parameters Theta p plus 1
and I set them as Theta t plus an ETA times a negative gradient of
something right the function you care about
right so this has a you know a t in here
right so what you want to do is gradient descent with momentum
foreign
I want to replace this object with instead a smoothed version of the
gradient a low-pass filtered version of the gradient so I can do that so I say instead
okay and
does everyone see what we did we had we just it's like just like gradient descent except instead of using
the gradient you use this smooth gradient which has been low pass filtered
okay I'll pause here and take questions to make sure that people understand yeah
yeah so exactly the question is if this you right the input of this little box right
is constant is steady or is moving gently which is what would happen to the small uh singular derivative in the
direction of small singular values because you haven't moved that much right it's very steady
that will just pass through here happily and will not be attenuated at all
so all of these guys don't get any attenuation while this guy gets a lot of attenuation
what that means is that you can jack up the learning rate
great question the question is will this prevent Divergence for the sigma Max and
the answer is no but it will hold it off for a larger learning rate right it'll still diverge
but the thing is that okay so um week this is so important you have an
entire homework problem dedicated to analyzing what's going on here but let me tell you like some higher level
pictures of what's going to happen um high level picture what's going to
happen is singular value by singular value you instead of having one state only
which is Theta in that singular value Direction you'll also have
the component of a that's in that same direction okay for a linear problem
so what you have instead of a when you finally look at it instead of having a one
a recurrence relation in a scalar you'll have a two by two recurrence
right because you'll have two two State variables the actual parameter and this
kind of smoothed uh velocity
how can two by two matrices evolve we'll look at complex eigenvalues and complex eigenvectors and so they will when they
diverge they will diverge by oscillation uh by spiraling out
okay so you can still get Divergence except instead of being kind of bang
bang oscillation it'll be like a spiral that spins out of control
and so momentum approaches that are called
momentum uh I if I had my way I would call it just
gradient descent with smooth gradients because that's more what you're trying to do
it's called momentum because what you effectively have is the old gradients
that happened are still affecting how you move through this
so that's the idea that's why it got the name momentum now a warning
uh I have drawn it for you and described it in the context of a parametrization that is the
most natural to understand what is going on right because you have one minus beta beta and ETA kind of
separated out in some nice way if you look at pie Church's documentation in pytorch's documentation the pi torch
parameter is the control the level of momentum are not cleanly separated this way
so here I've chosen learning rate to mean the same thing it meant before
but in pytorch learning rate in the context of calling it momentum means something different
this is confusing yes but so oh what so well you know nothing you can do best
life that's why it's important to understand stuff and actually read the documentation and look at the formulas
instead of just saying why is this not working I had the same learning it as before now it's not working anymore now
it's diverging what's going on well you know like you have to understand what's actually the interaction the terms of drugs
okay so um I'm out of time um next time what I'll talk about is
there's one more approach you could do I wanted to get to it today but you guys ask great questions uh
which is the other thing you could try to do is to try to be adaptive and try to see what's going on with the
gradients there's a way of doing it that kind of builds off of gradient descent momentum
um and that's called the atom Optimizer which we'll talk about next time and then we'll like get into how you use all
this stuff okay lecture ends here switched office hours any questions
midterm is on March 21st that's the week before spring break
on the Tuesday Okay so um what we will do is we will when we
give you the mid we give the midterm we will also then have the midterm available for people to do as the
homework for that week so we won't be assigning a different homework for that week except maybe a
couple of small problems or something um okay so the
topic we want to continue is optimization if you got talked to talk momentum last time I want to recap where
we are there were some questions that I got I want to make sure that everyone understands and then uh talk about adaptive methods
like Adam and kind of the idea behind them and then we will go into a brief survey
of architectures and problems I'm going to cut this part of the class down to make up for time
um and then we'll start into con nuts you want to make sure we hit the main technical material in the course as
opposed to survey stuff um Okay so
let's recall optimization basic properties so recall
plain vanilla gradient descent what are the properties that you've learned so hopefully discussion
uh on the last two days has made this much much clearer to everyone everyone's
on the same page just make sure and it's on the homework again for those people who didn't go to discussion
but you know that there's a a limit on the learning rate
which you've been using Ada sometimes use Alpha is used in some parts of the community that comes from
the largest singular value
right Max
right in the non-linear case you can think of this as the local uh training features
so when you have this limit on the learning rate the impact of it is that
for the smaller singular values and for the direction that they represent you have to use more iterations to converge
okay now I think one of the important things to keep in mind is the difference
between the perspectives in an optimization course and the perspectives in a deep learning
course right so I thought it might be a good worthwhile kind of keeping these two perspectives a little bit different
in an optimization course that you've taken the optimization objective is typically
believed to be the thing that you care about you actually care about you know minimizing or maximizing your
optimization objective uh in the context of a deep learning course
uh we only kind of care about actually minimizing or maximizing whatever our
optimization objective is the optimization objective and the iterations everything is merely
instrumental to us in that our actual goal is to have good generalization performance to learn
patterns correctly so we would gladly trade off
for a worse optimization if it gives us better generalization
okay so for us in a deep learning class
the choice of the regularizer and things that are the the properties of the learning algorithm matter
especially to the extent that they might change what we actually end up learning
for the better or worse okay that's why we're spending time in a
deep learning class talking about optimizers um because we need you to understand the
basic idea behind them but also to keep in mind always that they can change what
you converge to okay so this is really important uh to keep
in mind I think some people ask me questions that were a little bit kind of coming from this uh this kind of
philosophical distinction okay so uh then I was a point I want to
clarify so when
the Ada is small small enough
what do I mean by small enough I mean by small enough when if you look at on the unit circle
where the 1 minus two
ETA Sigma you know whatever we're using right where these guys end up
right where the closed loop singular values closed loop eigenvalues of the iterates of gradient descent where they
end up if they end up like this
okay all on this side of the unit circle that's what I mean when I say small enough that's typically what I mean
okay when you end up in this situation right this is the one here
that comes from Sigma 1 or Alternatives you want to
think of it you know Sigma Max to remind yourself that the first one is the biggest and
you know these are the ones coming from small ones
when Six Ada is small enough you have this Behavior why because you know at a
equals zero everyone starts off here at one and then they all move back depending on how big a is right
so when it is small enough uh what you see is
we converge first in directions
associated with Sigma one
then Sigma 2 and so on so this is what I meant about the uh
when I talked about the question of hey you first learned the you know dominant
Direction in the training data then the next one and the next one kind of by the iterates of gradient descent so people
ask the question so that was really good um saying that well when Ada isn't small enough right does this story still hold
so the thing is you already know the answer to that question so let's look at
medium
values of ADA so for medium values of beta what you get is
on the unit circle some things have crossed over perhaps
okay so now I'm going to say uh okay I'll put one more here okay
by show of hands I'm going to ask you all to vote for a
b c d e f g these are them uh show of
hands which one converges first okay does everyone see the picture keep
on the back see the circles uh see the unit circle and where the closed loop
1 minus two ETA Sigma end up can you guys see in the back
okay good so now let's vote which one of these converges first so
over here we said this one converges first Sigma one converges the that direction conversion okay how many
people vote for a which comes from Sigma one B Sigma 2.
C Sigma 3. everybody yes D Sigma okay good everyone gets it right so what
matters is which one is the closest to the center which one has the fastest uh
convergence right that's what matters so for medium learning rates the order
in which gradient descent will sort of fit these things can be different
right so you can see how the learning rate even when things converge can
um through if you look at the entire learning system which includes early stopping
the choice of learning rate can affect where you end up right in terms of
what model you'll end up picking because suppose it was the case that the model generalizes best was the one that fit C
really well um and a and b kind of didn't really help that much in generalizing
right in that case this with early stopping isn't going to do as
well as this will do with early stopping okay so this is why you know hyper
parameters like learning rate and so on actually you know this is kind of picture in your mind to say why that
actually might affect things so this is also a really good time to
again review something else I mentioned it at the beginning um in earlier in the in the class but I
want to make sure it's completely clear uh you might be thinking that wait in my convex optimization
class you know my optimization class I was taught that if problems are nicely
convex that you know they will have a unique
Global minimum for my optimization problem and so the choice of the
optimizer uh is just a matter of details right you know we end up where we'll end
up where we end up at the end so how does that square with what I'm
telling you okay we're saying it matters so it's a very good question to have
right so again the distinction is that first um in terms of the actual flow of how
one does training and the use of deep neural Nets we are you're typically
going to invoke checkpoints along the way and think about early stopping so we're not just interested in where we
converge to at the end what the minimum is that's like the one very important
reason there's also other reasons that are more surface level um less deep that is well our problems
aren't necessarily going to be convex we might not have a unique Global uh minimum uh in that case
that's kind of also true but I think philosophic is less important than the first one
um but there's one more thing too which is that even when problems are you know
seemingly convex and nice you also have the possibility of having your
optimization problem having uh infinitely many Global Optima
okay and so first can anyone give me the
simplest possible example of a problem that has all the beautiful nice
convexity type pictures that you would expect because the kind of problem you're like yeah that's like the poster
child for a nice optimization problem but it has infinitely many Global Optima
you don't understand the question so it's like I'm not asking a very technical way but you say wait no the theorem says if something is strictly
convex yes it's true I'm not not asking you to deny the theorem yes it's true right they'll have a unique but I'm
saying a problem that looks very nice but has infinitely many
um Global Optima flat okay yes so that's a very good
somewhat trivial example but spiritually correct which is the problem optimize
zero times the parameter Vector minus x squared or minus y squared okay yes because your
parameters don't matter infinitely many Global Optima namely anything okay good that's actually a good answer but let's
make it more realistic something that'll actually you know can show up where you don't have zero necessarily yeah
got a parabola that's been like extruded been like a third dimension okay so the
the the picture that was asked was was asked is to think of a third dimension so let's think of a parabola but let me
like make it into uh like a big like a a like a a skateboarder uh playground
right so it's like a long pipe okay that's what you're thinking of an optimization like that good can anyone
give me a mathematical representation that would have this as the Lost surface
it's good to be able to go back and forth I just want to make sure that everyone has these intuitions exercise so
um okay I'm gonna like don't censor yourself if you know the answer give me a give me a
something mathematically some kind of optimization problem who's lost surface to look like this
yeah X1 X2 maybe I'll just use more deep
learning e language w1w2
yeah right did everyone see that you don't care about the other parameter now this is themes very special but
actually let's make it completely not special right these pictures
where this is the wide Matrix
okay so it's good to have this intuition if a is a wide Matrix
you have more unknowns than equations you can fit everything with room to spare
right you can fit everything with room to spare it means you can have a global
minimum namely something which achieve zero can get better than zero
and there's infinitely many of them okay so you might say why am I talking about
this you know what might seem like uh from optimization point of view a weird Corner case like oh why do you need
positive definiteness and not positive semi-definiteness you know an optimization course would be something like that it's like it feels like I have
some weird technicality right but here this really matters because
many modern uh deep Learning Systems will operate in a regime where you have gobs and gobs and gobs of parameters so
many parameters and you have fewer training points than you have parameters so if you have fewer training points
then you have parameters you're in something that locally always looks like this and so that thing will tend to have
infinitely many Global Optima and once you have infinitely many Global
Optima you know there's this question right that arises which is your algorithm
which one does it go to even if it converges it's not just a question of the path
right as it would be for early stopping once there's infinitely many Global optimize to go to any one of them
and so it matters a lot for generalization which one it goes to
because although you have a small number of trading points you
actually want to work on a whole bunch of points you haven't seen yet right so you know they might behave all
of them might behave the same on the training points but differently on other points okay so this I want to make sure that I
know I'm spending a little bit of time on this but it's really important to keep the perspective in mind right that
the Deep why do deep learning classes talk about optimization this is why okay
so speaking of this if you just we know that if you do
vanilla gradient descent on a problem like this
we actually know where you converge to okay which is if you start at zero
will I initialize my weights at zero for this problem if I initialize at zero the only W's
I'll ever get we talked about this before and we talked about the actual iterates of gradient descent for Ridge or for uh gradient descent the only
iterates of w I'm ever going to get are going to be linear combinations of data points
right which means you're in this you're in exactly the same Subspace that guarantees that you'll get the mid-norm
solution so it's important to remember that by itself vanilla gradient descent will
tend to and this is a very robust thing it will tend to go to the Min Norm solution
where by mid Norm we mean it's lazy wherever you start it'll try to go to
the closest place that'll get it to minimizing the loss
okay where close is in this local sense right remember everything here is in terms of the local features
if it can get there locally it'll get it'll go there okay good
is all this clear to people okay so now you understand like why do we care about what's going on here
so now we talked last time about sdd with momentum
which is a gradient to set with momentum okay so actually this is a good point
to comment on one thing which is we often talk about vanilla
gradient descent uh but we use in practice is always stochastic gradient
descent and the two are not the same
okay they're not the same so let's take a brief moment to
understand the difference okay so when you're doing stochastic gradient
descent what you expect to see is that when you do the gradient step you're going to get
some noise that comes from the randomness of which batch you chose so the general feeling is that if there
is still a dominant uh singular value locally that probably again this is a
hope this doesn't have to be this way the dominant singular value is probably something which will be well represented
in many many many data points so your batch will probably have it and you'll probably still get a pull in that
direction typically and so for this reason you believe that even when you do SGD that
kind of in spirit these findings will still hold
um except you're gonna get a little bit of shaking everything will shake more right so it'll be some noise that makes
a chick um it's actually a really interesting thing
uh and we actually I have a homework problem I could give on this but I don't know if I will um
if you do SGD in so first first something you should
be able to prove right if you do SGD and you're in this situation here
do you go to the Min Norm solution if you converge
so this is the
difference on the question you're not going to do gradient descent so you're not going to look at the entire a matrix at once you're just
going to take a subset of rows randomly every time and you're going to take an
SGD step so I'm going to say suppose you actually converge
I'm going to make that let's assume that we converge okay the SGD suppose I say SGD converges
on this problem and the question is will you converge having
started at zero will you converge the Min Norm solution everyone understand
the question okay so now let's uh see what you guys
think uh let's first take a initial poll how many people think yes you will converge
to the Min Norm solution raise your hand few people okay how many people think no
you won't converge them in normal solution raise your hand okay a few other people a few more people some people are like I'm not sure
and I didn't just say I'm not sure about this how high I raised my hand it's really nice it's the Deep learning class
good to have confidences right yeah sort of no definitely you know like so
um good for you um but how many people are like I really am not sure at all okay so it's a good mix okay so let's
see uh couldn't someone advocate for why they think that if this thing converges
it will converge to the Min Norm solution you start at zero you're doing STD
taking random rows at a time who wants to advocate for that
yeah so ever heard that the argument was you're moving you're still moving within
the Subspace of data points right and if you remember
when you prove that you know the appropriate more Penrose pseudo-inverse formula holds
for the minimum solution you say that once you're constrained moving within that Subspace there is a unique
Optimum point that fits the data points okay
so in fact that argument carries the day if SGD were to converge on this problem
right it would have no choice to converge except to the Min Norm solution
why is that okay because the reason is anywhere else
that's not a solution would still Shake
right because you draw that point randomly it wouldn't fit there would be a stochastic gradient and you would move
and the point of convergence is that you stop moving or you know asymptotically stop moving so indeed that's correct so
SGD so this is one I bring this up because this is one of the reasons we believe generally that SGD will tend to
inherit the properties of gradient descent except you know some amount of like shaking around
uh to do that um it's actually a very interesting and
completely non-trivial fact that not only will SGD converge in these cases
but you can get SGD to converge in these cases with a constant step size you
don't even need the step size to get go down typically for SGD and your optimization classes you learned you need the step size to drop why because
things are shaking you need to do more averaging so but the point is here if it converges it stops shaking which means
it can converge even with a constant substance so just a comment on that okay so
now we're going to talk about gradient descent momentum like we talked about last time and we're gradient descent with momentum says is that
we would like to basically be able to use a larger learning rate and to use a larger learning rate so
it's really important to keep that in mind like that's the goal the goal is to converge faster these are
called acceleration methods right um and from a traditional optimization
perspective the goal is to converge to the same place the unique Global Optimum just to do it faster
um so you want to go faster
uh Ada and to do that we want to basically
avoid the problem that you saw in the discussion which is this thing as it approaches this side is going to start swinging quite violently up and down
right and we want to dampen those oscillations so the idea is
low pass filter the gradient
and you know you do that so now here it's good to
do a couple of things to like comment in case people haven't seen this before it's also relevant there's a good good
way to review something a comment
on low pass filtering and averaging
so how do you smooth out the shaking or ups and downs in something very
intuitively you can average right you might just believe that by you
know yourself averaging is good for removing variation you can also think
about an alternative picture you know circuits picture which is the simplest low-pass filter
looks like this right and if this input voltage shakes
the capacitor will help you know achieve averaging and the output voltage won't Shake as much right that's kind of
circuit's picture now what connects these two pictures what connects these two pictures is that this is like a
continuous time uh differential equation right and you know the solution of
linear continuous time differential equations involves integrals that look like this
foreign called the convolution integral
where this is the input function of time
and this is the alternatively called
impulse response or kernel
depending on the community and it's important to remember that all
this integral is is a fancy generalized average
right because what's an average add a bunch of things up together with some weights
right that's an average what is this doing what's an integral it's a limit of Riemann sums what's that
sums right so there's adding up a bunch of these with some weight
okay so it's like a fancy average all convolutions are fancy averages
so for these kinds of systems again for those who remember for first order differential equation
the H of Tau looks like this
exponential
and what that means is it's a it's a move this thing is like the convolent relevant convolutional convolutional
integral is like a moving average that most heavily awaits the present
and weights the past in a decaying way so that's the kind of average this
represents so when you think of this kind of a thing the way it does the way it dampens out oscillations is that it
weighs whatever is going in in this kind of exponentially weighted way okay so this is this should be review uh
for people probably you've seen the convolution integral in some course
um and when you talk about the response of uh differential equations to linear linear differential equations to systems
some of you in physics uh context might have seen this in the context of greens functions and the like
um all related things okay so this is like a differential equation
point of view I could draw a circuit to the differential equation what's the discrete time counter part of this
right you basically take a sum
oops and you know that's what you do you just do
everything with a awesome so I could put an H here but let me just capture something that looks
like this with a specific specific discrete time counterpart of this you get something that looks like this
foreign
so you notice this is just a geometric that dies down
and I just chose a parameterization that made it an actual averaging right so it
integrates to one or sums up to sums up to one right so it's like like an average so you have this kind of uh infinite uh
response that corresponds to this thing but the nice thing is this corresponds
this this solution corresponds to the following difference equation
I guess I used brackets so I'll put a bracket
for you Okay so
this is where so you know and when I talked about last time I jumped immediately to this equate this kind of
equation I want to show you like where would that come from right it's the natural discrete time counterpart of a
first order differential equation right and it achieves smoothing so this a
is going to be a smooth version of the input U whatever you stick into it
where the smoothing is going to happen this way if you want to make it more smooth
right for the exponential you want to make it decay slower right because if the exponential decays
slower you're averaging you're getting more weight to more terms in the past and so here decaying slower is making
beta closer to zero right so just to see the see the
connection any questions on this kind of comment on how you know where this particular
formula we did last time where it came from any questions on how this works why it
works what it's connected to and so on so
uh what's nice about it again remember from the perspective of deep learning in
practice uh you care about actual resource limitations on your machines right there's like stuff has to stay in
appropriate memory you have to be able to keep your parameters and store them if you're going to be doing updates of
things everything you need to do the updates has to stay in memory
okay so uh what's really nice about this is
that if you had to say I would like to average the last three gradients that's the way I'm going to be averaging I'll
average the last three things how many things you have to keep in memory
you know at least three maybe two right if you're like tricky right
you do this you keep one thing in memory the state so you have your actual
gradient and one more you keep actual parameters and one more thing is that this running average
what's nice about this running averages if you say I don't want three I want more smoothing I want to do 10. you just
change beta now of course this is not the same as averaging the last three or last four but in spirit it is
right and all we care about is the appropriately dampen things right we don't care about this average
the last five like it doesn't matter to us we just want the effect of smoothing of averaging the last five and this will
also give us that effect so in deep learning we always like to choose architectures and
parameterization and ways of doing things that will within a little bit of an eye towards the implementation to
make sure that we're not like blowing our memory budget because of some fancy thing that we're doing so keep it simple
right okay so using this in the context of momentum
the update we actually do for gradient descent or SGD with momentum is
we'll still keep the update to the parameters just as it was before with one exception
instead of using the gradient we're going to use our smooth gradient
to compute our smooth gradient as I mentioned before we just use that
formula
whatever upgrading to the losses that we want to do evaluated at
the parameterization that we have last time
okay is momentum clear in terms of like the how it works
oh this connection yeah ah so okay
this kind of thing right this kind of thing but so it goes this
way for integrals this kind of thing solves linear differential equations with inputs
right so you can go in spirit back and forth between this kind of integral
and a differential equation right look at the solution so it's like I can
give you the questions or I can give you the answer same information in both
is that part clear okay so same way
this thing solves a recurrence relation A first order in particular A first
order discrete time recurrence relation namely this one
so instead of doing this which is crazy because you
have to do infinite sums right you just do this and keep going
so it's this realization that if I want to do this I can also do this
ah the output so this whole thing is
instead of 80 plus one ah good question excellent question the question is why
do I write a t plus one here right is the spirit of the question you asked
um why didn't I write a t it's kind of weird that makes t and t plus ones in one equation usually you have t plus one
on this side and everything here is on T so first is like you can just substitute this in and then everything will be in
terms of t so that's that part's clear so now why did I put t plus one here the reason is
the Step I Take I want to depend on the gradient at where I was not where I was two times
ago but this actually leads to a very interesting and kind of twisty little
point that I want to get to okay and you probably kind of brought me right to it
so where should we evaluate the gradient
we have a choice of where to evaluate the gradient right standard thing is we'll evaluate the variant where my
parameters are right now okay
now one of the interesting things that one can do in the context of momentum
is you say look wait stop wait a minute if I did traditional gradient descent
right I'd say look I'm here right now what does my local neighborhood look
like let me look at the gradient and then let me move so for gradient descent it's
clear that you should look at the gradient where you are right now okay but here you think wait a minute
I want to look at the gradient in the local neighborhood of
where to know how to steer the goal of the gradient is a steer
okay and so in the context of momentum I'm already moving
and I already know a little bit about where I'm going to end up
because of this term so the question is should I evaluate the gradient where I am now
or where I know I'm going to end up and then steer from there
you see subtly different okay if you're moving right
generally in a control setting if you're moving you want to know
if it's already inevitable that you'll end up here you want to know and all you can do is
going to choose what happens after you get there you want the best information there
not here very subtle it's a small thing okay
so there is a variant of SGD of SGD and gradient descent momentum in which we
change this part and this is sometimes called the
nesterov variant so alternatively
instead of evaluating the gradient at WT you evaluated at the part where
you know you're going to end up you're already committed to by your algorithm
so you see like next time you're going to move minus ETA times oh beta with the a down right
you're going to move minus ETA times 80 plus 1. this part is already determined
that's not going to change so evaluate the gradient there
get a little bit better local View of what's going on
so you know use this instead
so if you look for example in pi torch there's a flag that you can set that will make it do
this one versus this one and so the theoretical reasons why in
certain settings this is provably um you can prove it's better for practically speaking it makes very
little difference in memo settings on which one you do in the context of using SGD but you know should know it's there
yeah yeah
great question so I've given you a treatment of
momentum that I believe is easier to understand where it's coming from
uh in the code they actually mix in pi
torch standard implementation they mix eight the role of ADA and beta to be
like beta 1 beta 2 and so on so let's think what happens right
the reason remember I said we did this so that we would have an average meaning an average this is like a normalization
we did to have it be this way what governs the kind of amount of of
smoothing is actually this thing but this is telling you how far you go back in the past
this is like a normalization term to make the average be whatever it is but remember a
only enters into what you care about after being multiplied by Ada so already has a scaling that gets hit by so you
don't need this thing to be normalized in the way that I did here in terms of interpretation it makes it
easier to understand but in terms of having the same expressive power
it doesn't matter right I can have two different things here and here
however if you do that if you put something here different than one minus
beta then effectively what you're doing is changing the learning rate
and this confuses a lot of people that's why I did this why Pi torch does it that way
uh is historical I have no idea like there's some historical reason some paper had it be a
separate parameter and they just stuck to that way of doing it I think
this is a lot easier to understand yeah [Music]
like our goal is to go faster so
um yeah so what happens if you use this gradient so again in the homework we
have you grind out you know what happens so just I'll answer your question about what happens but in reference to the
homework right in the homework you're grinding this one out for analysis purposes um
and when you analyze this thing uh you see first that look although everything
here is vectors in the context of a problem like this
you can just go to SVD coordinates From perspective of the Observer and everything will again decouple
right all the different um SVD directions will decouple and you'll get a bunch of two by two
problems for every different dimension and then you have to deal with the eigenvalue structure of this the resulting two by two okay
notice that that entire Global story doesn't change if you use this or this
like it doesn't matter just get a different two by two if you did that
so uh basically if you do this you can
prove theoretically that with the appropriate choice of beta and the learning rate you can in general do
better in terms of convergence then if you do this right so in the homework we say hey just
but he's kind of involved right like you you grind it out you get like these quadratics there's like some limit then
you can kind of see empirically what happens and you can see that acceleration that this gives you
it's just you know we could have added a part we say now repeat for this right it would have just changed the
equations a little bit um there would still be some grungy thing you still have to grind it out and
you'd see that okay can we get it make it a little faster um it won't make it instant because you
still have the problem you can still diverge so does that answer your question
okay uh in general you want to you want to use a small step small enough step size so
that the local things are changing slowly for General non-linear problems right
now let's come back just as in vanilla gradient descent now
what happens is that the resulting eigenvalues that govern what's happening
is you get a pair of eigenvalues for every one of these things instead of one eigenvalue why because you've added
you've doubled your state Dimension before all you had was your weights now you have the weights and average
gradients two things you're tracking so you have twice as many eigenvalues and
so each of these will split into two things and then your convergence is going to be
dominated by which ones are closer which ones are further and the same kind of story will occur
right depending on the learning rate depending on how what momentum term is you will get different modes converging
fast fastest okay so that's again why the choice of
momentum and the choice of learning rate momentum parameter and learning parameter these are hyper parameters
that will effect can affect what you how the how the generalization behavior is in the context of especially in the
context of early stopping okay so that's hopefully makes this clear
um I wanted to draw before I move on to Adaptive methods uh
I wanted to draw a little
a little circuit analogy for what happens when you're the small learning rate
Ada is small enough
okay so comment uh if you look at the Deep learning literature
you will often in from people trying to unders understand what's happening in deep learning
um one of the tools that they will use is they will say let me instead of
thinking about the discrete time behavior of the iterates of gradient descent or stochastic gradient descent
let me make the Assumption for purposes of understanding that
we actually have a continuous time flow the reason is that our tools and our
concepts for understanding differential equations are more mature and better developed than our tools and concepts
for understanding discrete time iterations in part because discrete time iterations in the non-linear context can have all kinds of pathological behavior
that nice continuous time differential equation flows will not so you try to
use so people will in papers will try to use differential equation flows whenever so when I when I when we draw right now
is kind of the same Ada is small enough spirit and I'm going to express the
differential equations that come out of this thing using a physical analogy of a
circuit so you can kind of see what's happening so simple circuit analogy of gradient
descent so gradient descent first looks like this
like a toy again for those who understand these things this can be helpful for those who
do not it's fine don't worry about it
okay so what this this is a simple circuit that has an inductor a current
source and a resistor and the way you think about training is
that when training starts this current Source turns on
this is is what does this represent
is represents the training data in the mode the relevant mode of uh the you know so
think about the singular value structure let's think of each of those different singular values as correspond to a mode
like a physical mode of a system and the training data is exciting that mode
when you start training that excitement is this current that's what it's
modeling l the gradient the this uh sorry the
inductor represents what the parameters learn so at convergence
everything about all this current is accounted for in this inductor the parameters perfectly fit
uh the the excitement that the training is doing you fit okay
however when you start off there's no current through here and so you have a residual the current
that's coming out of here coming through here is not the current that's going here that residual passes through this
resistor generating a voltage here which is the gradient that gradient moves the current here and
makes again it makes a change so this is gradient descent expressed as a RL circuit
okay and so what you get is the convergence of this current
this reflects the current through here reflects the parameter that's being learned okay
so the interesting thing here is that in the continuous time setting you could just if you want to make it
converge faster you just increase this make the voltage bigger and you know a plot make this current
converge faster that would correspond to increasing the learning rate by a lot which is why in continuous time
gradient flows you know there's no limit on how big the learning rate is because the learning rate is assumed to be infinitesimal and
so the infinitesimal can be as big as it wants it's still infinitesimal so never actually you never feel this great time
so this is gradient descent so what's gradient descent or momentum
it's the same picture because you still have
the excitation that's coming from the training data you still have the parameters that you need to learn
you still have a gradient that's going to be generated by having a residual go somewhere
but you're going to dampen the voltage by momentum term and that's a capacitor
you add to it so with momentum the averaging of the
gradient that happens is captured by another state variable another thing
that holds state in this physical analogy this state represented by this
element is the parameters this state is the average and again if you look at this if you
have physical anal uh feeling for this I could draw this like mechanical counterpart too this is like a electrical counterpart there's a
mechanical counterpart the same thing what happens is this thing will initially it will go here
there's not current flowing through here but it will just get sunk here first
and make the voltage start going up that corresponds to what if you initialize this at zero initially
this gradient is just going here it's not really moving this very much because this is ramping up
but once this thing ramps up it sees the average gradient it's smooth the gradient and now you get convergence
but now for this case you know that there's many different settings you could get convergence that looks like it
did before you could also get conversions that looks like this
oscillating modes you know and the same thing applies to the
understanding of what happens for great uh gradient descent momentum
even in discrete time of course in the context of continuous time analogies which are going to be in
the infinitesimal learning rate case you will never have instability there can't be any instability in this system if
everything is passive right but you know it just still helps you understand why
they'll have the oscillations and why there's a consideration of how you how much dampening you have and so on here's
this helped helps some people understand um on the lecture Ed post I posted a a
link to a Blog which I think has a really nice visualizations of like some of this
story of The Continuous time kind of picture and and so on so that can help you understand momentum
okay any questions I mentioned before I talk about adaptive methods
again here if this helps you understand great if this does not help you understand that's fine too you can just
think of it in terms of differential equations or in terms of recurrence relations and eigenvalues it's all it's
all good okay
so with that I'm going to no questions
okay
I'm going to now talk about adaptive methods
it's actually a long history of adaptive methods but I could talk about more of them but I'm not going to I'm just going
to talk about the one that everyone will use which is called Adam
but if you want to know some of the history of what's going on I mentioned it before uh
there's an entire approach to thinking about learning problems that is
coming from an area called online optimization right it's sometimes called online
optimization sometimes called online learning and that has to do with how do you think
about doing uh learning problems where you don't see all the data all at once
you just see it Bit by Bit by Bit it's quite beautiful it's quite fun
um you have lots of interesting theorems and lots of interesting perspectives that that kind of way of looking at the
world um connects to but one of the interesting things about online learning is the
spirit that says as the game continues you learn what
game you're playing like you don't know what situation you're actually facing because it's
being unfurled to you bit by bit by bit and so in that context the idea of
adaptation is really important right because you need to adapt to the
situation you find yourself in in that context it's like fundamental that context
and so a lot of the analysis and ways of thinking about adaptive algorithms kind
of you can intellectually root in that literature and so there's a whole story behind Adam
building up pieces that comes from that literature that I'm just going to completely ignore
and I'm just going to talk about it kind of more directly if you want to go if you go back and
read the original papers about Adam they'll reference earlier methods like at a grad and RMS prop and things like
this which if you read those papers you will see the connections to online learning kind
of overt in terms of what's Happening Here I'm just going to kind of short circuit all
that and talk about you know what what's what how these things work from a kind of basic point of view
so um with that kind of commentary out of the way the idea behind adaptive methods is to
say wait even after we did things like momentum
we're still stuck with the fact that there is this huge dynamic range
potentially between the singular values and so the big ones are going to limit
how big we can make the learning rate and what our averaging terms are like in momentum
um and then we have to live with it for the small ones
does everyone see that kind of core tension that we've had from the beginning for gradient descent even these acceleration methods you're stuck
with one learning rate and so the idea in adaptive methods is
can we pick can we have
different Adas
four different modes
right because if you could do that we could make everything converge super you know fast everyone understand the intuition like
why do I have to have one learning rate let me have lots okay now unfortunately there's a huge
underlying problem which is that when we talk about thinking from the SVD perspective and so on we're taking the
perspective of an external Observer that has access to the whole training data all at once and can kind of look and
knows what directions to look at and the algorithm itself never sees it
right an algorithm that's running doesn't know what the modes are
it just knows okay I have this data I have this batch of data it doesn't know what the relevant directions are
so at first glance you might think okay that's a pretty fatal flaw
right how am I supposed to have adaptive different learning rates for the
different modes if I can't even know where the modes are so again the Deep learning Spirit comes
into play which is you look where the light is and so you say well I don't know what
the different modes are but I do know different parameters so let me use a different learning rate
for each of the different parameters does everyone see the thing like I can
definitely see that this is the first weight and this is the second wait and this is the third way now it might be that actually this underlying Direction
which is three times the first minus two times the second plus 0.02 times the
third and that's the relevant Direction I should be thinking about I can't see that but I can definitely see this is
what this is one thing this is something different this is something different so let me just use
so the drunk light bulb idea drunk lamp post
says use
different Adas or
different ways not modes
because that's what you can see so
um how do you do this the idea is okay what do you want to do
you want to do the idea then becomes what do I want to achieve by using different learning rates what I want to
achieve is where the gradients are small I want to move
with bigger steps because it's safe to do so the gradients are small where the gradients are very large I
want to be more conservative in the step size I take Okay so
idea
gradients are small
so what you need to do is you need to know when are gradients small and when are they big
right like most basic thing your idea is to use them bigger data when gradients are small so
you track size somehow
so what do you do so again the algorithm is as follows
you will add another state variable so we're going to average the gradients and
we're also going to keep track of size okay so instead of just having the
parameters we'll have two more things the average gradient and something which
tracks size so algorithmically we'll still have a t
plus one equaling one minus beta
a t plus beta
gradient evaluated at WT and we're going to try track the average
size so I'll write v t plus one
equals only the same averaging trick right because I want to track how big
the sizes are and I want my sizes to be something that's reasonably stable
right because you could always say how big is the gradient well I know how big it is I just look how big it is it's
seven it's point two but that's not really what you want right you want to know is this
particular gradient like always kind of small versus is this particular gradient
always kind of big that's what I want to set my step steps because if I just used the actual
current size of the gradient I would just move by a constant amount in every
direction and I don't necessarily want that I want to be able to for example converge
if I do that I will never converge did everyone see that
like I I need some kind of thing that'll make me kind of smooth this out a bit
the size so to smooth the size out I do the same trick
but now you got to pick something that you track and there's many things you could track
uh you could track the absolute value of the gradient but what people do is they
just use uh the squares add together the squares so I just look at the gradient
with respect to the first parameter the derivative respect the first parameter of the loss but again
evaluated at WT and I just Square it
I do the same thing all the way to whatever the biggest
parameter is I have the last parameter I have is okay so this is like the square gradient
the element y squared gradient why do I do this it's cheap to compute
right it's kind of like tracking the standard deviation if you want like or the variance I'm distracting the
variance having done this this is this is telling
me how big things are this is telling me what my average gradient is I have to combine it together into actually taking
a step an Adaptive step so once you think of it this way the
Adaptive step rule practically writes itself and I'm going to describe it to you in like a simple form first
but let's just look at the ith component don't have different steps in different directions right so look at ith
component so the ith component of the updated weight is going to be the previous weight in the ith component
I'm going to do minus some learnable parameter Ada
and then I'm going to want to do a t plus 1
in the ith component which is what the average gradient is but now I want something here
that will make this thing get bigger when the sizes are small and smaller
when sizes are big okay so how do I do that well what is the
this thing is tracking the average square basically so I want to basically look at the
square root of V2 Plus 1.
I in spirit and I want to take a reciprocal
okay that way I'm like normalizing by the average size
standard deviation basically right but you look at this and every time you
have a division you should always be nervous and so the first thing you do is you say okay I'm just going to add some
Little Fudge Factor Epsilon here so that I don't divide and make this thing Giant in case these gradients are very small
like Let It converge in Final End Game to just have it like sit somewhere okay
and so this is so you can think of this as to prevent things from going crazy
you have this plus Epsilon this is normalizing to the average size of the
gradients and this is the typical step size
this is the heart of Adam now there's a few additional
considerations um that are there that I want to talk about so again if I wrote out all of
Adam for you if you look at the code you might look at the code and go it is very confusing there's lots of stuff going on
but if you understand this is the heart of what's Happening then you can understand the code kind of iteratively
in terms of what it's doing like first level of understanding and then like the next level
before I do anything more is this clear how this works in terms as an algorithm
okay so now we track three things important to know so our memory has
increased you have to track the weights
the average gradients and the average sizes three things have
to be tracked but fortunately only three things so a 3X increase in your memory requirements
for running atom over running gradient descent
sometimes this matters because this is memory that's not available for example for batches
okay so here I want to make the first comment
is what about these betas
okay like what do we have to think about the betas so comment
the beta Prime that's averaging the sizes has to be more averaging than
the average you keep of the gradients themselves
what you want is something that reflects the average size of the
gradients on a bigger time scale then the actual averaging that determines the average gradient itself
otherwise you'll have too much variability coming from this you want this to be like something that's more
steady because you want the action of where you move to be largely determined by the
gradient or the smooth gradient Okay so
beta Prime is typically smaller than beta
so V is more smoothed
so this is a consideration in the in atom
next thing is another level of detail that don't even want to write out the details out for
once you've committed to this okay that the beta Prime is going to be smaller then you have an issue which
already existed in gradient descent momentum but kind of comes to a head here
which is if this is a very small number this thing has to be initialized
somewhere okay and this thing is being initialized in some way so this thing is actually going
to be ramping up to the average size very slowly so by itself like in the context of just
gradient descent momentum if this is ramping up right as it is here it's like not a big
problem but if this is ramping up it just means you're moving slowly at the beginning
fine you can do things to accelerate that but it's okay does everyone see that like moving slowly won't cause bad
things to happen it'll just slow you down you'll get the acceleration later after this is converged
and make sure does everyone understand that okay here however
if the v's are very small bad things are happening
okay because this is too big like so danger right normalization of a
appropriately during slow startup is yeah okay you can do it you cannot do it
it's okay normalization of this during slow startup is essential
okay so what do you do you basically just put this on the right local scale
like you know how fast this is converging you know how big it can get and you can just like act knock this guy
up in such a way that it's on the right scale so this thing doesn't become too giant so you make the bigger
to prevent one over square root V from becoming too small for the purposes of
this calculation so if you look at the formula in pi torch you will see something which is basically that
so it's looking at the you know you know exactly how this geometric sum is converging
so you can just use the finite geometric sum part to normalize this appropriately
okay that's the other dimension part of of atoms
deal
foreign thing to do it's like nothing once you
realize the scale you know how to fix it and it's done that's Adam
so Adam is
believed by many people today uh I think reasonably to be if you don't know what
Optimizer to use and there isn't a previous paper that argued for a
different one you should use Adam as the good default General starting
optimizer practically people have found that it tends to to work quite well
in practice in in large part because it buys you uh you know it gets convergence
beef to you significantly faster in many cases and tends not to be that sensitive to the choice of the learning rate that
you make um yeah foreign
oh why you still need the learning rate because yeah it kind of does but you still need
to have like a some overall scale of the learning rate to have it adopt
this is the way it's set up right you you have to okay
if you want to okay why do you know learning rate
um how do you answer this question one way in many engineering or science contexts is to Simply say let me look at
units okay uh
what's happening is that this thing has kind of gradient to units
okay it has units of like stuff on parameters but this has units of squared that
okay do you see that this has squared units this is whatever units are on gradients
gradient units you know can be translated by some connection to applying to parameters
but especially when you divide by this you've now killed the units
do you see that like there's whatever units are here square root V has the same units and now
there's no units okay but this has units
in spirit right it's a kind of thing so you better Translate
you know what I'm saying like it's like a this is a very generic kind of science
engineering intuition that you've probably all learned in your chemistry class or something right like way back
but you just keep it in mind it's always there right so you kind of from that perspective something needs to kind of
translate you need to set the right scale so there's still this kind of at a
spirit level um but you expect to be less important because this thing will try to you know
help you out in terms of getting things to the right scale
um so more comments on on Adam uh so
people actually found however that in many controlled careful experiments
that they would see settings where SGD would actually converge to something
especially with momentum would converge to something that had better generalization performance
then Adam did So Adam would converge faster but in many cases SGD would work better
so it's not like you always can say just use atom so to understand why
you have to see that this drunk lamp post principle that we
applied right the drunk near the Lamppost like
you should have felt there that was risky right we know what's matter what matters
is the actual modes but we're just looking at different etas effectively for different weights so we're wetted to
a particular coordinate system in this adaptation and the possible match or mismatch
between this coordinate system that we're wedded to and what you want to have happen can cause
good or bad things to happen so we might actually like lose some of the implicit regularization
that gradient descent originally was giving you because we might be converging too fast
in junk directions right and the junk directions had small
gradients so greater than was like yeah I'm not going to do anything and Adam is helpfully gaining them up for you
right so you have to be aware the inductive bias of Adam and what it does
can make a difference to what's going on um in particular in the context of
Transformer architectures there is a variant of so notice everything here I've always talked about in the context
of straight up um gradient descent right and I've taken the gradients with respect to the loss
remember we talked about uh how Ridge regularization can also be interpreted as weight decay where I add a a term
here that kind of decays this way now you might think okay fine but I could have
just folded that into the gradient by putting uh into the walls you see that like I can just it didn't
matter for a gradient descent but here that for sure matters because if I just do weight Decay here
that's very different than having it show up inside these losses
and get normalized in some goofy way and so there's a variant of atom called atom w
that is often used in the context of Transformers it gives better inductive bias in many times in practice which
basically says keep your weight Decay explicit out here instead of somehow hidden inside your loss because you
don't want Adam mucking around with that adaptations that have to do with the weight decay
so it's like all like deep learning considerations when it comes to Adaptive optimizers I am out of time and I'm
slower as usual uh okay whatever I'm fine I'll make it up on on Monday in time for discussion just next week is
going to be continent discussion so we'll get there um any questions come up and ask well I
can answer briefly here then I'm going to have to move over to Cory 258.
