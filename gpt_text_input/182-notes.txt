Gradient Descent: SGD and Learning Rate
Optimizers are used to solve for models’ parameters. The optimizers themselves usually have their own hyperparameters (learning rate, choice of the optimizer, etc.). The most basic optimizer is gradient descent, an iterative local optimizer that we have touched on briefly in Note 1. Just to recap a bit, the idea is to change parameters a little bit each time and see how that changes the model’s performance. If we have some parameter vector θ ∈ Rn, we update it according to:
where θt denotes the parameter at time step t, η is the learning rate hyperparameter, and Ltrain,θ is the training loss function. At each iteration, we take a small step in the direction opposite the steepest ascent of the training loss function. If we look at a first-order Taylor series approximation of the training loss function, it is clear that we are looking at the training loss function locally, and asking the question, “where will we be if we take a small step in this direction?" This lets us gain an understanding of how the loss landscape depends on our parameters.
If we look at our training loss function more closely, we notice that it includes a huge summation over the entire dataset. Remember here that ltrain(yi,fθ(xi)) is a surrogate loss function that is compatible with our choice of optimizer, and R(·) is a regularization term that ensures our parameters θ satisfy some predetermined constraints (like sparsity). The size of the dataset n is typically huge, so evaluating Ltrain,θ can be extremely wall clock time consuming. To address this, stochastic gradient descent (SGD) is typically used.
In SGD, instead of all n training points, nbatch << n random samples are used. Our loss function now becomes.
If we take a random draw of a subset of points, we get an estimate of the average. We can assume that this gives
us an unbiased estimate of the actual loss function across the entire dataset, albeit one with some amount of variability (because nbatch << n). In a deep learning context, the batch size nbatch is the largest that your
system can sustain, and ensure adequate wall clock time.
Figure 1: Contributions from each data point in the dataset balance, so the gradient is zero.
Why do we use SGD? The standard Deep Learning answer is that we can’t afford to look at all the data points, but we can afford to look at a few of them. Beyond that, we can expect that the loss surface has different regions of varying “quality" (flat/non-flat, near global optima, etc.). We have no expectation that we’ll begin training in a “good" region, so going in the right general (but not exact) direction might help in this scenario. It’s also very probable that our estimate based off a few points very closely aligns with the direction of the gradient across all data points. We’re only taking a small η-sized step in the direction opposite the gradient, so we only need a level of precision that ensures we take our step appropriately.
1.2 Learning Rate
The updated parameters θt+1 are a discrete time dynamic system, and the learning rate η controls the system stability. If η is too large, we have instability and our model’s loss will diverge. If η is too small, we make (almost) no progress towards getting a working model. The badness of choosing a large learning rate η is very clear, if our loss function diverges, our model doesn’t learn anything. Although choosing a small learning rate does not cause divergence, it is an equally bad scenario as it corresponds to more real world wall clock time, energy usage, and cost.
Changing the step size η might help us (or not) depending on the loss surface. We can use a learning rate schedule to enforce some step-size-changing policy. Doing this is especially useful in some non-vanilla niches of machine learning, such as online learning (where data is only made available to you in a sequential order).
A step size selection that causes convergence for regular Gradient Descent might not guarantee convergence for SGD. If we pick η so that the gradient doesn’t diverge when taken across all points, we can generally expect that it won’t cause divergence when evaluated across some points. What actually needs to happen, is that the model needs to converge to some setting that works well. Convergence happens when the parameters stop changing in our gradient descent iterations. Once we settle so that θt+1 ≈ θt, we converge unless the gradient term −η∇θLtrain,θ suddenly spikes. At optima, derivatives/gradients should all be zero, meaning we converge. The derivative is zero because the sum of contributions to the gradient at all the different points cancels out to zero (see Figure 1). In SGD, we’re not using all the data points, so we might be at some optimum, but our small-batch estimate of the gradient will likely be unbalanced (see Figure 2). So we might step away from an optimum and “only sort of converge." This wiggling behavior is illustrated in Figure 3.
So to actually converge, we may need to make sure that η is decaying.
There are other explanations for why learning rate ought to decrease over time. Picking a large step size at the beginning may help explore the space in a “path less traveled” sort of way, while a small step size at the end helps refine our evaluation.
It’s also worth noting that there’s technically a difference between oscillatory and random walk type behav- ior. A non-decreasing step size in gradient descent might cause oscillatory behavior, but the stochasticity in SGD will cause random walk behavior.
Nonetheless, in deep learning, even a constant step size may lead to convergence.
The time it takes to go through an entire shuffled instance of all the training data set is called an epoch. Shuffling (and reshuffling) data between epochs can help avoid us learning subtle patterns that arise because of the ordering of our dataset. Sometimes, this sort of structured approach and commitment to see all data points is not desired. Because datasets can be so large, if we just randomly sample from the training dataset we are likely to get similar results. In these cases, sampling with and without replacement are equally performant.
Generally, getting an optimizer to work well involves lots of consideration to be taken as to the application and the underlying hardware that the model runs on.
2 Intro. to Neural Nets via ReLU (Rectified Linear Unit) Nets
2.1 What is a Neural Net (Differentiable Programming)?
A neural net is an object that is easy to take derivatives (e.g. Analog circuits realized as computation graphs with (mostly) differentiable operations compatible with nice vectorization). Neural nets are prevalent today because they’re able to exploit gradient descent/SGD to do learning/computation.
2.2 Two goals of the analog circuits
(a) Expressivity: the ability to represent a wide variety of functions.
(b) Learnability: the ability to learn functions without too much trouble.
2.3
Expressivity is important because we need to be able to express patterns we’re interested in, which we don’t know and need to be learned. We can’t guarantee that some model is expressive enough to capture these patterns. Traditionally, the solution to this is to be expressive enough to capture any pattern. If a model can learn any function, to within some amount of precision, we call it a universal function approximator.
Example of Neural Networks
The figure below shows a 1-D nonlinear (Blue) function, piecewise linear (Orange) functions, and data points (Green). As shown in the figure, the piecewise functions describe the nonlinear function pretty well. Our goal is to find a set of piecewise linear functions (Orange) that best match the nonlinear function (Blue) based on the data points using Neural Nets.
Then, how do we create the piecewise linear functions? We do it by forming a linear combination of elbows (Rectified Linear Units) that looks like:
 The standard ReLU function is shown below.
Also, the standard ReLU function can be modified if needed. For example, x can be replaced with wx + b, so that the modified ReLU function becomes f(x) = max(0,wx + b), which can be represented by a computational graph like this:
We can manipulate an elbow to fit our line by changing w (weight) and b (bias). More specifically, Elbow is located at −wb and slope = w. Here, w and b are the parameters(θ) we want to minimize using a loss function.
To provide the circuit interpretation of ReLU, we are going to look at an example of a rectifier circuit, which is shown in the figure below the computational graph. It is composed of a diode and a resistor. The diode prevents the current from flowing in the opposite (or negative) direction. Setting the positive direction of the current to be from left to right, it means that the current can never flow in the negative direction (from right to left). All negative currents will be set to zero resulting in Vout readings being zero. On the other hand, positive currents (from left to right) will go through the diode resulting in Vout readings on the other side of the diode. In this example, Vin is x and Vout is f (x).
 The above image illustrates the big picture of what our simple ReLU Neural Net looks like. We tune the elbows to fit our training points, the optimizer is trying to make the final real number (loss) as small as possible across all the training points. To push the loss down, how much do I have to move y, b’s, and w’s? This process goes backward and the elbow changes.
With one layer and an arbitrarily large layer width (number of ReLUs), we can approximate any function. Historically, people have just stuck with using single-layer universal approximators. So why move into the domain of deep networks? We use deep neural networks because they tend to exhibit learnability, meaning they can actually be trained to learn patterns of interest. When we make this shift to deep networks, some of our parameters become redundant, so the number of parameters is not directly (though highly correlated with) the degrees of freedom our network has.
2.4
Asides for ReLUs
Non-saturating
They work well with gradient based methods because they are non-saturating (unlike sigmoid, for example). For ReLUs, the gradient doesn’t depend on the value of x other than its value relative to the threshold. For a saturating non-linearity, the magnitude of the gradient approaches zero as the magnitude of x itself increases to infinity. Using a non-saturating non-linearity ensures that our gradients don’t diminish in deep networks with long partial derivative chains.
Dead ReLUs
If both weights and biases are distributed with normal distributions, the ratio wb will be a Cauchy distribution. This causes some ReLUs to be located far apart from the others. These ReLUs are considered "dead" since they output 0, and changing the weights or biases slightly doesn’t affect their output.
1 Pragmatic view of regularization
• By adding the penalty term, we hope to shape the optimization to favor something that we like.
• Note that (explicit) regularization does not change the expressive power of the network. Therefore, you only change what the optimization is favoring.
2 Regularization in Least Squares
For a more intuitive understanding of regularization, we can look at least squares. Let’s start by reviewing the problem: we have a matrix X and vector y, and we want to calculate the weights w that best approximates Xw = y.
We can add regularization to OLS. One version of regularized OLS is called ridge regression:
λ is the regularization parameter. It penalizes high-magnitude weight vectors (can you see why?). We can select different λ values to control the behavior of Ridge regression. When λ is higher, the weight magnitude penalty is more severe; when it’s lower, the penalty is lighter, which allows Ridge regression to settle on weight vectors with higher magnitudes.
Note that when λ = 0, ridge regression becomes unregularized. This unregularized formulation simplifies to the OLS problem and solution. Intuitively, setting λ = 0 tells ridge regression to disregard the weight vector’s magnitude altogether, allowing it to solve the problem as though it were simply OLS.
We can also solve least squares with gradient descent. Gradient descent is a process where we aim to improve our model by repeatedly moving its parameters in the direction that minimizes the cost function.
This gives the interpretation that for large singular values, gradient descent moves much more and faster than smaller singular values, which can take longer to converge. Thus, we first fit in the largest σi direction, then the next and so on. When working with very small singular values that have the propensity to give large solutions, we may converge to something very large, but slowly, especially without regularization.
Solution: Use gradient descent, but limit the number of steps to a finite number to avoid overtraining. This is sometimes called early stopping.
There are a few different tricks that can be used to avoid explictly including Ridge regularization while still accomplishing the same result, either by adding extra entries or extra features to the data matrix.
Implicit regularization is the regularization that occurs when we aren’t consciously doing any regularization. When performing explicit regularization as above, we must specify a specific regularization hyperparameter and modify the training data, model architecture, or loss function. In contrast, implicit regularization is an unexpected benefit stemming from our choice of optimizer. We will see that choosing gradient descent as our optimization algorithm, combined with the large size of DNNs, provides enough regularization for DNNs to generalize well without us intentionally restricting the parameters.
Observe that this is roughly a linear function with an extremely small slope if σi is tiny. In this situation, GD barely moves in the early stages even though it will eventually converge to a very large value, as pre- viously discussed. Together with early stopping, this means that GD is trying to do something like ridge regularization for us because it will resist enlarging the directions corresponding to small singular values. Early stopping is when we stop the training process because validation performance has gotten worse or has not improved for a long time. It is important to note that GD, when initialized at zero, will converge to the minimum-norm solution. This is a good exercise for the reader to verify.
To summarize, there are three kinds of regularization: explicit regularization, data augmentation (adding fake observations or features), and implicit regularization (optimizer has an implicit regularizing effect). Regarding DNNs, the combination of the min-norm seeking behavior of gradient descent and the feature augmentation that is implicit when using large networks gives a lot of regularization even if we weren’t thinking about it.
Gradient Descent is the tool that we understand the most and is often our first choice.
• The loss function will typically contain training data with labels that we would like to predict with good accuracy.
• The singular values of the data matrices involved greatly matter and can lead to overfitting, slow convergence, and unreasonably large parameters.
• In solving this problem, we usually add regularization, which we can understand through SVD as bounding the solution to the smaller singular values while leaving the solution of the larger singular values unchanged.
• It’s desirable for the input to our ReLU layers to be centered near 0 and to be close to where the “action" is, otherwise poor initialization can lead to dead ReLU nodes from the beginning. Dead ReLUs are problematic because they make all the computations with the weights and biases, but are always less than 0, leading to wasted computations.
• Additionally, dead ReLUs are usually unable to recover, leading to a portion of our network being unable to learn further.
We want to achieve a distribution of N (0, 1) at initialization, the outputs of the layer will be standardized. So the main idea is that we want our inputs to have a distribution of N (0, 1) at initialization.
Current Basic Folk Wisdom:
• Use whatever worked on a related problem. (e.g., pre-trained network, literature review, etc)
Xavier Initialization aims to initialize weights such that the variance across every layer is the same. Let d be the fan-in (number of inputs of the layer) of the target layer.
Problem: This doesn’t work well for ReLU!
Why? If the previous layer was ReLU, we expect half of the outputs to be 0 on average.
There are typically four approaches that are used to initialize biases:
(a) Treat fan-in as d + 1 and use Xavier Initialization
This treats the bias as a weight in order to use a weight initialization technique, which is why we use
d + 1 instead of d for the fan-in. (b) Just use bias b = 0
This is essentially letting our optimizer initialize the bias in the first few steps. (c) Just use b = 0.01
Empirically, people have seen that this sometimes performs better than b = 0. (d) Just use any small, random number
This claims that it doesn’t matter what small, random number we select, since our optimizer will quickly change it anyways.
Vanilla gradient descent has many benefits, but speed is not one of them. The reason behind that is the constraint on the learning rate.
The idea of momentum is finding a safe way to make the η bigger. We know that the dimensions with larger singular values start to oscillate quickly. So the thought is to somehow low pass filter (LPF) those directions that would otherwise oscillate if we take smaller steps. By doing that, instead of moving in the direction of the gradient, the update is moved towards the direction of the average gradient. The functioning of the
simplest LPF is where this idea of averaging originally came from.
This low-pass filter (LPF) will regulate voltage.
This average is beneficial, as it only requires one internal state to keep to track of, and β will determine how long averaging will go for. This technique is commonly referred to as exponential smoothing. Now we want to apply this technique to create a more stable gradient descent.
In order to achieve gradient descent with momentum, we want to replace our gradient term in regular gradi- ent descent with our low-pass filter equation applied to our gradient. Gradient descent with momentum can be considered as a “smooth gradient".
The main difference with momentum is that gradients at previous time steps have an impact on the current gradient’s value.
The vanilla momentum uses the gradient where the current weights are. In the Nesterov momentum, we take advantage of the fact where we are going by peeking into the future, as given from the first part of the momentum term (9). This is because we already know where we are going to end up, and by taking that new information to the calculation of the gradient at this step, we can take a bit of an advantage on learning.
The simple perspective of the origin of the adaptive methods comes from the fact that even with momentum, the largest singular value and the smallest singular value are the ones that govern the process. Values in between are not relevant for setting up the parameters. So the simple idea is that instead of having a single step size, use different step sizes (ηi) along different dimensions of the parameter vector. In vanilla gradient descent, we go down in the steepest gradient direction. But in the adaptive methods, we no longer go in the steepest direction but still in a downward direction.
The formulation of Adam’s method where different step sizes in a different direction are shown in equation 12. In the large gradient directions, it takes smaller steps and vice versa. That way, it takes evenly sized steps along different directions. To track the size of the gradient, a vector V is introduced, which depends on the element-wise product of the gradient vector. Then the gradient update for the ith parameter is computed with the square root of elements of V to make the units right. A constant (ε) is added to the denominator to make sure it is stable.
Topics covered last time were Key ideas & Convnet manifestation. Today, we continue with Con- vnets, focusing on input standardization and activation normalization. The lecture included the following topics:
• Respect locality
• Respect ”invariance” within data for problem domain • Support hierarchical structure (Fine → Coarse)
• ”Room to play”
which are related to the following realization techniques: • Convolutional Structure
• Weight-sharing
• Data Augmentations
• Down Sampling
• Lifting to more channels as we get coarse • Diversity to support learning
• Dropout
• More layers
2 Locality
We have the observation that many useful image features are local. To tell if a particular patch of an image contains a feature, It is enough to look at the local patch. We get a different output at each image location using the same filter. We assume locality exists in CNN, where inputs that are more close to each other are more correlated. In the context of image inputs, this assumption is qualitatively valid because of local patches of similar color, texture, or lighting. Figure 8.1 shows an example of locality.
3 Weight sharing
Weight sharing in CNN is that a convolutional kernel or filter scans across the whole image input and produces a feature map, so every neighborhood of pixels in the image is processed by the same kernel or filter. That is, the weights in the kernel or filter are shared. We don’t assign weight to each individual pixel.
Example: Suppose we have a 5x5 gray-scale picture and a 3x3 kernel (Figure 8.2). We apply the kernel to one pixel of the input image and its surrounding local patch with the same size as the kernel. We get a number from the previous calculation, add the bias, apply non-linearity, and we finally get the new output of one depth of that one pixel of the image. When the image has multiple channels and each channel is convolved with a different kernel, this operation is called depthwise convolution.
The weight-sharing structure provides translational equivalence. The resulting activation map remains the same under the translation of input feature map. Weight sharing also significantly reduces the number of weights of the network, which ensures higher computational speed.
Sometimes, the padding operation is introduced. Padding is the addition of crafted pixels on the sides of the image so that the pixels on the borders are not lost from the output of convolution. Padding size is usually one less than kernel size. If the crafted pixels are all zeros, this operation is called zero-padding. If the crafted pixels are mirrored values from the original input, this operation is called mirror-padding. Deep learning application typically uses zero-padding for practice.
Figure 8.3 shows an example of convolution operation with padding.
Note: depth of one input/output layer in the CNN is a term for the number of channels of the layer. For example, the depth of an RGB image is 3.
Receptive Field is a term borrowed from biological vision. A particular pixel of the output generated by the neural network has a dependence on some, but not all pixels in the input. So this term defines what pixels in the original image this output depends on. Figure 8.4 is an example of receptive field.
If we keep adding convolutions layers on the images, the receptive field will grow linearly. However, linear growth is slower than desired. Faster growth is desired because the neural network can easily learn the full hierarchy.
Therefore, we need a way to do dimensionality reduction on feature maps, i.e., down sampling.
For example, if we want to transform a 2x2x4 region of the output into a 1x1x4, we need to represent 2x2 elements with 1 pixel. This is also known as pooling.
There are several ways for pooling, including
• average
• max
• pick-one, i.e., pick the pixel at a certain location of the computed local patch, and discard the rest
• weighted average
Pick-one pooling seems a waste of computation because it discards most of the computed pixels. This method of pooling is implemented by stride instead, which only computes the pixel at the desired location to save time and computation.
The weighted average can be viewed as a convolutional layer and a stride.
Pooing layers make the receptive field grow exponentially. In addition to dimensionality reduc- tion, pooling also provides local translational invariance, allowing the CNN to be more robust to features varying in their locations.

1. Convolutional layers
(a) A way to avoid needing millions of parameters with images
(b) Each layer is ”local”
(c) Each layer produces an ”image” with roughly the same width and height, and number of channels = number of filters
2. Pooling: moving from fine to coarse but more abstract
(a) If we ever want to get down to a single output, we must reduce resolution as we go
(b) Max pooling: downsample the ”image” at each layer, taking the max in each region. Max is differentiable. It stops gradient descent to the smaller value which it discards, so it ensures the gradient descent to the important feature.
(c) This makes it robust to small translation changes.
3. Finishing it up
(a) At the end, we get something small enough that we can ”flatten” it (turn it into a vector), and feed it into a standard fully connected layer.
(b) Suppose the input size is W, the kernel size K, the stride S, and the padding P. Input and kernel are both squared.
Regularity is needed for the neural network, which is the invariance to some insignificant changes to our knowledge. For example, it is expected that a pattern can still be recognized when it shifts horizontally by 2inches. However, these insignificant changes are not present in the training set. Data augmentation can apply these changes to the input images and feed the newly generated data to the neural network to train for robustness.
In practice, in order to save space, the newly generated images are not stored along with the original huge dataset. These images with data augmentation applied are generated on the fly. Modern deep learning applications typically never see the same image twice, because all data have been augmented during the training. During data augmentation, the label along with the input image normally does not change.
Data Augmentation represents some of the easiest ways that people can transfer the domain knowledge into the training process. We humans understand what are insignificant features by our evolved vision system. This knowledge can be encoded into the practice of data augmentation and drop it into the training. This practice can help the neural network know what is important based on our knowledge.
Basic augmentations include autocontrast, rotation, translation, posterization, etc. Some basic augmentations are exemplified in Figure 8.5. More aggressive augmentations include cutout, Mixup, CutMix, and PixMix [3].
The design choice with zero mean and unit variance is good because 0 + 0 = 0 and 1 ∗ 1 = 1. We want to make sure that the neural network is learning with the greatest sensitivity when the dataset is essentially changing. For example, the elbow point of a ReLU network can capture where the predicted values vary. It will be hard for a ReLU network to learn if the elbow points are not aligned with the output action space. Standardization is a way to align inputs to where the function can learn more easily.
The expressive power of the network is not lost with standardization and normalization. The expressive power is embedded in the bias and the weight terms, which can shift the mean and variance.
We perform standardization on the training set by subtracting the mean and scaling the variance computed from the training set. We can use the same mean and variance from the training set to normalize the validation set.
We demonstrated in the above aside that it was possible to preprocess the data such that a specific data point which we know with high confidence can be weighted proportionally to its spread. Note that, in doing so, the data point now has a mean of zero and unit variance. But that’s exactly what normalization is!
In the toy example above, normalization had the effect of correctly weighting points based on their importance (in this case, confidence). Normalizing our training data before inputting them into neural networks has a similar effect of scaling the parameters such that the raw magnitudes of the values don’t necessarily impact the gradient calculation during the gradient descent backpropagating step (if we had massive, un-normalized values, the gradient would always be large). Moreover, the data points are generally centered at zero because the ReLU layers are often initialized at that point, making the neural net most sensitive and expressive when input values are around zero. Thus, normalization can additionally decrease the neural network’s training time.
We are not generally provided with prior knowledge of the probability distribution from which our sample X data are drawn, so these values must be derived empirically, i.e. from the data set X itself during training time. Let us focus firstly on ways such an empirical average E[X] can be derived.
Consider the following scenario where we have n 100 x 100 images in our data set, each with 3 channels: red, green and blue, with values between 0 and 255, inclusive. The following are ways we can compute an empirical expectation for any given point in any image:
1. For any given position (x,y) in channel i, take the expected value corresponding to that point to be the average of the channel i values at (x, y) across all images, i.e. the expected value for position (50, 50) in the red channel is the average of the red values at (50, 50) across all images. This can be done across our entire data set, with or without the addition of augmented images.
2. Use the numeric midpoint of the range as the expected value for all positions (e.g. choose 128 for range 0-255). This method is simple, but it assumes that the color values across all images are distributed evenly around the middle of the range. Consider what might happen if our data set consisted only of dark images (images with channel values between, say, 0 and 12). Would our normalization still be centered at 0?
3. Take the expected value for position (x, y) at channel i to be the average of all the channel i values for that image. For example, for any given image, the expected value for position (50, 50) of the red channel would be the average of the red channel value across all positions of that specific image.
4. Set the expected value to be an average of the values within a local patch of pixel positions in the same channel within the same image. For the expected value for position (50, 50) in the red channel, we may want to take the average of a 3 × 3 patch centered at that coordinate, i.e. the rectangle [49, 51] × [49, 51].
Food for thought: What are some of the advantages/disadvantages of some of the expected value definitions listed above? Especially for the definitions that average within a single image, what information may be lost in the normalization process?
These are just a few of the more common ways to compute an empirical expected value. Note that the empirical variance is calculated analogously, except, instead of averaging, we take the variance across a certain subset of points. There are myriad other definitions depending on the specific application, some of which are combinations/variations of the ones listed above. For example, for a much coarser average, we could define E[X] to be the average of all values within the image across all channels (a variation of option 1). One could also imagine averaging across all channel positions across all images as an extension of this definition. When choosing what type of normalization to use for a specific problem, it is considered good practice to use whatever method has given good results for a similar problem in the past.
Each expectation calculation scheme can be seen as different ways to span 3 axes: the image positions, the image channels, and the different images within the data set (Figures 9.1-9.4). For example, we can visualize expectation (3), the average of the values at all positions in one channel for one image, as a prism that spans all positions, 1 channel, and 1 image (Figure 9.1). A few other examples are listed in Figures 9.1-9.4.
Some of the more common normalization methods have special names:
• Batch normalization
Batch normalization normalizes the contributions to a layer for every mini-batch.
At training time, the gradients are not calculated for all data at one time; instead, a batch of data is used. A batch normalization layer uses a mini-batch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the mini-batch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features. The batch normalization will become unstable when the batch size is too small.
• Layer normalization
Layer normalization normalizes input across all channels in one image.
• Instance normalization
Instance normalization normalizes across each channel in each training images. The problem instance normalization tries to address is that the network should be agnostic to the contrast of the original image.
• Group normalization
Group Normalization normalizes over a group of channels for each training images.
Group normalization is a medium between Instance normalization and layer normalization. When we put all the channels into a single group, group normalization becomes layer normalization. When we consider each individual channel a single group, it becomes instance normalization.
9.1.4 Weight Standardization
What we talked before is input standardization. Another way to control the movement of gradient descent is weight standardization. Instead of applying the weights directly during the gradient calculation, the weights are normalized beforehand, preventing weights from getting too big.
We’ve established that normalization in the input layer centers the data where the ReLU elbows are most active, but do we need to normalize each layer in the convolutional net (Figure 9.5)? After all, the output of each layer is the input to the next layer, and the repeated applications of the weighted convolutional layers can very quickly lead to an output that is, again, extremely small (leading to a problem known as the vanishing gradient) or very large (the exploding gradient).
The solution is to not just normalize the initial training data, but to also normalize the outputs of interme- diate convolutional layers. Intuitively, normalizing after every layer in the network should solve the problem, which was exactly what early researchers placed did. But since gradients don’t tend to get extremely large or small over the course of the application of at least a few weighted convolutional layers, adding normalization at the very start and after every few layers is sufficient and is what is typically used in modern networks.
Another way to combat the effects of the vanishing/exploding gradients is the introduction of skip connec- tions, bridges that allow the output in one layer to be the input to not just the layer immediately subsequent, but also to layers further down the net. In a network without skip connections, gradient updates must pass through all subsequent layers before reaching the weights of the current layer, which may lead to a gradient update that is minuscule. Skip connections make the weight layers more sensitive to gradient updates as the gradients have to backpropagate through much fewer layers of the network during the gradient calculation and are therefore much less likely to get extremely small from repeated applications of weight layers. 
The modern convolutional neural network era was ushered in by the idea that each successive layer modifies the data in the pipeline, as opposed to completely transforming it.
We can also see how depth affects plain nets versus residual nets in Figure 2. An issue people found (that motivated this new residual architecture) was that in plain nets, introducing new layers created more error instead of creating more expressivity.
With this new residual net architecture (cf. Figure 3), each successive layer no longer completely transforms the data. Instead, through the use of the skip-connection, an identity operation is applied and added to the actual output of the layer. The effect of this is allowing the network to less dramatically modify the data in the pipeline, which in turn allows the depth of deep networks to more consistent learn salient features of the data.
Now, as the number of these layers increases, do we eventually reach convergence? In other words, with enough layers, do things stop changing? We cannot in fact make general statements about the steady-state behavior of such deep networks. This is mainly due to the fact that we also cannot make any such guarantees about the behavior of ODEs of the form shown above. However, we can say if the residual blocks are small and get smaller in successive layers, then we would be able to make some guarantees about convergence.
A new architecture emerged that borrow from the success of transformers (without the actual transformer part). It differs in architecture from the residual net as shown in Figure 4.
It is important to note that in ConvNeXt, instead of using the traditional batch normalization and ReLU activation, layer normalization and Gaussian ReLU (GeLU) are used. Furthermore, the 7 × 7 convolution is done within each channel, as opposed to across the channels (this is signified by the “d” in front of d7 × 7). Furthermore, ResNet has convolution operations taking place in the middle of the pipeline, whereas ConvNeXt has convolution in the beginning. This ultimately allows ConvNeXt to save a factor of 64 in parameters space, while also using and only one activation layer.
In Depthwise Convolution, filters and inputs are broken into different channels, convoluted separately, and then concatenated.
In the ConvNeXt structure, Depthwise Convolution is used instead of typical convolution as it is less computationally demanding. In general nowadays, Depthwise Convolution is more commonly used than typical convolution. The difference between typical convolution and depthwise convolution is visualized in Figure 6.
Since ResNets in 2015, newer models like the 2022 ConvNeXt have improved performance as seen in Figure 7.
These newer models take bits and pieces from the old models. For example residual connections from ResNets are still common. However, there have been changes made as well, with depthwise (grouped) convolution being more common now than typical convolution.
ConvNeXt uses a cosine learning rate schedule, AdamW (Adam with weight decay), label smoothing, a type of dropout called Stochastic Depth Regularization, and aggressive data augmentation, all helping lead to higher accuracy.
Inspiration How can we simulate having an ensemble of diverse neural networks that we output the average of, like in Random Forest, in a method that’s less costly? By utilizing dropout, there is essentially a slightly different neural network at each training step which provides an ”ensemble-like” behavior.
Implementation
• During Training: While training on our minibatch, randomly ”kill” certain units by setting them to zero before training. We will have a hyperparameter that provides the probability of ”nulling” out a unit. When we ”kill” a unit, all the attached weights do not get updated and we can think of the remaining weights ”shaking” a bit to pick up the slack.
• Evaluation: The standard way evaluation is done after dropout is to use expected behavior.
Results of Dropout Dropout promotes diversity and redundancy in networks since at each training step we are essentially training a different neural network. It ensures that it isn’t one unit’s job to learn everything since that one unit can’t always be relied on. It has a regularizing effect and is usually only used for MLPs (multi-layer perceptrons) on 1x1 blocks.
Does Dropout make training slower? No. Learning rate and dropout are trained together, so while dropout does reduce the size of our gradients, the learning rate can compensate for this.
Can Dropout hurt the performance? All regularization techniques have the ability to hurt performance since they shape the inductive bias. Depending on what we are modeling dropout can affect the performance differently. For a brief period of time, people thought that Normalization might negate the need for Dropout, but in practice having both performs better.
Mathematical Reasoning Behind Dropout In another lecture we discussed that we can think of our training algorithm seeing some version of the big singular values that are defined in the inductive bias. Generally for a matrix there are two ways you can have a large singular value. Either you have many different things pointing in the same direction, or you have one particular row or column that has a big value. Dropout drives us towards the direction of having lots of little things pointing in the right direction.
3.2 Stochastic Depth Regularization
Implementation During training randomly drop entire residual blocks. So during training some will be active and being trained while others are ”gone”. Very similar to Dropout but on a different scale.
3.3 Other Methods Similar to Dropout that Have Been Explored
• Drop Connect: Instead of ”killing” certain units why not try ”killing” weights? People have found that in practice this does not work as well.
• Another method that also adds the regularizing effect we see in Dropout is to multiply by random noise (in between 0 and 1) instead of multiplying by 0. Dropout tends to perform better in practice.
• What if we have logic behind what we turn off instead of randomly choosing? People have explored ”killing” units more or less often based on what the size of their activation’s tend to be as well as other similar ideas, but these have not been proven to be useful in practice so far.
4 Label Smoothing
4.1 Before Label Smoothing: One Hot Encoding
For Classic Cross-Entropy/Log-Loss Classification when we have a label ”class 1”, we represent this using one hot encoding. Since one hot encoding has an array of 0s with a 1 for the correct label as a goal instead of probabilities, it forces the model to try and be ”super” confident. This means that the model will constantly be trying to get closer and closer and since with softmax we cannot actually reach a probability of 1 unless the input is infinity, the model will never be to reach the goal classification.
Reaching this goal y array is possible and in practice label smoothing does improve performance. We can think of the
behavior as now being more similar to squared-error since they can both be satisfied.
Concern: Originally, people were concerned about this idea since in the real world some items are more similar than others. For example, a dog should be more similar to a cat than to the ocean. With that in mind, we can see that label smoothing tries to say that something is a dog but has an equal small probability of being either a cat or ocean when logically we would expect the probability for the cat to be higher.
Images have a structure with patterns that we want to learn and we need to create inductive biases to help us learn the structure. The key ideas including:
1. Convolutions with weight-sharing AND having an “image” at each layer: build the local convolutions and then use hierarchical depth to see the entire image
2. Residual Connections to fight dying gradients: every layer has an effect when parameters change on what happens at the end
3. Normalization to “adaptive speed bump” exploding gradients: brings down growing activation values
We might have different numbers of neighbors to “me”. In contrast, one pixel of an image usually has eight neighbor pixels in CNNs.
• We don’t have separate names for neighbors of “me”. This means that the neighbors do not have a particular order in GNNs. In contrast, the eight neighbor pixels of one pixel have their particular position in CNNs, like the pixel “b” is on the top of “me” and the pixel “f” is on the right.
The most important idea behind CNN is the convolution with weight-sharing. What kind of function can we have in place of convolution, respecting the properties of a graph?
1. First, this function should have some learnable parameters associated with it (like w1 in Eqn 11.1a).
2. Second, it should take two arguments, the node itself and its neighbor nodes, because these are all the
information we know related to this node.
3. Then, a method is needed to combine the information of these neighbor nodes regardless of the ordering or cardinality of neighbors. This method is not learnable and we need to choose from possible choices, including Sum (used in Eqn 11.1a), Maximum, Minimum, Product, Softmax, Variance, and so on.
4. Finally, neighbor nodes also have their learnable function, gw2 (neighbor node).
We can further factorize gw2 (me, them) into sw2 (me, them) and gw3 (them) and get Equation 11.1c, where gw3 (them) is regarded as the learnable function of “them” and sw2 (me, them) is regarded as the connection (or similarity) of “me” and “them”. In Transformer Architecture, sw2 (me, them) is also known as “attention” because it is a learned amount of how much we pay attention to this neighbor.
Food for thought: If we view the GNN in an adjacency matrix formulation, is there a way to use our usual CNN weight operations more directly?
Answer: There is a way to leverage adjacency matrix formulation with use-cases in graph signal processing for signal reflection.
Researchers have generalized the idea of signal processing from one- or two-dimensional functions to general graph relationships. A convolution can be seen as an operation that respects the natural shift invariance on the infinite topology. The infinite topology means that the signal can keep going to the right or left. For a graph, there is no information about what the natural shift would be but there is indeed something we can do with the adjacency matrix. We can take a walk on the graph, or we can take products of the adjacency edge and itself. We can think of shifts on a graph as a kind of repeated product on the unit line. As an example, we can generalize the infinite line to the finite line by shifting on the circle because we can rotate on the circle.
There is also a beautiful connection between convolutions and the invocation of a different domain called frequency domain convolutions. In signal processing, convolution in the original domain corresponds to the multiplication in the frequency domain. But it turns out that the frequency domain is just the eigenbasis corresponding to a particular matrix associated with the structure because convolutions commute with each other and can share the eigenbasis. This allows people to consider what actions can commute with such a matrix formulation of a graph network, with one such formulation being the adjacency matrix. So you could imagine what operations (as matrices) can commute with the adjacency matrix. Those will be the counterpart of convolutions and will respect the entire graph in this abstract way. Graph signal processing considers those objects relevant in graph neural nets. This entire approach is sometimes encapsulated by the spectral methods.
The number of nodes in a graph can sometimes be on the order of millions, and the number of edges per node can be highly variable. Often, this leads to very sparse adjacency matrices, which are space-inefficient. Another problem is that many adjacency matrices can encode the same connectivity, and there is no guarantee that these different matrices would produce the same result in a deep CNN (that is to say, they are not permutation invariant).
Pooling (downsampling) groups similar nodes and coarsens the image in CNNs. Here, the similarity usually means how close the pixels are. In GNN, we just need a similar clustering method to “coarsen” the graph. Under the assumption that the graph topology is fixed, we can pre-compute a specific clustering of this graph based on its topological attribute. In this way, the four nodes in Figure 11.1 are shrunk to two nodes in Figure 11.2.
The clustering can also be learnable. We can use some similarity measures to cluster, like similar neighbor- hoods or similar values inside them. We can even use the learned similarity to simulate the effect of the clustering without doing a full clustering.
In fact, almost everything else remains the same. For example, the residual connection only requires the structure of the object to be the same across layers, which GNN satisfies as well. GNN can also employ all normalization used in CNN, like layer norm, batch norm, instance norm, group norm, and so on. All the normalization needs are that the outputs have some sense of “likeness” to average over.
Let’s generalize our assumption: the graphs can still give a single output but no longer have the same topology. What’s the most naive thing we could do?
We could try to force the graphs into a similar shape. One way of doing so is to get rid of all topology, make all the nodes fully connected, and use edge labels to tell whether they were connected in the original or not. However, this does not work so well, especially from a computational point of view, because lots of graphs of interest are sparsely connected. Making the graph fully connected will result in a lot of unnecessary computation.
So are there any other solutions? In fact, we could ignore the topology mismatch and see whether it still works. To see whether something still works, two different parts need to be checked, whether it can still run and whether it can give good answers if it can still run.
Will the network still run if we have lots of different graphs as our data during training?
This question can also be broken down into two questions: does it still run as pseudo-code, and does it still run as code?
In terms of running pseudo-code, the answer is yes. Because everything is local, we are just iterating over local neighborhoods. We just have fewer local neighborhoods or more local neighborhoods for different graphs. The residual connections would definitely still work as long as the topology of each layer stays the same. The normalization will still be meaningful, for example, if we just divide the nodes by the different numbers for different sizes of layers in the layer norm. It’s only when the nodes are clustered ahead of time that the network may not be able to run. However, we seldom employ clustering because it does not make much of a difference (as mentioned before). Therefore, we can still run it as the pseudo-code.
In terms of running the actual code, we might have to worry about the size of the arrays allocated to make sure things fit. But the important thing is that the weights are not changed. The number of weights we have to learn does not change even if we have more different graphs.
Food for thought: Do we need to pad the graph with null nodes?
In CNN, we need to care about the pixels on the boundaries of images because they have less neighbor pixels. Usually we have zero padding for them or we just ignore these pixels. However, the nodes do not have the same number of neighbors generically. We don’t have to pad the graph with null nodes because we never have to compute anything for the null nodes.
After learning GNN, one question that pops up is whether we could have more weight sharing. In a ConvNet traditionally, the weights are shared within a layer but not across layers. We talked about the idea of the neural ODE, which was the perspective of a ConvNet as a ResNet solved as a differential equation. If the convergence of the neural ODE is wanted, we had to invoke some kind of weight sharing, at least hard or soft weight sharing across layers to ensure the limit existence since there were similar behaviors with respect to time. This requirement raises the question: should we share weights across layers?
We should do weight-sharing if the hierarchical structure has a self-similarity at different scales. This is one of the key design choices in the RNN family.
• Note that the word “layer” can be used in the same way we thought about the layer from a convolution point of view, which is what we backprop through. Once we implement weight sharing across layers, we can still backprop through it.
Let’s give an example of an RNN now. Consider a task where you are required to identify a person’s attributes based on their name. First, you represent the name by a sequence of characters, then represent these characters as a graph with internal labels. As such, we associate different names with different sizes of graphs, as shown in Figure 11.3.
One possible solution is to just use a GNN. However, when people started working on these problems, theyFood for thought: Do we need to pad the graph with null nodes?
In CNN, we need to care about the pixels on the boundaries of images because they have less neighbor pixels. Usually we have zero padding for them or we just ignore these pixels. However, the nodes do not have the same number of neighbors generically. We don’t have to pad the graph with null nodes because we never have to compute anything for the null nodes.
were actually motivated by the connection with signal processing. In signal processing, you might have seen finite impulse response (FIR) filters and infinite impulse response (IIR) filters. The difference between them is that IIR filters have internal states, like momentum form in the momentum acceleration method. So for the things that are sequential in nature, we can employ the analogy of an IIR filter and think of a network that has internal states. We can treat these self-connections as a kind of internal state.
Another thing worth mentioning is that FIR filters act on the input but the IIR filter consumes the input, one at a time, like the momentum consuming the gradient in each cycle. The whole point of an RNN is to have this eating input behavior used in the network.
So far, we have covered convolutional networks for images and generalized versions of convolutional network, graph neural nets.
2
We have the following are features for convolutional network for images. Features of convolutional nets for images:
1. Weight-sharing across space
2. Residual Modules to support depth. Recall how we use the skip-connection to make sure the gradient is not vanishing.
3. Pooling to more quickly support long-range dependency. Pooling can process the convolu- tional result quickly.
Features of graph neural network (topology of image → topology of graphs) :
1. 2. 3.
Weight-sharing across graph nodes. This is a more generalized version of CNN sharing since instead of grids, this shares weights across graph nodes.
modules in each layer can reference self, global state, and neighbor. This is because of the structure of graphs.
But need to aggregate symmetrically over neighbours. This is because we still need to maintain the invariance of the model.
Filters
We want to understand RNNs, and it turns out we want to first recall the filters in signal processing. They give the ideas of recurrent neural networks.
Definition 2.1 (FIR: Finite Impulse Response: generalized moving average). Moving average is taking the average while moving across different inputs. Recall in the previous lecture that momentum is just an exponentially weighted average.
This is the definition of convolution (discrete). Notice that this is the same thing behind con- volutional neural networks. Indeed, when we generalize this to 2D dimension we get convolutional neural networks.
What if instead of having finite signals, we have infinite signals? This motivates us to look into IIR: infinite impulse response. Before going to the definition, we first realize that we can not really express the infinite operation by just writing them out. We want a compact way to write out infinite operations. The key idea to do this is to use hidden states.
Question: why do we take the loss like that? We compare our estimated value to the real value. Question: why don’t we compute h from x? This is because we want h to be dependent on time and want our system to reflect that. In the real world, we do not know what real data and
we want to learn the filter dynamics.
We generalize the system described above to include non-linearities. (Aside, for the linear
features, recall the neural network lectures, we can replace linear features with MLP).
In MLP, we can add expressiveness to the model by making the layers wider and the model deeper. Now the question is how to make complicated RNN models expand expressiveness. Recall the figure of RNN from above (figure 3). We can approach the problem from two kinds of per- spectives: either we change the internal structure of RNN by a little bit, or like doing with other
deep learning models, we stack over layers.
Since RNN are recurrent, the gradient would involve a lot of multiplication. Therefore, in order to prevent the gradient from exploding, we want to make sure the output value of the activation function would be smaller than one, otherwise, the result would diverge if we have too many RNN time steps.
From another perspective, we can use Layer Normalization on the data to prevent exploding gradient. We can apply layer normalization in two ways in the context of RNN: either we normalize the hs above, or, we can do the layer norm in the x direction. We can also put layer norm inside wis. The reason why we do not usually use Batch Normalization is because it would not consider the recurrent part of the network, as in each recurrence calculation the statistics about the data would change.
On the other hand, how do we combat dying gradients? Can we use “skip connection“ like the one in ResNet? Yes, we can, but there should be discussions about which direction we should use
As we can see, the input is from the bottom, the hidden state are from the left. We have two options:
• Do ResNet skip connection in the vertical direction (orange lines) • Do ResNet skip connection in the horizontal direction (red line)
We get the vertical skip connection could work, but the horizontal one might not work. The reason is that the horizontal direction is for the hidden state, so we ignore the current input slightly. In many applications, adding horizontal skips changes inductive bias in a bad way (since it can not go back without knowing the ignored inputs).
In order to address the horizontal direction of the skip connection, we introduce the following idea.
Key Idea: add a memory cell: make horizontal paths have a way for gradients to flow backward when those gradient values make sense, but which can learn to block gradients as well.
Context is represented by the symbol ct and it should change but at a slower rate most of the time. However, when the input changes a lot, the context should change too.
We have learned that the natural architecture design of deep learning models is useful for different tasks. For example, convolutional structure for computer vision tasks, sequential structure in RNN for language processing, forget gates in LSTM models, etc. Although the architecture of RNN model is useful for sequential data, it still has limitations in understanding human languages. One reason is that different human languages behave differently (e.g. different grammar, word orders, etc.) To handle this problem, we need to design some new architectures. First, let’s look at the problem of machine translation as an example.
When humans read a sentence, we read one word at a time. Therefore, we design the RNN encoder to do the same thing. As shown in Figure 14.1, each word is input into the encoder layers sequentially and each layer outputs a hidden state that is used as an input to the next layer. Each layer shares the same weights.
Aside: Why do we call the hidden output a state?
In a dynamic system, we define “state” as something that summarizes all the past that is relevant to the future. That is to say, if we know a state, we don’t need to care about its past and that state will provide us with the information that we can use for the future. In the sentence example, the hidden state in the last layer should summarize all the previous words.
Up to this point, the normal RNN architecture works well. However, we want the model to translate the sentence into another language. Thus we need a decoder network after the encoder. The decoder will also use the sequential architecture and output the words in other languages, as shown in figure 14.2. The output in each layer will be used as the input into next layer.
Note that there is a bottleneck between the encoder and decoder. We hope that this bottleneck can include everything about the input sentence (recall “state”). That is to say, we are squeezing all the previous information into this tiny bottleneck. This may cause a problem because there may be some details lost that are irrelevant to figuring out the structure of the output sentence but relevant to which specific word is in that sentence. For example, we input “My green cat is an alien”. the workd “green” is used to decorate the “cat”. In the output sentence in another language, we need to know which word (“green”) is used to decorate the “cat”. Then our model needs to look back into the input sentence. However, the architecture we just built cannot look back at things at different scales.
Recall that for image segmentation, the U-Net architecture concatenates features from the earlier layers to the upsampled features in the later layers to allow the model to look back into the encoder layers. Also, images have a nice matrix structure for the model to process considering that we can clearly depict what the neighbors of pixels are. However, in the case of language, it is hard for the model to figure out the global idea of the neighborhood since it is not well defined in the language itself. Therefore, we need to add something to our network, that allows the model to look back at the words that are originally embedded, and allows information to flow along the layers.
Takeaway:
We want to add something in the language model, which allows the decoder layers to look back at what words are embedded. This is like a kind of memory where we are able to store something important in it and retrieve it later. We call this “Attention.”
In the last section, we explained the motivation for “attention” and how it behaves similarly to “memory” in a computer. There are two methods of storing memory. One is like an “array,” where we can store the value at an address and retrieve it later by looking into that address. The other method is like a “hash table,” where we have a key that corresponds to a specific value and can retrieve the value by querying the key. Attention will use the hash table structure.
Unlike the traditional hash table, now we can have a query vector that is not always exactly matched to keys in the hash table. So we need to design a “pooling” layer which not only can get an appropriate forward value, but also backpropagate gradients. Here are some ideas on how to design this hash table structure for attention.
Sometimes, we might need to take care about order information in the keys. For example, in natural language processing, “Prof. Sahai bites dog” and “Dog bites Prof. Sahai” makes the meaning totally different. One possible way to encode order/position/time is using complex vectors. If we let t be the position in sentence we want to encode, this can be differentiated from other sentence positions using complex expression.
In many contexts, we lack enough labeled data for supervising learning, so we may need to turn to un- supervised learning. There are two kinds of unsupervised learning: (1) Dimensionality reduction style (pre-regression) (2) Clustering style (classification).
Note that in these styles of unsupervised learning, there are no gradients or loss functions. But what if we can understand (1) and (2) in terms of loss function and gradient? That is to say, can we design these unsupervised learning techniques in terms of loss function, and then do gradient descent to update weights, which transforms this unsupervised learning problem into a “supervised learning”, or “self-supervision” problem. Next lecture, we will see how we can turn the dimensionality reduction style of unsupervised learning into two different kinds of self-supervised learning problems – one we call the “autoencode style,” and the other we call the “masked reconstruction style.
1. When talking about query, key and values, please use more clear examples to explain this concepts.
2. It would be better if the professor can give examples of word embedding.
3. We wish the professor can spend more time clarifying how position/time information is included in Attention. It would be better if the professor could present a trivial example.
4. It would be better if the professor talk more about the details of how the encoder/decoder is trained. For example, the professor mentioned we can just train the decoder alone without the encoder but we are not sure how. Currently it is very hand-waving.
We start by recalling the main idea of unsupervised learning: given some data {⃗xi}, discover an underlying pattern in the data. There are two basic types of unsupervised learning, which are:
1. Dimensionality reduction (e.g. principal component analysis). This is vaguely similar to regression, and the intent is to summarize or distill important information from the data. The algorithm we typically use for principal component analysis involves solving for a singular value decomposition, and thus is an eigenvalue computation-style algorithm.
2. Clustering (e.g. K-means). This is vaguely similar to classification, and the intent is to group related data points. K-means is typically solved using Lloyd’s algorithm (Lloyd, 1982), which is an iterative alternating minimization-style algorithm.
We provide an overview of some of the possible uses of unsupervised learning in the two subsections.
The naive approach when doing supervised learning on datasets with unlabeled data is to simply discard the unlabeled data. Unsupervised learning allows one to take advantage of unlabeled data, and potentially improve upon the mapping found by only utilizing the supervised learning algorithm.
For example, imagine a large, high-dimensional dataset of dimension d with a small number of labeled points n ≪ d. A pure supervised learning algorithm could not be applied to this dataset, as it would need to learn how to label data with d dimensions while only making use of the n labeled points. There are a couple of ways to resolve this using unsupervised learning:
• A dimensionality reduction algorithm can be applied to the entire dataset, including the unlabeled points. If the unsupervised learning algorithm is able to learn a mapping down to a low dimensional space with dimension d′ ≤ n, the supervised learning algorithm could potentially be applied to the low-dimensional representation of the dataset and successfully learn the proper labels.
• Similarly, clustering can be used to predict labels for unlabeled points in the dataset (assuming that points in the same cluster have similar labels). If the number of labeled points post-clustering n′ ≥ d, then the supervised learning algorithm can be applied to the new dataset with inferred labels.
For many problems in, for example, natural language processing and image processing, it is much easier to obtain unlabeled data than labeled data. The models often require large quantities of data to train, so it is important to be able to utilize unlabeled data. In the context of deep learning and deep neural networks, this requires some extension of our unsupervised learning concepts like dimensionality reduction to work with gradient descent or another, similar approach.
Another possible use of unsupervised learning is in exploratory data analysis. High-dimensional data is difficult to understand and visualize, and sometimes one might want to use dimensionality reduction and clustering techniques to simplify the dataset for visualization purposes. This is also done in the context of deep learning, but will not be covered in this lecture.
This feels like a supervised learning problem. We are learning a mapping with parameters encapsulated in A and B that minimizes the difference between our reconstruction ⃗x and our input x. Specifically, it
is a self-supervised learning problem, since our target is a function only of the input itself. Note that the identity is not expressible via this mapping since we pass through a bottleneck layer with dimension k, and the identity is a rank d linear map. The product BA can only produce a map of rank k, as A has dimensions k×d and B is d×k. Thus we are learning an approximate reconstruction of our original data that minimizes the error for a given botteneck size k.
In general, we can replace A and B with non-linear encoder and decoder neural networks. This approach is called an autoencoder approach, since it encodes our input ⃗x in a low-dimensional representation ⃗y and learns how to reconstruct that input. Historically, autoencoder strategies were often used to first learn an initialization of the neural network weights, before training the network using the actual targets. This approach to neural network training was eventually replaced by end-to-end supervision, but has regained its footing in recent years as a pre-training approach for large models.
This pure form of autoencoder is not always an ideal way to solve a self-supervised learning problem, and many variations on the basic structure outlined here exist. Some of these are discussed in the subsequent section (Section 15.4).
We have spent a lot of time understanding the idea of self-supervision where we only have access to unlabeled data (just like in unsupervised learning) and use data themselves to generate labels. One example that proved such learning scheme useful is doing PCA-style dimensionality reduction “purification” in an autoencoder style with self-supervision. Apart from PCA, k-means style clustering where we assign unlabeled data to groups is another kind of traditional unsupervised learning. In the first part of the lecture, we attempt to make k-means clustering compatible with gradient descent. Then, we will connect to the idea of attention and introduce transformer models.
In the previous lecture, we talked about RNN which uses states to memorize knowledge. Attention mechanism can also be viewed as a kind of memory, which is a soft approximate hash table. Here we try to only use attention to “write things down” instead of using states as internal memory. Now, what we can do is to write memory and query memory. Then it comes to the first key idea of transformers: “I don’t need internal memory if I can write notes to myself”. What happens is we drop recurrence from the RNN approach, but keep weight sharing (across time).
Based on that, we can intuitively consider transformer models as GNN, which share weights between different nodes. Nodes in transformer models that pass across time can also be viewed as a fully connected graph. These basic ideas are similar.
This idea relies on the attention mechanism. We have introduced two ways of understanding attention: 1. Soft approximate hash table
2. Queryable softmax pooling
In the field of computer science, we can consider attention as a hash table; but in deep learning style, it is a pooling layer combined with softmax.
Last time, we talked about different ingredients that go into building a transformer [3]. The basic core ingredient was the attention mechanism, which is just a query-able softmax pooling. We also talked about the multi-headed variant where we can execute many different queries at once into many different tables.
There are two kinds of attention: self-attention and cross-attention. The difference lies in whether we are querying the same table we are sticking stuff into or a different table. Attention by itself has no learn-able parameters associated with it so we need to have learn-able parameters making the queries, keys and values. On top of that, we have ResNet-style blocks to combine something designed to use the attention mechanism with standard building blocks like multi-layer perceptrons and layer normalization to make something we can stack and make deep.
The input to the transformer has no sense of ordering. To have some way of imposing order, we use position encoding to encode the positions of input tokens. The one we talked about last time was inspired by the hands of the clock, i.e, sines and cosines. This method is friendly to matrix multiplication and has the nice analogy to advancing the hands of the clock or moving back the hands of the clock. It gives us an absolute encoding of position which is friendly to relative position querying.
There are many variations of position encoding. In particular, there is a distinction between the one we talked about in last lecture and the one that is used in practice. Last time, we talked about implementing position encoding as concatenating our input to the position encoding (fig. 19.1). Here, we can think of the input to the transformer as having two parts: (1) the data and (2) where that data was in the sequence.
While this ’concatenation’ method can be done in practice and is very logical, a more common approach is to have the input and position added together (fig. 19.1). The question that arises is why this even works as it seems less natural.
First, recall that the vector sum is happening in a very high dimensional space (512, 1024, ... dimensions). In such high dimensional spaces, the position encodings are occupying a very limited set of the space which leaves a lot of space for the input data. Then, suppose we want to query for the position only. When we take the inner product, by linearity, we get (1) the inner product of the position oriented query and the input, added to (2) the inner product of the position oriented query with the position encoding. If the position encoding has a high correlation with the query, then (2) will be high whereas the position oriented query and the input will be something completely different. Therefore, the inner product has the ability to be able to pick up positions vs things in the input. Of course, there will be some interference with the query but not that much and so we expect that the sum can still make position encoding work. Thus, in high-dimensional space, adding is also fine for the inner product to be able to pick up positions versus things in the input. Addition is used in practice and usually has better performance.
Question: We want to use complex numbers because matrix multiplications are rotations (of the clock) but we are not actually using complex numbers. By using only sines and cosines, so do we still have some property of rotation?
Answer: The idea of the complex numbers rotating around is used as inspiration for the hands of the clock since we know complex multiplication can result in rotating these relative positions. Yet, if we represent complex numbers using a vector of two numbers, then we can think of complex multiplication as a linear operation represented by a matrix. This means that with matrix multiplication, it is easy to express a rotation, so the same property we had with the multiplication of complex numbers, we can basically inherit from the vector representation using sines and cosines.
Question: If you want to rotate the hands of a clock by some angle, say 10 degrees, then we have to compute the sines and cosines of 10 degrees to populate the transformation matrix. Yet, if we look at the linear and non-linear operations in a transformer that define the entire path of the query or keys, we don’t see a sine or cosine as a non-linearity. So how is it that we are actually able to move by 10 degrees?
Answer: The queries are the result of a learned transformations. We don’t need to explicitly compute the sine and cosine computation at runtime. The transformation just has to be learn-able during training. Whatever the sines and cosines we actually need will come out during training. It is easy to learn a rotation as a transformation.
Follow Up Question: Does that require a lot of weights if we want to learn all the relevant angles?
Answer: Yes, there are potentially many different relevant angles we need to learn and so indeed, we would have many weights. The hope is that the model has enough expressive power to learn the transformations we need. In fact, transformers are very large models with high memory cost. For a 1000-length sequence, each with hidden dimension 1000, we can will need 1 million parameters. In addition, to compute softmax, we need O(109) number of multiplications and additions.
3.2 MLP
Finally, the output is passed through some MLP (fig. 19.2). We need the latter because we need a non-linearity stronger than the non-linearities involved in attention.
3.3 Layer Normalization
There is also a question of why we need layer normalization and what is it doing. Recall that the attention mechanism is computing an inner product. From the query’s point of view, the inner product is trying to say, of all the different keys, which is the one which is the most in this direction. Then we can see that layer norm is useful because it gives the query a particular direction.
Question: Do we need to normalize the keys too?
Answer: Not really. The same query is applied to a bunch of keys but for some keys you want them to be able to say, “i’m in this direction, yes, but i’m not that strong. If there is another direction that is stronger, then pick that on”. Whereas the query is really picking a direction. If we change the norm of the query, we would still end up with the same ordered sequence of winners of the keys but all that would change is what happens with the softmax, in terms of how well it does the normalization. But that’s not what we want so we don’t want to impose a normalization constraint on the keys.
3.4 Query Standardization
Input standardization involves three parts. Take data,
• subtract mean
• divide by standard deviation
• possibly shift the mean and standard deviation by something that’s learn-able.
These steps involve two sides: normalizing the size of everything and moving the means around. When it comes to preventing exploding gradients or vanishing gradients, the means are not relevant whereas sizes are. In the context of a transformer, when it comes to what we want from the query, we want it to have a norm that is something reasonable and doesn’t change a lot between queries. Then, the query doesn’t need biases and only needs scaling. Hence, one of the modifications to the Transformers that is often done is to remove the bias and stick to the scaling.
Question: When we are looking at a particular encoder layer, we have a self attention block inside which is looking into a table. Are the contents of the table the same for different input positions?
Answer: Yes. They are only different if we enforce some sort of sequentiality. On the decoder side, the sequentiality is often required.
We usually have two different design choices when it comes to the inputs of the attention layers. For instance, in an arbitrary decoder, which output of the encoder should the attention layer of block 7 (of the decoder) be looking at? One option would be to have every block look at the last output of the encoder. So if the encoder is 20 levels deep, then it is looking at the output of level 20. Every single layer of the decoder here is looking at the output of the last layer, layer 20. This follows the auto-encoder spirit where the output of the encoder is a distilled version of the output. Typically, this is what is done.
Another option is in the style of a U-net where we can look over at different levels of fineness of the encoder. So we can imagine a transformer decoder looking at the middle layers of the encoder for attention.
Question: When we say tables or weights are shared, are they shared horizontally or vertically?
Answer: The standard answer is horizontally. For the same layer, we have the same weights and if we have an attention block, it will have the same table. But at a different layer, there will be a different set of weights and a different table that it looks into. That said, it is often sometimes done to have weight sharing across layers as well.
3.5 Data
Transformers don’t have a strong inductive bias so we need a lot of data to train. We get data scraped from the web and hopefully if we get all of this, we can get things to learn the underlying representation of what that language is. That’s what we want our model to actually capture and then we can use it for different purposes.
4 Word Embeddings
For us to be able to learn from text data, we need to have a way of representing words such that their representations are meaningful. For example, in computer vision, the pixels of images mean something. They don’t mean much, but they mean something. Thus, maybe if we had a more meaningful representation of words, then learning downstream tasks would be easier!
We represent words with embeddings, but how do we learn these representations? The problem with words is that in the English language alone, you have something of the order of 100,000 words, with 8,000 common words. That’s a lot of words. The default way of thinking of categorical variables using one-hot encoding is really annoying for words. We would have lots of words close to each other in meaning but which would not be reflected in this embedding. For example, ‘actual‘ and ‘actually‘ are going to have the same distance from each other as ‘actual‘ and ‘banana‘. Therefore, this encoding doesn’t tell us anything about the meaning of these words.
There also is a question of why we want vectors corresponding to similar words to be close to each other in the embedding space. All of our functions are continuous during training, so we expect to have the basic behaviour of things going in that are close will give things that are close going out. In particular, for losses, during training, we have we want a kind of continuity where words that are close get less words than things that are far. Then, if we have representation with vectors of similar words being close to each other, then hopefully if we make a small mistake during training, we will have something that’s also close.
Now that we want our vectors to have this property, the question becomes, how do we learn them? The key idea in word2vec is that words occur in context with other words, i.e, the words that surround a word tells you what a word is like. Therefore, we want to find a representation that reflects this idea of having similar words have close embeddings.
Suppose, we want to learn the embedding for the center word c (fig. 19.3). We create a prediction task, such that, we try to predict the neighbors of the center word. To generate this probability, we use the standard go to way, i.e, we predict the words that are closest to the center word in terms of the inner product. Thus, we can take the embedding for a word o given the context and sum up over all other choices that could be there for the context word.
Question: Intuitively, what we want to do is to have an encoding of embedding of words so that given the neighbors, we can predict a particular word. Here, with word2vec, we are doing what feels like the opposite, i.e, we are predicting the neighbors given a particular word. Why do we do it this way?
Answer: The answer is simply because it is easier to do it this way. If there are multiple inputs and a single output, we need to figure how to combine all this information from the input. It is possible to do it, maybe through a weighted average of the neighbors, but at the end of the day, this is a surrogate task, so we can try the easiest thing first.
Note: There is a subtle distinction that two words should have the same representation if they are roughly interchangeable with each other in a sentence, not that they sit close to each other in a sentence.