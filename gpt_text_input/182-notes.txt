Gradient Descent: SGD and Learning Rate
Optimizers are used to solve for models’ parameters. The optimizers themselves usually have their own hyperparameters (learning rate, choice of the optimizer, etc.). The most basic optimizer is gradient descent, an iterative local optimizer that we have touched on briefly in Note 1. Just to recap a bit, the idea is to change parameters a little bit each time and see how that changes the model’s performance. If we have some parameter vector θ ∈ Rn, we update it according to:
where θt denotes the parameter at time step t, η is the learning rate hyperparameter, and Ltrain,θ is the training loss function. At each iteration, we take a small step in the direction opposite the steepest ascent of the training loss function. If we look at a first-order Taylor series approximation of the training loss function, it is clear that we are looking at the training loss function locally, and asking the question, “where will we be if we take a small step in this direction?" This lets us gain an understanding of how the loss landscape depends on our parameters.
If we look at our training loss function more closely, we notice that it includes a huge summation over the entire dataset. Remember here that ltrain(yi,fθ(xi)) is a surrogate loss function that is compatible with our choice of optimizer, and R(·) is a regularization term that ensures our parameters θ satisfy some predetermined constraints (like sparsity). The size of the dataset n is typically huge, so evaluating Ltrain,θ can be extremely wall clock time consuming. To address this, stochastic gradient descent (SGD) is typically used.
In SGD, instead of all n training points, nbatch << n random samples are used. Our loss function now becomes.
If we take a random draw of a subset of points, we get an estimate of the average. We can assume that this gives
us an unbiased estimate of the actual loss function across the entire dataset, albeit one with some amount of variability (because nbatch << n). In a deep learning context, the batch size nbatch is the largest that your
system can sustain, and ensure adequate wall clock time.
Figure 1: Contributions from each data point in the dataset balance, so the gradient is zero.
Why do we use SGD? The standard Deep Learning answer is that we can’t afford to look at all the data points, but we can afford to look at a few of them. Beyond that, we can expect that the loss surface has different regions of varying “quality" (flat/non-flat, near global optima, etc.). We have no expectation that we’ll begin training in a “good" region, so going in the right general (but not exact) direction might help in this scenario. It’s also very probable that our estimate based off a few points very closely aligns with the direction of the gradient across all data points. We’re only taking a small η-sized step in the direction opposite the gradient, so we only need a level of precision that ensures we take our step appropriately.
1.2 Learning Rate
The updated parameters θt+1 are a discrete time dynamic system, and the learning rate η controls the system stability. If η is too large, we have instability and our model’s loss will diverge. If η is too small, we make (almost) no progress towards getting a working model. The badness of choosing a large learning rate η is very clear, if our loss function diverges, our model doesn’t learn anything. Although choosing a small learning rate does not cause divergence, it is an equally bad scenario as it corresponds to more real world wall clock time, energy usage, and cost.
Changing the step size η might help us (or not) depending on the loss surface. We can use a learning rate schedule to enforce some step-size-changing policy. Doing this is especially useful in some non-vanilla niches of machine learning, such as online learning (where data is only made available to you in a sequential order).
A step size selection that causes convergence for regular Gradient Descent might not guarantee convergence for SGD. If we pick η so that the gradient doesn’t diverge when taken across all points, we can generally expect that it won’t cause divergence when evaluated across some points. What actually needs to happen, is that the model needs to converge to some setting that works well. Convergence happens when the parameters stop changing in our gradient descent iterations. Once we settle so that θt+1 ≈ θt, we converge unless the gradient term −η∇θLtrain,θ suddenly spikes. At optima, derivatives/gradients should all be zero, meaning we converge. The derivative is zero because the sum of contributions to the gradient at all the different points cancels out to zero (see Figure 1). In SGD, we’re not using all the data points, so we might be at some optimum, but our small-batch estimate of the gradient will likely be unbalanced (see Figure 2). So we might step away from an optimum and “only sort of converge." This wiggling behavior is illustrated in Figure 3.
So to actually converge, we may need to make sure that η is decaying.
There are other explanations for why learning rate ought to decrease over time. Picking a large step size at the beginning may help explore the space in a “path less traveled” sort of way, while a small step size at the end helps refine our evaluation.
It’s also worth noting that there’s technically a difference between oscillatory and random walk type behav- ior. A non-decreasing step size in gradient descent might cause oscillatory behavior, but the stochasticity in SGD will cause random walk behavior.
Nonetheless, in deep learning, even a constant step size may lead to convergence.
The time it takes to go through an entire shuffled instance of all the training data set is called an epoch. Shuffling (and reshuffling) data between epochs can help avoid us learning subtle patterns that arise because of the ordering of our dataset. Sometimes, this sort of structured approach and commitment to see all data points is not desired. Because datasets can be so large, if we just randomly sample from the training dataset we are likely to get similar results. In these cases, sampling with and without replacement are equally performant.
Generally, getting an optimizer to work well involves lots of consideration to be taken as to the application and the underlying hardware that the model runs on.
2 Intro. to Neural Nets via ReLU (Rectified Linear Unit) Nets
2.1 What is a Neural Net (Differentiable Programming)?
A neural net is an object that is easy to take derivatives (e.g. Analog circuits realized as computation graphs with (mostly) differentiable operations compatible with nice vectorization). Neural nets are prevalent today because they’re able to exploit gradient descent/SGD to do learning/computation.
2.2 Two goals of the analog circuits
(a) Expressivity: the ability to represent a wide variety of functions.
(b) Learnability: the ability to learn functions without too much trouble.
2.3
Expressivity is important because we need to be able to express patterns we’re interested in, which we don’t know and need to be learned. We can’t guarantee that some model is expressive enough to capture these patterns. Traditionally, the solution to this is to be expressive enough to capture any pattern. If a model can learn any function, to within some amount of precision, we call it a universal function approximator.
Example of Neural Networks
The figure below shows a 1-D nonlinear (Blue) function, piecewise linear (Orange) functions, and data points (Green). As shown in the figure, the piecewise functions describe the nonlinear function pretty well. Our goal is to find a set of piecewise linear functions (Orange) that best match the nonlinear function (Blue) based on the data points using Neural Nets.
Then, how do we create the piecewise linear functions? We do it by forming a linear combination of elbows (Rectified Linear Units) that looks like:
 The standard ReLU function is shown below.
Also, the standard ReLU function can be modified if needed. For example, x can be replaced with wx + b, so that the modified ReLU function becomes f(x) = max(0,wx + b), which can be represented by a computational graph like this:
We can manipulate an elbow to fit our line by changing w (weight) and b (bias). More specifically, Elbow is located at −wb and slope = w. Here, w and b are the parameters(θ) we want to minimize using a loss function.
To provide the circuit interpretation of ReLU, we are going to look at an example of a rectifier circuit, which is shown in the figure below the computational graph. It is composed of a diode and a resistor. The diode prevents the current from flowing in the opposite (or negative) direction. Setting the positive direction of the current to be from left to right, it means that the current can never flow in the negative direction (from right to left). All negative currents will be set to zero resulting in Vout readings being zero. On the other hand, positive currents (from left to right) will go through the diode resulting in Vout readings on the other side of the diode. In this example, Vin is x and Vout is f (x).
 The above image illustrates the big picture of what our simple ReLU Neural Net looks like. We tune the elbows to fit our training points, the optimizer is trying to make the final real number (loss) as small as possible across all the training points. To push the loss down, how much do I have to move y, b’s, and w’s? This process goes backward and the elbow changes.
With one layer and an arbitrarily large layer width (number of ReLUs), we can approximate any function. Historically, people have just stuck with using single-layer universal approximators. So why move into the domain of deep networks? We use deep neural networks because they tend to exhibit learnability, meaning they can actually be trained to learn patterns of interest. When we make this shift to deep networks, some of our parameters become redundant, so the number of parameters is not directly (though highly correlated with) the degrees of freedom our network has.
2.4
Asides for ReLUs
Non-saturating
They work well with gradient based methods because they are non-saturating (unlike sigmoid, for example). For ReLUs, the gradient doesn’t depend on the value of x other than its value relative to the threshold. For a saturating non-linearity, the magnitude of the gradient approaches zero as the magnitude of x itself increases to infinity. Using a non-saturating non-linearity ensures that our gradients don’t diminish in deep networks with long partial derivative chains.
Dead ReLUs
If both weights and biases are distributed with normal distributions, the ratio wb will be a Cauchy distribution. This causes some ReLUs to be located far apart from the others. These ReLUs are considered "dead" since they output 0, and changing the weights or biases slightly doesn’t affect their output.
1 Pragmatic view of regularization
• By adding the penalty term, we hope to shape the optimization to favor something that we like.
• Note that (explicit) regularization does not change the expressive power of the network. Therefore, you only change what the optimization is favoring.
2 Regularization in Least Squares
For a more intuitive understanding of regularization, we can look at least squares. Let’s start by reviewing the problem: we have a matrix X and vector y, and we want to calculate the weights w that best approximates Xw = y.
We can add regularization to OLS. One version of regularized OLS is called ridge regression:
λ is the regularization parameter. It penalizes high-magnitude weight vectors (can you see why?). We can select different λ values to control the behavior of Ridge regression. When λ is higher, the weight magnitude penalty is more severe; when it’s lower, the penalty is lighter, which allows Ridge regression to settle on weight vectors with higher magnitudes.
Note that when λ = 0, ridge regression becomes unregularized. This unregularized formulation simplifies to the OLS problem and solution. Intuitively, setting λ = 0 tells ridge regression to disregard the weight vector’s magnitude altogether, allowing it to solve the problem as though it were simply OLS.
We can also solve least squares with gradient descent. Gradient descent is a process where we aim to improve our model by repeatedly moving its parameters in the direction that minimizes the cost function.
This gives the interpretation that for large singular values, gradient descent moves much more and faster than smaller singular values, which can take longer to converge. Thus, we first fit in the largest σi direction, then the next and so on. When working with very small singular values that have the propensity to give large solutions, we may converge to something very large, but slowly, especially without regularization.
Solution: Use gradient descent, but limit the number of steps to a finite number to avoid overtraining. This is sometimes called early stopping.
There are a few different tricks that can be used to avoid explictly including Ridge regularization while still accomplishing the same result, either by adding extra entries or extra features to the data matrix.
Implicit regularization is the regularization that occurs when we aren’t consciously doing any regularization. When performing explicit regularization as above, we must specify a specific regularization hyperparameter and modify the training data, model architecture, or loss function. In contrast, implicit regularization is an unexpected benefit stemming from our choice of optimizer. We will see that choosing gradient descent as our optimization algorithm, combined with the large size of DNNs, provides enough regularization for DNNs to generalize well without us intentionally restricting the parameters.
Observe that this is roughly a linear function with an extremely small slope if σi is tiny. In this situation, GD barely moves in the early stages even though it will eventually converge to a very large value, as pre- viously discussed. Together with early stopping, this means that GD is trying to do something like ridge regularization for us because it will resist enlarging the directions corresponding to small singular values. Early stopping is when we stop the training process because validation performance has gotten worse or has not improved for a long time. It is important to note that GD, when initialized at zero, will converge to the minimum-norm solution. This is a good exercise for the reader to verify.
To summarize, there are three kinds of regularization: explicit regularization, data augmentation (adding fake observations or features), and implicit regularization (optimizer has an implicit regularizing effect). Regarding DNNs, the combination of the min-norm seeking behavior of gradient descent and the feature augmentation that is implicit when using large networks gives a lot of regularization even if we weren’t thinking about it.
Gradient Descent is the tool that we understand the most and is often our first choice.
• The loss function will typically contain training data with labels that we would like to predict with good accuracy.
• The singular values of the data matrices involved greatly matter and can lead to overfitting, slow convergence, and unreasonably large parameters.
• In solving this problem, we usually add regularization, which we can understand through SVD as bounding the solution to the smaller singular values while leaving the solution of the larger singular values unchanged.
• It’s desirable for the input to our ReLU layers to be centered near 0 and to be close to where the “action" is, otherwise poor initialization can lead to dead ReLU nodes from the beginning. Dead ReLUs are problematic because they make all the computations with the weights and biases, but are always less than 0, leading to wasted computations.
• Additionally, dead ReLUs are usually unable to recover, leading to a portion of our network being unable to learn further.
We want to achieve a distribution of N (0, 1) at initialization, the outputs of the layer will be standardized. So the main idea is that we want our inputs to have a distribution of N (0, 1) at initialization.
Current Basic Folk Wisdom:
• Use whatever worked on a related problem. (e.g., pre-trained network, literature review, etc)
Xavier Initialization aims to initialize weights such that the variance across every layer is the same. Let d be the fan-in (number of inputs of the layer) of the target layer.
Problem: This doesn’t work well for ReLU!
Why? If the previous layer was ReLU, we expect half of the outputs to be 0 on average.
There are typically four approaches that are used to initialize biases:
(a) Treat fan-in as d + 1 and use Xavier Initialization
This treats the bias as a weight in order to use a weight initialization technique, which is why we use
d + 1 instead of d for the fan-in. (b) Just use bias b = 0
This is essentially letting our optimizer initialize the bias in the first few steps. (c) Just use b = 0.01
Empirically, people have seen that this sometimes performs better than b = 0. (d) Just use any small, random number
This claims that it doesn’t matter what small, random number we select, since our optimizer will quickly change it anyways.
Vanilla gradient descent has many benefits, but speed is not one of them. The reason behind that is the constraint on the learning rate.
The idea of momentum is finding a safe way to make the η bigger. We know that the dimensions with larger singular values start to oscillate quickly. So the thought is to somehow low pass filter (LPF) those directions that would otherwise oscillate if we take smaller steps. By doing that, instead of moving in the direction of the gradient, the update is moved towards the direction of the average gradient. The functioning of the
simplest LPF is where this idea of averaging originally came from.
This low-pass filter (LPF) will regulate voltage.
This average is beneficial, as it only requires one internal state to keep to track of, and β will determine how long averaging will go for. This technique is commonly referred to as exponential smoothing. Now we want to apply this technique to create a more stable gradient descent.
In order to achieve gradient descent with momentum, we want to replace our gradient term in regular gradi- ent descent with our low-pass filter equation applied to our gradient. Gradient descent with momentum can be considered as a “smooth gradient".
The main difference with momentum is that gradients at previous time steps have an impact on the current gradient’s value.
The vanilla momentum uses the gradient where the current weights are. In the Nesterov momentum, we take advantage of the fact where we are going by peeking into the future, as given from the first part of the momentum term (9). This is because we already know where we are going to end up, and by taking that new information to the calculation of the gradient at this step, we can take a bit of an advantage on learning.
The simple perspective of the origin of the adaptive methods comes from the fact that even with momentum, the largest singular value and the smallest singular value are the ones that govern the process. Values in between are not relevant for setting up the parameters. So the simple idea is that instead of having a single step size, use different step sizes (ηi) along different dimensions of the parameter vector. In vanilla gradient descent, we go down in the steepest gradient direction. But in the adaptive methods, we no longer go in the steepest direction but still in a downward direction.
The formulation of Adam’s method where different step sizes in a different direction are shown in equation 12. In the large gradient directions, it takes smaller steps and vice versa. That way, it takes evenly sized steps along different directions. To track the size of the gradient, a vector V is introduced, which depends on the element-wise product of the gradient vector. Then the gradient update for the ith parameter is computed with the square root of elements of V to make the units right. A constant (ε) is added to the denominator to make sure it is stable.
Topics covered last time were Key ideas & Convnet manifestation. Today, we continue with Con- vnets, focusing on input standardization and activation normalization. The lecture included the following topics:
• Respect locality
• Respect ”invariance” within data for problem domain • Support hierarchical structure (Fine → Coarse)
• ”Room to play”
which are related to the following realization techniques: • Convolutional Structure
• Weight-sharing
• Data Augmentations
• Down Sampling
• Lifting to more channels as we get coarse • Diversity to support learning
• Dropout
• More layers
2 Locality
We have the observation that many useful image features are local. To tell if a particular patch of an image contains a feature, It is enough to look at the local patch. We get a different output at each image location using the same filter. We assume locality exists in CNN, where inputs that are more close to each other are more correlated. In the context of image inputs, this assumption is qualitatively valid because of local patches of similar color, texture, or lighting. Figure 8.1 shows an example of locality.
3 Weight sharing
Weight sharing in CNN is that a convolutional kernel or filter scans across the whole image input and produces a feature map, so every neighborhood of pixels in the image is processed by the same kernel or filter. That is, the weights in the kernel or filter are shared. We don’t assign weight to each individual pixel.
Example: Suppose we have a 5x5 gray-scale picture and a 3x3 kernel (Figure 8.2). We apply the kernel to one pixel of the input image and its surrounding local patch with the same size as the kernel. We get a number from the previous calculation, add the bias, apply non-linearity, and we finally get the new output of one depth of that one pixel of the image. When the image has multiple channels and each channel is convolved with a different kernel, this operation is called depthwise convolution.
The weight-sharing structure provides translational equivalence. The resulting activation map remains the same under the translation of input feature map. Weight sharing also significantly reduces the number of weights of the network, which ensures higher computational speed.
Sometimes, the padding operation is introduced. Padding is the addition of crafted pixels on the sides of the image so that the pixels on the borders are not lost from the output of convolution. Padding size is usually one less than kernel size. If the crafted pixels are all zeros, this operation is called zero-padding. If the crafted pixels are mirrored values from the original input, this operation is called mirror-padding. Deep learning application typically uses zero-padding for practice.
Figure 8.3 shows an example of convolution operation with padding.
Note: depth of one input/output layer in the CNN is a term for the number of channels of the layer. For example, the depth of an RGB image is 3.
Receptive Field is a term borrowed from biological vision. A particular pixel of the output generated by the neural network has a dependence on some, but not all pixels in the input. So this term defines what pixels in the original image this output depends on. Figure 8.4 is an example of receptive field.
If we keep adding convolutions layers on the images, the receptive field will grow linearly. However, linear growth is slower than desired. Faster growth is desired because the neural network can easily learn the full hierarchy.
Therefore, we need a way to do dimensionality reduction on feature maps, i.e., down sampling.
For example, if we want to transform a 2x2x4 region of the output into a 1x1x4, we need to represent 2x2 elements with 1 pixel. This is also known as pooling.
There are several ways for pooling, including
• average
• max
• pick-one, i.e., pick the pixel at a certain location of the computed local patch, and discard the rest
• weighted average
Pick-one pooling seems a waste of computation because it discards most of the computed pixels. This method of pooling is implemented by stride instead, which only computes the pixel at the desired location to save time and computation.
The weighted average can be viewed as a convolutional layer and a stride.
Pooing layers make the receptive field grow exponentially. In addition to dimensionality reduc- tion, pooling also provides local translational invariance, allowing the CNN to be more robust to features varying in their locations.

1. Convolutional layers
(a) A way to avoid needing millions of parameters with images
(b) Each layer is ”local”
(c) Each layer produces an ”image” with roughly the same width and height, and number of channels = number of filters
2. Pooling: moving from fine to coarse but more abstract
(a) If we ever want to get down to a single output, we must reduce resolution as we go
(b) Max pooling: downsample the ”image” at each layer, taking the max in each region. Max is differentiable. It stops gradient descent to the smaller value which it discards, so it ensures the gradient descent to the important feature.
(c) This makes it robust to small translation changes.
3. Finishing it up
(a) At the end, we get something small enough that we can ”flatten” it (turn it into a vector), and feed it into a standard fully connected layer.
(b) Suppose the input size is W, the kernel size K, the stride S, and the padding P. Input and kernel are both squared.
Regularity is needed for the neural network, which is the invariance to some insignificant changes to our knowledge. For example, it is expected that a pattern can still be recognized when it shifts horizontally by 2inches. However, these insignificant changes are not present in the training set. Data augmentation can apply these changes to the input images and feed the newly generated data to the neural network to train for robustness.
In practice, in order to save space, the newly generated images are not stored along with the original huge dataset. These images with data augmentation applied are generated on the fly. Modern deep learning applications typically never see the same image twice, because all data have been augmented during the training. During data augmentation, the label along with the input image normally does not change.
Data Augmentation represents some of the easiest ways that people can transfer the domain knowledge into the training process. We humans understand what are insignificant features by our evolved vision system. This knowledge can be encoded into the practice of data augmentation and drop it into the training. This practice can help the neural network know what is important based on our knowledge.
Basic augmentations include autocontrast, rotation, translation, posterization, etc. Some basic augmentations are exemplified in Figure 8.5. More aggressive augmentations include cutout, Mixup, CutMix, and PixMix [3].
The design choice with zero mean and unit variance is good because 0 + 0 = 0 and 1 ∗ 1 = 1. We want to make sure that the neural network is learning with the greatest sensitivity when the dataset is essentially changing. For example, the elbow point of a ReLU network can capture where the predicted values vary. It will be hard for a ReLU network to learn if the elbow points are not aligned with the output action space. Standardization is a way to align inputs to where the function can learn more easily.
The expressive power of the network is not lost with standardization and normalization. The expressive power is embedded in the bias and the weight terms, which can shift the mean and variance.
We perform standardization on the training set by subtracting the mean and scaling the variance computed from the training set. We can use the same mean and variance from the training set to normalize the validation set.
We demonstrated in the above aside that it was possible to preprocess the data such that a specific data point which we know with high confidence can be weighted proportionally to its spread. Note that, in doing so, the data point now has a mean of zero and unit variance. But that’s exactly what normalization is!
In the toy example above, normalization had the effect of correctly weighting points based on their importance (in this case, confidence). Normalizing our training data before inputting them into neural networks has a similar effect of scaling the parameters such that the raw magnitudes of the values don’t necessarily impact the gradient calculation during the gradient descent backpropagating step (if we had massive, un-normalized values, the gradient would always be large). Moreover, the data points are generally centered at zero because the ReLU layers are often initialized at that point, making the neural net most sensitive and expressive when input values are around zero. Thus, normalization can additionally decrease the neural network’s training time.
We are not generally provided with prior knowledge of the probability distribution from which our sample X data are drawn, so these values must be derived empirically, i.e. from the data set X itself during training time. Let us focus firstly on ways such an empirical average E[X] can be derived.
Consider the following scenario where we have n 100 x 100 images in our data set, each with 3 channels: red, green and blue, with values between 0 and 255, inclusive. The following are ways we can compute an empirical expectation for any given point in any image:
1. For any given position (x,y) in channel i, take the expected value corresponding to that point to be the average of the channel i values at (x, y) across all images, i.e. the expected value for position (50, 50) in the red channel is the average of the red values at (50, 50) across all images. This can be done across our entire data set, with or without the addition of augmented images.
2. Use the numeric midpoint of the range as the expected value for all positions (e.g. choose 128 for range 0-255). This method is simple, but it assumes that the color values across all images are distributed evenly around the middle of the range. Consider what might happen if our data set consisted only of dark images (images with channel values between, say, 0 and 12). Would our normalization still be centered at 0?
3. Take the expected value for position (x, y) at channel i to be the average of all the channel i values for that image. For example, for any given image, the expected value for position (50, 50) of the red channel would be the average of the red channel value across all positions of that specific image.
4. Set the expected value to be an average of the values within a local patch of pixel positions in the same channel within the same image. For the expected value for position (50, 50) in the red channel, we may want to take the average of a 3 × 3 patch centered at that coordinate, i.e. the rectangle [49, 51] × [49, 51].
Food for thought: What are some of the advantages/disadvantages of some of the expected value definitions listed above? Especially for the definitions that average within a single image, what information may be lost in the normalization process?
These are just a few of the more common ways to compute an empirical expected value. Note that the empirical variance is calculated analogously, except, instead of averaging, we take the variance across a certain subset of points. There are myriad other definitions depending on the specific application, some of which are combinations/variations of the ones listed above. For example, for a much coarser average, we could define E[X] to be the average of all values within the image across all channels (a variation of option 1). One could also imagine averaging across all channel positions across all images as an extension of this definition. When choosing what type of normalization to use for a specific problem, it is considered good practice to use whatever method has given good results for a similar problem in the past.
Each expectation calculation scheme can be seen as different ways to span 3 axes: the image positions, the image channels, and the different images within the data set (Figures 9.1-9.4). For example, we can visualize expectation (3), the average of the values at all positions in one channel for one image, as a prism that spans all positions, 1 channel, and 1 image (Figure 9.1). A few other examples are listed in Figures 9.1-9.4.
Some of the more common normalization methods have special names:
• Batch normalization
Batch normalization normalizes the contributions to a layer for every mini-batch.
At training time, the gradients are not calculated for all data at one time; instead, a batch of data is used. A batch normalization layer uses a mini-batch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the mini-batch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features. The batch normalization will become unstable when the batch size is too small.
• Layer normalization
Layer normalization normalizes input across all channels in one image.
• Instance normalization
Instance normalization normalizes across each channel in each training images. The problem instance normalization tries to address is that the network should be agnostic to the contrast of the original image.
• Group normalization
Group Normalization normalizes over a group of channels for each training images.
Group normalization is a medium between Instance normalization and layer normalization. When we put all the channels into a single group, group normalization becomes layer normalization. When we consider each individual channel a single group, it becomes instance normalization.
9.1.4 Weight Standardization
What we talked before is input standardization. Another way to control the movement of gradient descent is weight standardization. Instead of applying the weights directly during the gradient calculation, the weights are normalized beforehand, preventing weights from getting too big.
We’ve established that normalization in the input layer centers the data where the ReLU elbows are most active, but do we need to normalize each layer in the convolutional net (Figure 9.5)? After all, the output of each layer is the input to the next layer, and the repeated applications of the weighted convolutional layers can very quickly lead to an output that is, again, extremely small (leading to a problem known as the vanishing gradient) or very large (the exploding gradient).
The solution is to not just normalize the initial training data, but to also normalize the outputs of interme- diate convolutional layers. Intuitively, normalizing after every layer in the network should solve the problem, which was exactly what early researchers placed did. But since gradients don’t tend to get extremely large or small over the course of the application of at least a few weighted convolutional layers, adding normalization at the very start and after every few layers is sufficient and is what is typically used in modern networks.
Another way to combat the effects of the vanishing/exploding gradients is the introduction of skip connec- tions, bridges that allow the output in one layer to be the input to not just the layer immediately subsequent, but also to layers further down the net. In a network without skip connections, gradient updates must pass through all subsequent layers before reaching the weights of the current layer, which may lead to a gradient update that is minuscule. Skip connections make the weight layers more sensitive to gradient updates as the gradients have to backpropagate through much fewer layers of the network during the gradient calculation and are therefore much less likely to get extremely small from repeated applications of weight layers. 
The modern convolutional neural network era was ushered in by the idea that each successive layer modifies the data in the pipeline, as opposed to completely transforming it.
We can also see how depth affects plain nets versus residual nets in Figure 2. An issue people found (that motivated this new residual architecture) was that in plain nets, introducing new layers created more error instead of creating more expressivity.
With this new residual net architecture (cf. Figure 3), each successive layer no longer completely transforms the data. Instead, through the use of the skip-connection, an identity operation is applied and added to the actual output of the layer. The effect of this is allowing the network to less dramatically modify the data in the pipeline, which in turn allows the depth of deep networks to more consistent learn salient features of the data.
Now, as the number of these layers increases, do we eventually reach convergence? In other words, with enough layers, do things stop changing? We cannot in fact make general statements about the steady-state behavior of such deep networks. This is mainly due to the fact that we also cannot make any such guarantees about the behavior of ODEs of the form shown above. However, we can say if the residual blocks are small and get smaller in successive layers, then we would be able to make some guarantees about convergence.
A new architecture emerged that borrow from the success of transformers (without the actual transformer part). It differs in architecture from the residual net as shown in Figure 4.
It is important to note that in ConvNeXt, instead of using the traditional batch normalization and ReLU activation, layer normalization and Gaussian ReLU (GeLU) are used. Furthermore, the 7 × 7 convolution is done within each channel, as opposed to across the channels (this is signified by the “d” in front of d7 × 7). Furthermore, ResNet has convolution operations taking place in the middle of the pipeline, whereas ConvNeXt has convolution in the beginning. This ultimately allows ConvNeXt to save a factor of 64 in parameters space, while also using and only one activation layer.
In Depthwise Convolution, filters and inputs are broken into different channels, convoluted separately, and then concatenated.
In the ConvNeXt structure, Depthwise Convolution is used instead of typical convolution as it is less computationally demanding. In general nowadays, Depthwise Convolution is more commonly used than typical convolution. The difference between typical convolution and depthwise convolution is visualized in Figure 6.
Since ResNets in 2015, newer models like the 2022 ConvNeXt have improved performance as seen in Figure 7.
These newer models take bits and pieces from the old models. For example residual connections from ResNets are still common. However, there have been changes made as well, with depthwise (grouped) convolution being more common now than typical convolution.
ConvNeXt uses a cosine learning rate schedule, AdamW (Adam with weight decay), label smoothing, a type of dropout called Stochastic Depth Regularization, and aggressive data augmentation, all helping lead to higher accuracy.
Inspiration How can we simulate having an ensemble of diverse neural networks that we output the average of, like in Random Forest, in a method that’s less costly? By utilizing dropout, there is essentially a slightly different neural network at each training step which provides an ”ensemble-like” behavior.
Implementation
• During Training: While training on our minibatch, randomly ”kill” certain units by setting them to zero before training. We will have a hyperparameter that provides the probability of ”nulling” out a unit. When we ”kill” a unit, all the attached weights do not get updated and we can think of the remaining weights ”shaking” a bit to pick up the slack.
• Evaluation: The standard way evaluation is done after dropout is to use expected behavior.
Results of Dropout Dropout promotes diversity and redundancy in networks since at each training step we are essentially training a different neural network. It ensures that it isn’t one unit’s job to learn everything since that one unit can’t always be relied on. It has a regularizing effect and is usually only used for MLPs (multi-layer perceptrons) on 1x1 blocks.
Does Dropout make training slower? No. Learning rate and dropout are trained together, so while dropout does reduce the size of our gradients, the learning rate can compensate for this.
Can Dropout hurt the performance? All regularization techniques have the ability to hurt performance since they shape the inductive bias. Depending on what we are modeling dropout can affect the performance differently. For a brief period of time, people thought that Normalization might negate the need for Dropout, but in practice having both performs better.
Mathematical Reasoning Behind Dropout In another lecture we discussed that we can think of our training algorithm seeing some version of the big singular values that are defined in the inductive bias. Generally for a matrix there are two ways you can have a large singular value. Either you have many different things pointing in the same direction, or you have one particular row or column that has a big value. Dropout drives us towards the direction of having lots of little things pointing in the right direction.
3.2 Stochastic Depth Regularization
Implementation During training randomly drop entire residual blocks. So during training some will be active and being trained while others are ”gone”. Very similar to Dropout but on a different scale.
3.3 Other Methods Similar to Dropout that Have Been Explored
• Drop Connect: Instead of ”killing” certain units why not try ”killing” weights? People have found that in practice this does not work as well.
• Another method that also adds the regularizing effect we see in Dropout is to multiply by random noise (in between 0 and 1) instead of multiplying by 0. Dropout tends to perform better in practice.
• What if we have logic behind what we turn off instead of randomly choosing? People have explored ”killing” units more or less often based on what the size of their activation’s tend to be as well as other similar ideas, but these have not been proven to be useful in practice so far.
4 Label Smoothing
4.1 Before Label Smoothing: One Hot Encoding
For Classic Cross-Entropy/Log-Loss Classification when we have a label ”class 1”, we represent this using one hot encoding. Since one hot encoding has an array of 0s with a 1 for the correct label as a goal instead of probabilities, it forces the model to try and be ”super” confident. This means that the model will constantly be trying to get closer and closer and since with softmax we cannot actually reach a probability of 1 unless the input is infinity, the model will never be to reach the goal classification.
Reaching this goal y array is possible and in practice label smoothing does improve performance. We can think of the
behavior as now being more similar to squared-error since they can both be satisfied.
Concern: Originally, people were concerned about this idea since in the real world some items are more similar than others. For example, a dog should be more similar to a cat than to the ocean. With that in mind, we can see that label smoothing tries to say that something is a dog but has an equal small probability of being either a cat or ocean when logically we would expect the probability for the cat to be higher.
Images have a structure with patterns that we want to learn and we need to create inductive biases to help us learn the structure. The key ideas including:
1. Convolutions with weight-sharing AND having an “image” at each layer: build the local convolutions and then use hierarchical depth to see the entire image
2. Residual Connections to fight dying gradients: every layer has an effect when parameters change on what happens at the end
3. Normalization to “adaptive speed bump” exploding gradients: brings down growing activation values
We might have different numbers of neighbors to “me”. In contrast, one pixel of an image usually has eight neighbor pixels in CNNs.
• We don’t have separate names for neighbors of “me”. This means that the neighbors do not have a particular order in GNNs. In contrast, the eight neighbor pixels of one pixel have their particular position in CNNs, like the pixel “b” is on the top of “me” and the pixel “f” is on the right.
The most important idea behind CNN is the convolution with weight-sharing. What kind of function can we have in place of convolution, respecting the properties of a graph?
1. First, this function should have some learnable parameters associated with it (like w1 in Eqn 11.1a).
2. Second, it should take two arguments, the node itself and its neighbor nodes, because these are all the
information we know related to this node.
3. Then, a method is needed to combine the information of these neighbor nodes regardless of the ordering or cardinality of neighbors. This method is not learnable and we need to choose from possible choices, including Sum (used in Eqn 11.1a), Maximum, Minimum, Product, Softmax, Variance, and so on.
4. Finally, neighbor nodes also have their learnable function, gw2 (neighbor node).
We can further factorize gw2 (me, them) into sw2 (me, them) and gw3 (them) and get Equation 11.1c, where gw3 (them) is regarded as the learnable function of “them” and sw2 (me, them) is regarded as the connection (or similarity) of “me” and “them”. In Transformer Architecture, sw2 (me, them) is also known as “attention” because it is a learned amount of how much we pay attention to this neighbor.
Food for thought: If we view the GNN in an adjacency matrix formulation, is there a way to use our usual CNN weight operations more directly?
Answer: There is a way to leverage adjacency matrix formulation with use-cases in graph signal processing for signal reflection.
Researchers have generalized the idea of signal processing from one- or two-dimensional functions to general graph relationships. A convolution can be seen as an operation that respects the natural shift invariance on the infinite topology. The infinite topology means that the signal can keep going to the right or left. For a graph, there is no information about what the natural shift would be but there is indeed something we can do with the adjacency matrix. We can take a walk on the graph, or we can take products of the adjacency edge and itself. We can think of shifts on a graph as a kind of repeated product on the unit line. As an example, we can generalize the infinite line to the finite line by shifting on the circle because we can rotate on the circle.
There is also a beautiful connection between convolutions and the invocation of a different domain called frequency domain convolutions. In signal processing, convolution in the original domain corresponds to the multiplication in the frequency domain. But it turns out that the frequency domain is just the eigenbasis corresponding to a particular matrix associated with the structure because convolutions commute with each other and can share the eigenbasis. This allows people to consider what actions can commute with such a matrix formulation of a graph network, with one such formulation being the adjacency matrix. So you could imagine what operations (as matrices) can commute with the adjacency matrix. Those will be the counterpart of convolutions and will respect the entire graph in this abstract way. Graph signal processing considers those objects relevant in graph neural nets. This entire approach is sometimes encapsulated by the spectral methods.
The number of nodes in a graph can sometimes be on the order of millions, and the number of edges per node can be highly variable. Often, this leads to very sparse adjacency matrices, which are space-inefficient. Another problem is that many adjacency matrices can encode the same connectivity, and there is no guarantee that these different matrices would produce the same result in a deep CNN (that is to say, they are not permutation invariant).
Pooling (downsampling) groups similar nodes and coarsens the image in CNNs. Here, the similarity usually means how close the pixels are. In GNN, we just need a similar clustering method to “coarsen” the graph. Under the assumption that the graph topology is fixed, we can pre-compute a specific clustering of this graph based on its topological attribute. In this way, the four nodes in Figure 11.1 are shrunk to two nodes in Figure 11.2.
The clustering can also be learnable. We can use some similarity measures to cluster, like similar neighbor- hoods or similar values inside them. We can even use the learned similarity to simulate the effect of the clustering without doing a full clustering.
In fact, almost everything else remains the same. For example, the residual connection only requires the structure of the object to be the same across layers, which GNN satisfies as well. GNN can also employ all normalization used in CNN, like layer norm, batch norm, instance norm, group norm, and so on. All the normalization needs are that the outputs have some sense of “likeness” to average over.
Let’s generalize our assumption: the graphs can still give a single output but no longer have the same topology. What’s the most naive thing we could do?
We could try to force the graphs into a similar shape. One way of doing so is to get rid of all topology, make all the nodes fully connected, and use edge labels to tell whether they were connected in the original or not. However, this does not work so well, especially from a computational point of view, because lots of graphs of interest are sparsely connected. Making the graph fully connected will result in a lot of unnecessary computation.
So are there any other solutions? In fact, we could ignore the topology mismatch and see whether it still works. To see whether something still works, two different parts need to be checked, whether it can still run and whether it can give good answers if it can still run.
Will the network still run if we have lots of different graphs as our data during training?
This question can also be broken down into two questions: does it still run as pseudo-code, and does it still run as code?
In terms of running pseudo-code, the answer is yes. Because everything is local, we are just iterating over local neighborhoods. We just have fewer local neighborhoods or more local neighborhoods for different graphs. The residual connections would definitely still work as long as the topology of each layer stays the same. The normalization will still be meaningful, for example, if we just divide the nodes by the different numbers for different sizes of layers in the layer norm. It’s only when the nodes are clustered ahead of time that the network may not be able to run. However, we seldom employ clustering because it does not make much of a difference (as mentioned before). Therefore, we can still run it as the pseudo-code.
In terms of running the actual code, we might have to worry about the size of the arrays allocated to make sure things fit. But the important thing is that the weights are not changed. The number of weights we have to learn does not change even if we have more different graphs.
Food for thought: Do we need to pad the graph with null nodes?
In CNN, we need to care about the pixels on the boundaries of images because they have less neighbor pixels. Usually we have zero padding for them or we just ignore these pixels. However, the nodes do not have the same number of neighbors generically. We don’t have to pad the graph with null nodes because we never have to compute anything for the null nodes.
After learning GNN, one question that pops up is whether we could have more weight sharing. In a ConvNet traditionally, the weights are shared within a layer but not across layers. We talked about the idea of the neural ODE, which was the perspective of a ConvNet as a ResNet solved as a differential equation. If the convergence of the neural ODE is wanted, we had to invoke some kind of weight sharing, at least hard or soft weight sharing across layers to ensure the limit existence since there were similar behaviors with respect to time. This requirement raises the question: should we share weights across layers?
We should do weight-sharing if the hierarchical structure has a self-similarity at different scales. This is one of the key design choices in the RNN family.
• Note that the word “layer” can be used in the same way we thought about the layer from a convolution point of view, which is what we backprop through. Once we implement weight sharing across layers, we can still backprop through it.
Let’s give an example of an RNN now. Consider a task where you are required to identify a person’s attributes based on their name. First, you represent the name by a sequence of characters, then represent these characters as a graph with internal labels. As such, we associate different names with different sizes of graphs, as shown in Figure 11.3.
One possible solution is to just use a GNN. However, when people started working on these problems, theyFood for thought: Do we need to pad the graph with null nodes?
In CNN, we need to care about the pixels on the boundaries of images because they have less neighbor pixels. Usually we have zero padding for them or we just ignore these pixels. However, the nodes do not have the same number of neighbors generically. We don’t have to pad the graph with null nodes because we never have to compute anything for the null nodes.
were actually motivated by the connection with signal processing. In signal processing, you might have seen finite impulse response (FIR) filters and infinite impulse response (IIR) filters. The difference between them is that IIR filters have internal states, like momentum form in the momentum acceleration method. So for the things that are sequential in nature, we can employ the analogy of an IIR filter and think of a network that has internal states. We can treat these self-connections as a kind of internal state.
Another thing worth mentioning is that FIR filters act on the input but the IIR filter consumes the input, one at a time, like the momentum consuming the gradient in each cycle. The whole point of an RNN is to have this eating input behavior used in the network.