Gradient Descent: SGD and Learning Rate
Optimizers are used to solve for models’ parameters. The optimizers themselves usually have their own hyperparameters (learning rate, choice of the optimizer, etc.). The most basic optimizer is gradient descent, an iterative local optimizer that we have touched on briefly in Note 1. Just to recap a bit, the idea is to change parameters a little bit each time and see how that changes the model’s performance. If we have some parameter vector θ ∈ Rn, we update it according to:
where θt denotes the parameter at time step t, η is the learning rate hyperparameter, and Ltrain,θ is the training loss function. At each iteration, we take a small step in the direction opposite the steepest ascent of the training loss function. If we look at a first-order Taylor series approximation of the training loss function, it is clear that we are looking at the training loss function locally, and asking the question, “where will we be if we take a small step in this direction?" This lets us gain an understanding of how the loss landscape depends on our parameters.
If we look at our training loss function more closely, we notice that it includes a huge summation over the entire dataset. Remember here that ltrain(yi,fθ(xi)) is a surrogate loss function that is compatible with our choice of optimizer, and R(·) is a regularization term that ensures our parameters θ satisfy some predetermined constraints (like sparsity). The size of the dataset n is typically huge, so evaluating Ltrain,θ can be extremely wall clock time consuming. To address this, stochastic gradient descent (SGD) is typically used.
In SGD, instead of all n training points, nbatch << n random samples are used. Our loss function now becomes.
If we take a random draw of a subset of points, we get an estimate of the average. We can assume that this gives
us an unbiased estimate of the actual loss function across the entire dataset, albeit one with some amount of variability (because nbatch << n). In a deep learning context, the batch size nbatch is the largest that your
system can sustain, and ensure adequate wall clock time.
Figure 1: Contributions from each data point in the dataset balance, so the gradient is zero.
Why do we use SGD? The standard Deep Learning answer is that we can’t afford to look at all the data points, but we can afford to look at a few of them. Beyond that, we can expect that the loss surface has different regions of varying “quality" (flat/non-flat, near global optima, etc.). We have no expectation that we’ll begin training in a “good" region, so going in the right general (but not exact) direction might help in this scenario. It’s also very probable that our estimate based off a few points very closely aligns with the direction of the gradient across all data points. We’re only taking a small η-sized step in the direction opposite the gradient, so we only need a level of precision that ensures we take our step appropriately.
1.2 Learning Rate
The updated parameters θt+1 are a discrete time dynamic system, and the learning rate η controls the system stability. If η is too large, we have instability and our model’s loss will diverge. If η is too small, we make (almost) no progress towards getting a working model. The badness of choosing a large learning rate η is very clear, if our loss function diverges, our model doesn’t learn anything. Although choosing a small learning rate does not cause divergence, it is an equally bad scenario as it corresponds to more real world wall clock time, energy usage, and cost.
Changing the step size η might help us (or not) depending on the loss surface. We can use a learning rate schedule to enforce some step-size-changing policy. Doing this is especially useful in some non-vanilla niches of machine learning, such as online learning (where data is only made available to you in a sequential order).
A step size selection that causes convergence for regular Gradient Descent might not guarantee convergence for SGD. If we pick η so that the gradient doesn’t diverge when taken across all points, we can generally expect that it won’t cause divergence when evaluated across some points. What actually needs to happen, is that the model needs to converge to some setting that works well. Convergence happens when the parameters stop changing in our gradient descent iterations. Once we settle so that θt+1 ≈ θt, we converge unless the gradient term −η∇θLtrain,θ suddenly spikes. At optima, derivatives/gradients should all be zero, meaning we converge. The derivative is zero because the sum of contributions to the gradient at all the different points cancels out to zero (see Figure 1). In SGD, we’re not using all the data points, so we might be at some optimum, but our small-batch estimate of the gradient will likely be unbalanced (see Figure 2). So we might step away from an optimum and “only sort of converge." This wiggling behavior is illustrated in Figure 3.
So to actually converge, we may need to make sure that η is decaying.
There are other explanations for why learning rate ought to decrease over time. Picking a large step size at the beginning may help explore the space in a “path less traveled” sort of way, while a small step size at the end helps refine our evaluation.
It’s also worth noting that there’s technically a difference between oscillatory and random walk type behav- ior. A non-decreasing step size in gradient descent might cause oscillatory behavior, but the stochasticity in SGD will cause random walk behavior.
Nonetheless, in deep learning, even a constant step size may lead to convergence.
The time it takes to go through an entire shuffled instance of all the training data set is called an epoch. Shuffling (and reshuffling) data between epochs can help avoid us learning subtle patterns that arise because of the ordering of our dataset. Sometimes, this sort of structured approach and commitment to see all data points is not desired. Because datasets can be so large, if we just randomly sample from the training dataset we are likely to get similar results. In these cases, sampling with and without replacement are equally performant.
Generally, getting an optimizer to work well involves lots of consideration to be taken as to the application and the underlying hardware that the model runs on.
2 Intro. to Neural Nets via ReLU (Rectified Linear Unit) Nets
2.1 What is a Neural Net (Differentiable Programming)?
A neural net is an object that is easy to take derivatives (e.g. Analog circuits realized as computation graphs with (mostly) differentiable operations compatible with nice vectorization). Neural nets are prevalent today because they’re able to exploit gradient descent/SGD to do learning/computation.
2.2 Two goals of the analog circuits
(a) Expressivity: the ability to represent a wide variety of functions.
(b) Learnability: the ability to learn functions without too much trouble.
2.3
Expressivity is important because we need to be able to express patterns we’re interested in, which we don’t know and need to be learned. We can’t guarantee that some model is expressive enough to capture these patterns. Traditionally, the solution to this is to be expressive enough to capture any pattern. If a model can learn any function, to within some amount of precision, we call it a universal function approximator.
Example of Neural Networks
The figure below shows a 1-D nonlinear (Blue) function, piecewise linear (Orange) functions, and data points (Green). As shown in the figure, the piecewise functions describe the nonlinear function pretty well. Our goal is to find a set of piecewise linear functions (Orange) that best match the nonlinear function (Blue) based on the data points using Neural Nets.
Then, how do we create the piecewise linear functions? We do it by forming a linear combination of elbows (Rectified Linear Units) that looks like:
 The standard ReLU function is shown below.
Also, the standard ReLU function can be modified if needed. For example, x can be replaced with wx + b, so that the modified ReLU function becomes f(x) = max(0,wx + b), which can be represented by a computational graph like this:
We can manipulate an elbow to fit our line by changing w (weight) and b (bias). More specifically, Elbow is located at −wb and slope = w. Here, w and b are the parameters(θ) we want to minimize using a loss function.
To provide the circuit interpretation of ReLU, we are going to look at an example of a rectifier circuit, which is shown in the figure below the computational graph. It is composed of a diode and a resistor. The diode prevents the current from flowing in the opposite (or negative) direction. Setting the positive direction of the current to be from left to right, it means that the current can never flow in the negative direction (from right to left). All negative currents will be set to zero resulting in Vout readings being zero. On the other hand, positive currents (from left to right) will go through the diode resulting in Vout readings on the other side of the diode. In this example, Vin is x and Vout is f (x).
 The above image illustrates the big picture of what our simple ReLU Neural Net looks like. We tune the elbows to fit our training points, the optimizer is trying to make the final real number (loss) as small as possible across all the training points. To push the loss down, how much do I have to move y, b’s, and w’s? This process goes backward and the elbow changes.
With one layer and an arbitrarily large layer width (number of ReLUs), we can approximate any function. Historically, people have just stuck with using single-layer universal approximators. So why move into the domain of deep networks? We use deep neural networks because they tend to exhibit learnability, meaning they can actually be trained to learn patterns of interest. When we make this shift to deep networks, some of our parameters become redundant, so the number of parameters is not directly (though highly correlated with) the degrees of freedom our network has.
2.4
Asides for ReLUs
Non-saturating
They work well with gradient based methods because they are non-saturating (unlike sigmoid, for example). For ReLUs, the gradient doesn’t depend on the value of x other than its value relative to the threshold. For a saturating non-linearity, the magnitude of the gradient approaches zero as the magnitude of x itself increases to infinity. Using a non-saturating non-linearity ensures that our gradients don’t diminish in deep networks with long partial derivative chains.
Dead ReLUs
If both weights and biases are distributed with normal distributions, the ratio wb will be a Cauchy distribution. This causes some ReLUs to be located far apart from the others. These ReLUs are considered "dead" since they output 0, and changing the weights or biases slightly doesn’t affect their output.
1 Pragmatic view of regularization
• By adding the penalty term, we hope to shape the optimization to favor something that we like.
• Note that (explicit) regularization does not change the expressive power of the network. Therefore, you only change what the optimization is favoring.
2 Regularization in Least Squares
For a more intuitive understanding of regularization, we can look at least squares. Let’s start by reviewing the problem: we have a matrix X and vector y, and we want to calculate the weights w that best approximates Xw = y.
We can add regularization to OLS. One version of regularized OLS is called ridge regression:
λ is the regularization parameter. It penalizes high-magnitude weight vectors (can you see why?). We can select different λ values to control the behavior of Ridge regression. When λ is higher, the weight magnitude penalty is more severe; when it’s lower, the penalty is lighter, which allows Ridge regression to settle on weight vectors with higher magnitudes.
Note that when λ = 0, ridge regression becomes unregularized. This unregularized formulation simplifies to the OLS problem and solution. Intuitively, setting λ = 0 tells ridge regression to disregard the weight vector’s magnitude altogether, allowing it to solve the problem as though it were simply OLS.
We can also solve least squares with gradient descent. Gradient descent is a process where we aim to improve our model by repeatedly moving its parameters in the direction that minimizes the cost function.
This gives the interpretation that for large singular values, gradient descent moves much more and faster than smaller singular values, which can take longer to converge. Thus, we first fit in the largest σi direction, then the next and so on. When working with very small singular values that have the propensity to give large solutions, we may converge to something very large, but slowly, especially without regularization.
Solution: Use gradient descent, but limit the number of steps to a finite number to avoid overtraining. This is sometimes called early stopping.
There are a few different tricks that can be used to avoid explictly including Ridge regularization while still accomplishing the same result, either by adding extra entries or extra features to the data matrix.
Implicit regularization is the regularization that occurs when we aren’t consciously doing any regularization. When performing explicit regularization as above, we must specify a specific regularization hyperparameter and modify the training data, model architecture, or loss function. In contrast, implicit regularization is an unexpected benefit stemming from our choice of optimizer. We will see that choosing gradient descent as our optimization algorithm, combined with the large size of DNNs, provides enough regularization for DNNs to generalize well without us intentionally restricting the parameters.
Observe that this is roughly a linear function with an extremely small slope if σi is tiny. In this situation, GD barely moves in the early stages even though it will eventually converge to a very large value, as pre- viously discussed. Together with early stopping, this means that GD is trying to do something like ridge regularization for us because it will resist enlarging the directions corresponding to small singular values. Early stopping is when we stop the training process because validation performance has gotten worse or has not improved for a long time. It is important to note that GD, when initialized at zero, will converge to the minimum-norm solution. This is a good exercise for the reader to verify.
To summarize, there are three kinds of regularization: explicit regularization, data augmentation (adding fake observations or features), and implicit regularization (optimizer has an implicit regularizing effect). Regarding DNNs, the combination of the min-norm seeking behavior of gradient descent and the feature augmentation that is implicit when using large networks gives a lot of regularization even if we weren’t thinking about it.
Gradient Descent is the tool that we understand the most and is often our first choice.
• The loss function will typically contain training data with labels that we would like to predict with good accuracy.
• The singular values of the data matrices involved greatly matter and can lead to overfitting, slow convergence, and unreasonably large parameters.
• In solving this problem, we usually add regularization, which we can understand through SVD as bounding the solution to the smaller singular values while leaving the solution of the larger singular values unchanged.
• It’s desirable for the input to our ReLU layers to be centered near 0 and to be close to where the “action" is, otherwise poor initialization can lead to dead ReLU nodes from the beginning. Dead ReLUs are problematic because they make all the computations with the weights and biases, but are always less than 0, leading to wasted computations.
• Additionally, dead ReLUs are usually unable to recover, leading to a portion of our network being unable to learn further.
We want to achieve a distribution of N (0, 1) at initialization, the outputs of the layer will be standardized. So the main idea is that we want our inputs to have a distribution of N (0, 1) at initialization.
Current Basic Folk Wisdom:
• Use whatever worked on a related problem. (e.g., pre-trained network, literature review, etc)
Xavier Initialization aims to initialize weights such that the variance across every layer is the same. Let d be the fan-in (number of inputs of the layer) of the target layer.
Problem: This doesn’t work well for ReLU!
Why? If the previous layer was ReLU, we expect half of the outputs to be 0 on average.
There are typically four approaches that are used to initialize biases:
(a) Treat fan-in as d + 1 and use Xavier Initialization
This treats the bias as a weight in order to use a weight initialization technique, which is why we use
d + 1 instead of d for the fan-in. (b) Just use bias b = 0
This is essentially letting our optimizer initialize the bias in the first few steps. (c) Just use b = 0.01
Empirically, people have seen that this sometimes performs better than b = 0. (d) Just use any small, random number
This claims that it doesn’t matter what small, random number we select, since our optimizer will quickly change it anyways.
Vanilla gradient descent has many benefits, but speed is not one of them. The reason behind that is the constraint on the learning rate.
The idea of momentum is finding a safe way to make the η bigger. We know that the dimensions with larger singular values start to oscillate quickly. So the thought is to somehow low pass filter (LPF) those directions that would otherwise oscillate if we take smaller steps. By doing that, instead of moving in the direction of the gradient, the update is moved towards the direction of the average gradient. The functioning of the
simplest LPF is where this idea of averaging originally came from.
This low-pass filter (LPF) will regulate voltage.
This average is beneficial, as it only requires one internal state to keep to track of, and β will determine how long averaging will go for. This technique is commonly referred to as exponential smoothing. Now we want to apply this technique to create a more stable gradient descent.
In order to achieve gradient descent with momentum, we want to replace our gradient term in regular gradi- ent descent with our low-pass filter equation applied to our gradient. Gradient descent with momentum can be considered as a “smooth gradient".
The main difference with momentum is that gradients at previous time steps have an impact on the current gradient’s value.
The vanilla momentum uses the gradient where the current weights are. In the Nesterov momentum, we take advantage of the fact where we are going by peeking into the future, as given from the first part of the momentum term (9). This is because we already know where we are going to end up, and by taking that new information to the calculation of the gradient at this step, we can take a bit of an advantage on learning.
The simple perspective of the origin of the adaptive methods comes from the fact that even with momentum, the largest singular value and the smallest singular value are the ones that govern the process. Values in between are not relevant for setting up the parameters. So the simple idea is that instead of having a single step size, use different step sizes (ηi) along different dimensions of the parameter vector. In vanilla gradient descent, we go down in the steepest gradient direction. But in the adaptive methods, we no longer go in the steepest direction but still in a downward direction.
The formulation of Adam’s method where different step sizes in a different direction are shown in equation 12. In the large gradient directions, it takes smaller steps and vice versa. That way, it takes evenly sized steps along different directions. To track the size of the gradient, a vector V is introduced, which depends on the element-wise product of the gradient vector. Then the gradient update for the ith parameter is computed with the square root of elements of V to make the units right. A constant (ε) is added to the denominator to make sure it is stable.