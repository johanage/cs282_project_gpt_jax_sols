{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1> Introduction to minGPT - A PyTorch re-implementation of GPT </h1>\n",
    "\n",
    "minGPT is originally a PyTorch re-implementation of GPT, a highly successful language modeling framework developed by OpenAI. Created by Andrej Karpathy, minGPT is a lightweight and efficient implementation of GPT, designed to be easy to use and highly customizable. With its modular architecture and flexible design, minGPT is a powerful tool for researchers and practitioners working in natural language processing and related fields. And don't worry, despite its name, minGPT is not small-minded! ;) \n",
    "<br/> This assignment will introduce you to the basics of minGPT and how to use it for language modeling tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "OpenAI’s generative pre-trained transformer (GPT) was first introduced in ”Improving Language\n",
    "Understanding by Generative Pre-Training” [1], and has since then developed into being one of the\n",
    "most topical models within the field of ML, and a common conversational topic on a global scale.\n",
    "Due to the significant societal impact that this model already has a lot of questions have been\n",
    "raised in the aftermath of its public release. This projects aim to make a homework assignment on\n",
    "the implementation of the model architecture and word embedding with an additional twist where\n",
    "we want to provoke students to make their own reflections on the societal impact of the model.\n",
    "\n",
    "<i> [1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\n",
    "understanding by generative pre-training. 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> GPT (current) Gold Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we dive into minGPT, let's look at the current gold-standard of GPT models, GPT-3.5. While there are several use cases of GPT, let's look at one of the most commonly used one, ChatGPT! ChatGPT is a large language model based on the GPT-3.5 architecture, trained by OpenAI to generate human-like text. It is designed to respond to human prompts with natural language responses, making it useful for a variety of applications such as chatbots, automated content generation, and more. Under the hood, ChatGPT uses a deep neural network to learn from massive amounts of data, allowing it to generate text that is coherent and contextually relevant\n",
    "<br/> You can interact with ChatGPT here: http://chat.openai.com (sign up using Google and your Berkeley account).\n",
    "<br/>\n",
    "Have a short conversation with ChatGPT about Transformers, GPT models, and the basic workings of ChatGPT. Limit your conversation to three questions. \n",
    "<br/><br/><b> Answer the following questions: </b>\n",
    "1. What questions did you ask ChatGPT?\n",
    "2. In 2-3 sentences, what did you learn from the conversation?\n",
    "3. Were you satisfied with the responses? On a scale of 1-5, rate the conversation you had with ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that you have interacted with ChatGPT, let's work on a much more simple and scaled-down version, minGPT! In this assignment, we will train minGPT to be a character-level language model on some arbitrary text input. \n",
    "<br/> <h5> Flax and Jax </h4>\n",
    "In this homework, we will be exploring the world of [Flax](http://flax.readthedocs.io/) and [Jax](https://github.com/google/jax), two powerful deep learning libraries made by Google that can be used to reimplement the PyTorch version of minGPT. Jax and Flax are both built on the concept of functional programming and provide a high-level interface for building and training neural networks, which can simplify the development process and reduce the amount of boilerplate code that needs to be written. Flax and Jax are designed to take advantage of modern hardware architectures such as GPUs and TPUs, which can significantly speed up the training process and reduce the time-to-deployment for new models.\n",
    "<br/> By using these libraries, you will gain a deeper understanding of the underlying principles of neural networks and develop your skills in functional programming. You will also have the opportunity to compare and contrast the PyTorch and Flax/Jax versions of minGPT, gaining valuable insights into the similarities and differences between these powerful tools. So let's roll up our sleeves and dive into the world of Flax and Jax!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h4> Train a character-level GPT on some text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First, some imports!\n",
    "!pip install flax\n",
    "!pip install optax\n",
    "!pip install jax\n",
    "!pip install transformers\n",
    "!pip install git+https://github.com/deepmind/dm-haiku\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "np.random.seed(182)\n",
    "\n",
    "from train import trainer, train_config\n",
    "\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 1. Attention is all we need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, we are going to be focusing specifically on the attention mechanism that is at the heart of its architecture. Attention is a critical component of many modern neural networks, and it plays a particularly important role in natural language processing tasks such as language modeling and machine translation. By completing this section of the homework, you will gain a deeper understanding of how attention works and how it can be used to improve the performance of language models. \n",
    "<br/>You will now implement the causal self attention for (min) GPT! You will implmement the code in the `model/model.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some tests here\n",
    "from tests.test_attention import TestAttention\n",
    "TestAttention().autograde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 2. Unpacking the MLP: Layer by Layer for Better Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The multi-layer perceptron (MLP) in GPT, also known as the feedforward network, is an essential component that helps to improve the model's ability to learn from sequential data. While the self-attention mechanism in GPT allows the model to attend to different parts of the input sequence, the MLP is responsible for processing and transforming the attended features before they are fed into the next layer. This additional non-linearity helps to capture more complex patterns and dependencies between the input tokens, leading to better performance on a wide range of language modeling tasks.\n",
    "<br/> In this section, you are going to implement the MLP for (min) GPT! You will implmement the code in the `model/model.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`.\n",
    "<br/> HINT: Read the documentation [here](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dense.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some tests here\n",
    "from tests.test_mlp import TestMLP\n",
    "TestMLP().autograde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 3. From Text to Numbers: Understanding Encoding in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The encoding section focuses on one of the most important aspects of natural language processing: converting textual data into numerical representations that can be understood and processed by machine learning models. In this section, we will explore the different types of encoding methods commonly used in NLP, including one-hot encoding, word embeddings, and more. By the end of this section, you should have a better understanding of how encoding works and why it is crucial for many language-based applications. You will implmement the code in the `encoding/bpe.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some tests here\n",
    "from tests.test_encoding import TestEncoder\n",
    "TestEncoder().autograde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 4. Crossing the Finish Line: Completing the MinGPT model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We're almost there! In this section, you will complete the final parts of the minGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tests import test_set_params\n",
    "# run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's run some unittests to see if your implementation is correct!\n",
    "from tests import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 5. Unleashing the power of minGPT: Training (and testing) the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will now train minGPT to be a character-level language model on some input text file. The input text file is an extract of Shakespearean text of about 1.1 MB. <br/> To use minGPT as a character level language model, we first preprocess the input text by converting it into a sequence of characters. We then train the model to predict the next character in the sequence given the preceding characters as input. Once the model is trained, we can use it to generate new text by feeding it a starting sequence (context) and iteratively sampling new characters from the model's predicted distribution until the desired length of text is generated. By using minGPT as a character level language model, we can generate new text that is similar in style and content to the input data, making it a useful tool for tasks such as text generation, language modeling, and autocompletion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('The input data has %d characters. %d of these characters are unique. These characters include uppercase and lower case letters, as well as punctuations.'\n",
    "        % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:ch for i,ch in enumerate(chars)} # will be used for prediction/text generation task\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_block = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        encoded_txt = [self.stoi[char] for char in text_block]\n",
    "        x = torch.tensor(encoded_txt[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(encoded_txt[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now load the input Shakespearean text file and look at the composition of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data has 1115393 characters. 65 of these characters are unique. These characters include uppercase and lower case letters, as well as punctuations.\n"
     ]
    }
   ],
   "source": [
    "# Let's load in the input data of Shakespearean text\n",
    "shakespeare_txt = open('./gpt_text_input/shakespeare.txt', 'r').read() \n",
    "train_dataset = TextDataset(shakespeare_txt, block_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Go through the `train_config.py` file in the `train` directory. It contains the parameters we will use to train the model. You can play aroud with these parameters after you have trained the model for the first time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0b5e19b0de7feeb6e6bb5f9738d975aa3f5dabb2cb545fec106b49f43b6978a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}