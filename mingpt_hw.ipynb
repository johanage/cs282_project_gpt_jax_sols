{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to Coding in jax and flax\n",
    "In this homework, we will be exploring the world of [Flax](http://flax.readthedocs.io/) and [Jax](https://github.com/google/jax), two powerful deep learning libraries made by Google that can be used to reimplement the PyTorch version of minGPT. Jax and Flax are both built on the concept of functional programming and provide a high-level interface for building and training neural networks, which can simplify the development process and reduce the amount of boilerplate code that needs to be written. Flax and Jax are designed to take advantage of modern hardware architectures such as GPUs and TPUs, which can significantly speed up the training process and reduce the time-to-deployment for new models.\n",
    "<br/> By using these libraries, you will gain a deeper understanding of the underlying principles of neural networks and develop your skills in functional programming. You will also have the opportunity to compare and contrast the PyTorch and Flax/Jax versions of minGPT, gaining valuable insights into the similarities and differences between these powerful tools. So let's roll up our sleeves and dive into the world of Flax and Jax!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Before we start implementing the GPT architecture the libraries `jax` [jax link](https://jax.readthedocs.io/en/latest/index.html) and `flax` [flax link](https://flax.readthedocs.io/en/latest/) should be introduced as they have some key differences from pytorch functionalities. These differences are explained in detaile [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers). \n",
    "\n",
    "First let us start with `jax`. In practice `jax` works almost exactly as `numpy` but use own datatypes which is more convenient in the context of high-performance numerical computing (HPC). Experience with such libraries is crucial as modern ML applications require resources that is beoynd the capabilities of a personal computer. Furthermore, `jax` supports automatic differentiation in both forward- and reverse-mode as well as accelerated linear algebra (XLA - domain-specific compiler [XLA link](https://www.tensorflow.org/xla)) .\n",
    "\n",
    "## Random Number Generation\n",
    "\n",
    "The first difference between `jax` and `numpy` we are going to highlight is ***how you generate random numbers***. In the code block below we demonstrate how you generate a random vector $x, x\\in\\mathbb{R}^{10}, x_i \\sim \\mathcal{N}(0,1)$. You can read more about the PRNG design [here](https://github.com/google/jax/blob/main/docs/jep/263-prng.md). A major advantage of this design is **reproducible program execution in a backend-indpendent way**. This is of major imortance for validating new discoveries within ML research as a many components (weight and bias initialization, dropout, masking, etc.) depend on PRNGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([ 0.690805  , -0.48744103, -1.155789  ,  0.12108463, -0.19598432,\n",
       "       -0.5078766 ,  0.91568655,  1.70968   , -0.36749417,  0.14315689],      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import random\n",
    "size = (10,)\n",
    "key = random.PRNGKey(1)\n",
    "# generate random vector with std normal distributed elements\n",
    "x = random.normal(key, size)\n",
    "x\n",
    "# alternatively use a mu sigma^2 normal distributed vector\n",
    "# x = mu + sigma2*random.normal(key,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To prevent generating random numbers from the same state represented by the variable `key`, the PRNG is split to get usable subkeys every time a new pseudorandom number is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old key [0 1]\n",
      "    \\---SPLIT --> new key    [2441914641 1384938218]\n",
      "             \\--> new subkey [3819641963 2025898573] --> normal [-1.1470195]\n"
     ]
    }
   ],
   "source": [
    "# copied from hax common gotchas\n",
    "print(\"old key\", key)\n",
    "key, subkey = random.split(key)\n",
    "normal_pseudorandom = random.normal(subkey, shape=(1,))\n",
    "print(\"    \\---SPLIT --> new key   \", key)\n",
    "print(\"             \\--> new subkey\", subkey, \"--> normal\", normal_pseudorandom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pure functions\n",
    "\n",
    "`jax` is a language of expressing and composing transformations of numerical programs and compile these programs for both CPU and accelerators (GPU/TPU). Compiling across devices requires that the code is written with certain constraints. One of the constraings is that the transformations and compilations are designed to only work on ***pure functions*** [pure functions link](https://en.wikipedia.org/wiki/Pure_function). This means that all the input data is passed through the function parameters, all the results are output throught hte functions results. Consequently, a pure function will always give the same output for the same input. In the below code block we have provided some examples of impure functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "[11.]\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "# using global\n",
    "global_var = 1\n",
    "def impure_input_is_output(x):\n",
    "    return x + global_var\n",
    "# uses initial assignment of the global variable\n",
    "print(jit(impure_input_is_output)(1))\n",
    "global_var = 10.\n",
    "# subsequent runs may cached value of the previous print\n",
    "print(jit(impure_input_is_output)(2))\n",
    "# when the type is changed the las update of the global variable is used\n",
    "print(jit(impure_input_is_output)(jnp.array([1]) ) )\n",
    "\n",
    "############\n",
    "# Add more #\n",
    "# examples #\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## In-place updates\n",
    "\n",
    "Another important difference to `numpy` is how in-place update are done. `jax` offers a functional array update using the `.at` property. This difference is examplified in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#### Jax #####\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# not that float 64 is not available, try by running the\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# commented-out line below\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#jnp_arr = jnp.zeros((3,3), dtype = jnp.float64)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m jnp_arr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), dtype \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mjnp_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/cs282_project_gpt_jax/venv/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py:263\u001b[0m, in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unimplemented_setitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, x):\n\u001b[1;32m    259\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object does not support item assignment. JAX arrays are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimmutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor another .at[] method: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 263\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "#### Numpy #####\n",
    "np_arr = np.zeros((3,3), dtype=np.float64)\n",
    "print(np_arr)\n",
    "# in place, mutating update\n",
    "np_arr[1,:] = 1\n",
    "print(np_arr)\n",
    "\n",
    "#### Jax #####\n",
    "# not that float 64 is not available, try by running the\n",
    "# commented-out line below\n",
    "#jnp_arr = jnp.zeros((3,3), dtype = jnp.float64)\n",
    "jnp_arr = jnp.zeros((3,3), dtype = jnp.float32)\n",
    "jnp_arr[1,:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### correct jax in-place update ####\n",
    "jnp_arr.at[1,:].add(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Permissive inputs\n",
    "\n",
    "`jax` requires premissive inputs. While `numpy` accepts lists or tuples as inputs, `jax` do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sum([1,1,1]))\n",
    "jnp.sum([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# first convert to jnp array and then sum\n",
    "print(jnp.sum(jnp.array([1,1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Out-of-bounds indexing\n",
    "For out-of-bound indexing `numpy` provides an error clearly explaining the error. In `jax` this is not so simple as raising an error from an accelerator (GPU/TPU) might be impossible or very difficult. Therefore, updates out-of-bounds will be skipped and the values will be filled with some value (in some cases the last value of the array but the behaviour is undefined) or some user-defined fill value using `.at[index].get(mode='fill', fill_value=fill_value')`. **Note** that when doing reverse mode automatic differentiation which turns index updates into index retrievals and vice versa will not preserve the semantics of out of bounds indexing. Therefore, out-of-bounds indexing should be thought of as undefined behaviour in `jax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.arange(10)[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Simple indexing: \", jnp.arange(10)[10])\n",
    "print(\"Using the ndarray at: \", jnp.arange(10).at[11].get())\n",
    "print(\"Using the ndarray at and fill values out-of-bounds with nan's: \",jnp.arange(10.0).at[11].get(mode='fill', fill_value=jnp.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Flax introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model parameters and initialization\n",
    "Parameters are not stored with the models themselves. You need to initialize parameters by calling the `init` function, using a PRNGKey and dummy input data. The parameter will have the shape of a nested dictionary whose leaves are `jax.numpy.array`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "model = nn.Dense(features = 4)\n",
    "key1, key2 = random.split(random.PRNGKey(1), 2)\n",
    "minval, maxval = 0,10\n",
    "# generate a randomly initialized input\n",
    "x = random.randint(key1, (4, 10), minval, maxval)\n",
    "# initialiaze the parameters\n",
    "params = model.init({'params' : key2}, x)\n",
    "# Checking output shapes\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) \n",
    "# apply the model on the input data\n",
    "y = model.apply(params, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Note the following about the above code block\n",
    "- the RNG has to be passed in the `model.init` function as the first argument if you are not explicitly using keyword and then the the input data\n",
    "- when applying the model on the input data additional RNGs as to be passed as a dictionary using the keyword `rngs = {'prng_name' : prng_key}` where the name is the same as you used when initializing the parameters\n",
    "- all the RNGs that is going to be used in the model has to be initialized in the `model.init`-function\n",
    "\n",
    "## Setting the parameters manually\n",
    "1. flatten the parameter dictionary\n",
    "2. edit the flattened parameters by setting new values to keys given by the flattened parameter dictionary\n",
    "3. unflatten the parameters\n",
    "4. freeze the parameters\n",
    "5. now the paremeters can be applied to the same model with different values.\n",
    "\n",
    "**Note** in the fully connected layers of the MLP the dimensions are flipped due to an implementation choice in `jax`, advice [here](https://flax.readthedocs.io/en/latest/advanced_topics/convert_pytorch_to_flax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from flax import traverse_util\n",
    "from flax.core import freeze\n",
    "# Get a flattened key-value list.\n",
    "flat_params = traverse_util.flatten_dict(params, sep='/')\n",
    "print(\"Flattened parameter tree: \\n\", jax.tree_util.tree_map(jnp.shape, flat_params) )\n",
    "# edit the parameters\n",
    "print('Before bias is assigned values : ', flat_params['params/bias'])\n",
    "flat_params['params/bias'] = jnp.ones(flat_params['params/bias'].shape)\n",
    "print('After bias is assigned values : ', flat_params['params/bias'])\n",
    "\n",
    "# Unflatten.\n",
    "unflat_params = traverse_util.unflatten_dict(flat_params, sep='/')\n",
    "# Refreeze.\n",
    "unflat_params = freeze(unflat_params)\n",
    "jax.tree_util.tree_map(jnp.shape, unflat_params)\n",
    "# apply to model\n",
    "y_new = model.apply(unflat_params, x)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# note that the new output is different from the old\n",
    "print(\"old - new output: \", y - y_new)\n",
    "\n",
    "# and that we still can apply the previous params to get the same result\n",
    "print(\"old - model applied on same old parameters : \", y - model.apply(params, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h4> Train a character-level GPT on some text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First, some imports!\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(182)\n",
    "\n",
    "\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 1. Attention is all we need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, we are going to be focusing specifically on the attention mechanism that is at the heart of its architecture. Attention is a critical component of many modern neural networks, and it plays a particularly important role in natural language processing tasks such as language modeling and machine translation. By completing this section of the homework, you will gain a deeper understanding of how attention works and how it can be used to improve the performance of language models. \n",
    "<br/>You will now implement the causal self attention for (min) GPT! You will implmement the code in the `model/model.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some tests here\n",
    "from tests.test_attention import TestAttention\n",
    "TestAttention().autograde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 2. Unpacking the MLP: Layer by Layer for Better Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The multi-layer perceptron (MLP) in GPT, also known as the feedforward network, is an essential component that helps to improve the model's ability to learn from sequential data. While the self-attention mechanism in GPT allows the model to attend to different parts of the input sequence, the MLP is responsible for processing and transforming the attended features before they are fed into the next layer. This additional non-linearity helps to capture more complex patterns and dependencies between the input tokens, leading to better performance on a wide range of language modeling tasks.\n",
    "<br/> In this section, you are going to implement the MLP for (min) GPT! You will implmement the code in the `model/model.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`.\n",
    "<br/> HINT: Read the documentation [here](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dense.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some tests here\n",
    "from tests.test_mlp import TestMLP\n",
    "TestMLP().autograde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 3. From Text to Numbers: Understanding Encoding in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The encoding section focuses on one of the most important aspects of natural language processing: converting textual data into numerical representations that can be understood and processed by machine learning models. In this section, we will explore the different types of encoding methods commonly used in NLP, including one-hot encoding, word embeddings, and more. By the end of this section, you should have a better understanding of how encoding works and why it is crucial for many language-based applications. You will implmement the code in the `encoding/bpe.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some tests here\n",
    "from tests.test_encoding import TestEncoder\n",
    "TestEncoder().autograde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 4. Crossing the Finish Line: Completing the MinGPT model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We're almost there! In this section, you will complete the final parts of the minGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tests import test_set_params\n",
    "# run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's run some unittests to see if your implementation is correct!\n",
    "from tests import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 5. Unleashing the power of minGPT: Training (and testing) the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will now train minGPT to be a character-level language model on some input text file. The input text file is an extract of Shakespearean text of about 1.1 MB. <br/> To use minGPT as a character level language model, we first preprocess the input text by converting it into a sequence of characters. We then train the model to predict the next character in the sequence given the preceding characters as input. Once the model is trained, we can use it to generate new text by feeding it a starting sequence (context) and iteratively sampling new characters from the model's predicted distribution until the desired length of text is generated. By using minGPT as a character level language model, we can generate new text that is similar in style and content to the input data, making it a useful tool for tasks such as text generation, language modeling, and autocompletion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('The input data has %d characters. %d of these characters are unique. These characters include uppercase and lower case letters, as well as punctuations.'\n",
    "        % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:ch for i,ch in enumerate(chars)} # will be used for prediction/text generation task\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_block = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        encoded_txt = [self.stoi[char] for char in text_block]\n",
    "        x = torch.tensor(encoded_txt[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(encoded_txt[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now load the input Shakespearean text file and look at the composition of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data has 1115393 characters. 65 of these characters are unique. These characters include uppercase and lower case letters, as well as punctuations.\n"
     ]
    }
   ],
   "source": [
    "# Let's load in the input data of Shakespearean text\n",
    "shakespeare_txt = open('./gpt_text_input/shakespeare.txt', 'r').read() \n",
    "dataset = TextDataset(shakespeare_txt, block_size = 64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Go through the `train_config.py` file in the `train` directory. It contains the parameters we will use to train the model. You can play aroud with these parameters after you have trained the model for the first time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initialize the model and the training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:46:40.674108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from train.trainer import create_train_state, Trainer\n",
    "from model import GPT\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, (0.9, 0.1))\n",
    "\n",
    "param_key, generation_key, dropout_key = random.split(random.PRNGKey(1), 3)\n",
    "\n",
    "init_rng = {\"params\": param_key, 'dropout' : dropout_key}\n",
    "\n",
    "model_config = {\n",
    "    \"n_layers\": 6,\n",
    "    \"n_head\": 6,\n",
    "    \"n_embd\": 32*6,\n",
    "    \"vocab_size\": dataset.vocab_size,\n",
    "    \"block_size\": dataset.block_size,\n",
    "    \"embd_pdrop\": 0.1\n",
    "}\n",
    "\n",
    "num_epochs = 6\n",
    "\n",
    "model = GPT(**model_config)\n",
    "\n",
    "state = create_train_state(model, init_rng, model_config, key=dropout_key)\n",
    "trainer = Trainer(train_dataset, test_dataset, train_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/15685 [00:52<15:08:44,  3.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/cs282_project_gpt_jax/train/trainer.py:120\u001b[0m, in \u001b[0;36mTrainer.run_trainer\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_trainer\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset:\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_epoch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/cs282_project_gpt_jax/train/trainer.py:140\u001b[0m, in \u001b[0;36mTrainer.run_epoch\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    138\u001b[0m     state \u001b[38;5;241m=\u001b[39m train_step(state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m    139\u001b[0m                        batch\u001b[38;5;241m=\u001b[39mbatch)  \u001b[38;5;66;03m# get updated train state (which contains the updated parameters)\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     state \u001b[38;5;241m=\u001b[39m compute_metrics(state\u001b[38;5;241m=\u001b[39mstate, batch\u001b[38;5;241m=\u001b[39mbatch)\n",
      "File \u001b[0;32m~/PycharmProjects/cs282_project_gpt_jax/venv/lib/python3.9/site-packages/flax/core/frozen_dict.py:162\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, keys, values)\u001b[0m\n\u001b[1;32m    157\u001b[0m   sorted_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict)\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    159\u001b[0m       [(jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mDictKey(k), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sorted_keys]\n\u001b[1;32m    160\u001b[0m   ), \u001b[38;5;28mtuple\u001b[39m(sorted_keys)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_unflatten\u001b[39m(\u001b[38;5;28mcls\u001b[39m, keys, values):\n\u001b[1;32m    164\u001b[0m   \u001b[38;5;66;03m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    165\u001b[0m   \u001b[38;5;66;03m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, values)}, __unsafe_skip_copy__\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    trainer.run_trainer(epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot the loss and accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loss = trainer.metrics_history[\"test_loss\"]\n",
    "plt.plot(np.arange(0, len(test_loss), 1), test_loss, label=\"loss\")\n",
    "test_accuracy = trainer.metrics_history[\"test_accuracy\"]\n",
    "plt.plot(np.arange(0, len(test_accuracy), 1), test_accuracy, label=\"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now use the trained model to generate new text. Try experimenting with different input texts and temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "I found a love, for me\n",
    "Darling, just dive right in and follow my lead\n",
    "Well, I found a girl, beautiful and sweet\n",
    "\"\"\"\n",
    "temperature = 0.5\n",
    "tokens_to_generate = 128\n",
    "\n",
    "# Ensure that the input is the correct length:\n",
    "text = text.rjust(model_config[\"block_size\"])\n",
    "text = text[:model_config[\"block_size\"]]\n",
    "\n",
    "# Tokenize the input:\n",
    "x = np.array([[dataset.stoi[t] for t in text]])\n",
    "\n",
    "sentence = \"\".join([dataset.itos[int(x_i)] for x_i in x[0]])\n",
    "\n",
    "print(f\"input sequence:\\n{sentence}\")\n",
    "x = jnp.array(x)\n",
    "\n",
    "state=trainer.train_state\n",
    "sequence= model.generate(state.params, x, tokens_to_generate, key1, temperature)\n",
    "\n",
    "sentence = \"\".join([dataset.itos[x_i] for x_i in sequence])\n",
    "print(f\"\\ngenerated sequence:\\n{sentence}\")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0b5e19b0de7feeb6e6bb5f9738d975aa3f5dabb2cb545fec106b49f43b6978a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
