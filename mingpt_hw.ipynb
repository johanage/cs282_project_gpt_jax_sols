{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thfFhBzfWZsm"
      },
      "outputs": [],
      "source": [
        "#@title Mount your Google Drive\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrM-U3uvWZsq"
      },
      "outputs": [],
      "source": [
        "#@title Set up mount symlink\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs182-minGPT_sp23'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs182-minGPT'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6vePJj8WZsu"
      },
      "outputs": [],
      "source": [
        "#@title Clone homework repo\n",
        "\n",
        "%cd $SYM_PATH\n",
        "if not os.path.exists(\"cs182-minGPT\"):\n",
        "  !git clone https://github.com/avikam-chauhan/cs282_project_gpt_jax-homework\n",
        "%cd cs182-minGPT\n",
        "%cd cs282_project_gpt_jax-homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yfJhU32WZsw",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Introduction to Coding in jax and flax\n",
        "In this homework, we will be exploring the world of [Flax](http://flax.readthedocs.io/) and [Jax](https://github.com/google/jax), two powerful deep learning libraries made by Google that can be used to reimplement the PyTorch version of minGPT. Jax and Flax are both built on the concept of functional programming and provide a high-level interface for building and training neural networks, which can simplify the development process and reduce the amount of boilerplate code that needs to be written. Flax and Jax are designed to take advantage of modern hardware architectures such as GPUs and TPUs, which can significantly speed up the training process and reduce the time-to-deployment for new models.\n",
        "<br/> By using these libraries, you will gain a deeper understanding of the underlying principles of neural networks and develop your skills in functional programming. You will also have the opportunity to compare and contrast the PyTorch and Flax/Jax versions of minGPT, gaining valuable insights into the similarities and differences between these powerful tools. So let's roll up our sleeves and dive into the world of Flax and Jax!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOIhYaxWZs0"
      },
      "source": [
        "\n",
        "Before we start implementing the GPT architecture the libraries `jax` [(Jax Docs)](https://jax.readthedocs.io/en/latest/index.html) and `flax` [(Flax Docs)](https://flax.readthedocs.io/en/latest/) should be introduced as they have some key differences from pytorch functionalities. These differences are explained in detail [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8KRfBjCWZs2"
      },
      "source": [
        "#### Before you start coding, make sure you turn on GPU runtime. You can do this by going to Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "03nUVVo9WZs5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Jax introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-WZjaaWsWZs6",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "First let us start with `jax`. In practice `jax` works almost exactly as `numpy` but use own datatypes which is more convenient in the context of high-performance numerical computing (HPC). Experience with such libraries is crucial as modern ML applications require resources that is beoynd the capabilities of a personal computer. Furthermore, `jax` supports automatic differentiation in both forward- and reverse-mode as well as accelerated linear algebra ([XLA](https://www.tensorflow.org/xla) - domain-specific compiler) .\n",
        "\n",
        "### Random Number Generation\n",
        "\n",
        "The first difference between `jax` and `numpy` we are going to highlight is ***how you generate random numbers***. In the code block below we demonstrate how you generate a random vector $x, x\\in\\mathbb{R}^{10}, x_i \\sim \\mathcal{N}(0,1)$. You can read more about the PRNG design [here](https://github.com/google/jax/blob/main/docs/jep/263-prng.md). A major advantage of this design is **reproducible program execution in a backend-independent way**. This is of major imortance for validating new discoveries within ML research as a many components (weight and bias initialization, dropout, masking, etc.) depend on PRNGs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftydX3ubWZs8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from jax import random\n",
        "size = (10,)\n",
        "key = random.PRNGKey(1)\n",
        "# generate random vector with std normal distributed elements\n",
        "x = random.normal(key, size)\n",
        "x\n",
        "# alternatively use a mu sigma^2 normal distributed vector\n",
        "# x = mu + sigma2*random.normal(key,size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5-DeXMmeWZs_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "To prevent generating random numbers from the same state represented by the variable `key`, the PRNG is split to get usable subkeys every time a new pseudorandom number is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTvLUtJ4WZtA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# copied from hax common gotchas\n",
        "print(\"old key\", key)\n",
        "key, subkey = random.split(key)\n",
        "normal_pseudorandom = random.normal(subkey, shape=(1,))\n",
        "print(\"    \\---SPLIT --> new key   \", key)\n",
        "print(\"             \\--> new subkey\", subkey, \"--> normal\", normal_pseudorandom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GQWDFJyfWZtD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Pure functions\n",
        "\n",
        "`jax` is a language of expressing and composing transformations of numerical programs and compile these programs for both CPU and accelerators (GPU/TPU). Compiling across devices requires that the code is written with certain constraints. One of the constraings is that the transformations and compilations are designed to only work on ***pure functions*** [pure functions link](https://en.wikipedia.org/wiki/Pure_function). This means that all the input data is passed through the function parameters, all the results are output throught hte functions results. Consequently, a pure function will always give the same output for the same input. In the below code block we have provided some examples of impure functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RpNc3-lWZtE",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from jax import jit\n",
        "import jax.numpy as jnp\n",
        "# using global\n",
        "global_var = 1\n",
        "def impure_input_is_output(x):\n",
        "    return x + global_var\n",
        "# uses initial assignment of the global variable\n",
        "print(jit(impure_input_is_output)(1))\n",
        "global_var = 10.\n",
        "# subsequent runs may cached value of the previous print\n",
        "print(jit(impure_input_is_output)(2))\n",
        "# when the type is changed the las update of the global variable is used\n",
        "print(jit(impure_input_is_output)(jnp.array([1]) ) )\n",
        "\n",
        "############\n",
        "# Add more #\n",
        "# examples #\n",
        "############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ORDxxsE0WZtG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### In-place updates\n",
        "\n",
        "Another important difference to `numpy` is how in-place update are done. `jax` offers a functional array update using the `.at` property. This difference is examplified in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rypB84IEWZtG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "#### Numpy #####\n",
        "np_arr = np.zeros((3,3), dtype=np.float64)\n",
        "print(np_arr)\n",
        "# in place, mutating update\n",
        "np_arr[1,:] = 1\n",
        "print(np_arr)\n",
        "\n",
        "#### Jax #####\n",
        "# not that float 64 is not available, try by running the\n",
        "# commented-out line below\n",
        "#jnp_arr = jnp.zeros((3,3), dtype = jnp.float64)\n",
        "jnp_arr = jnp.zeros((3,3), dtype = jnp.float32)\n",
        "jnp_arr[1,:] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHJ89du_WZtH",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#### correct jax in-place update ####\n",
        "jnp_arr.at[1,:].add(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eYn46IZsWZtI",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Permissive inputs\n",
        "\n",
        "`jax` requires premissive inputs. While `numpy` accepts lists or tuples as inputs, `jax` do not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzj4uZ6cWZtJ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(np.sum([1,1,1]))\n",
        "jnp.sum([1,1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHJKjTvcWZtJ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# first convert to jnp array and then sum\n",
        "print(jnp.sum(jnp.array([1,1,1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MP_cckSvWZtK",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Out-of-bounds indexing\n",
        "For out-of-bound indexing `numpy` provides an error clearly explaining the error. In `jax` this is not so simple as raising an error from an accelerator (GPU/TPU) might be impossible or very difficult. Therefore, updates out-of-bounds will be skipped and the values will be filled with some value (in some cases the last value of the array but the behaviour is undefined) or some user-defined fill value using `.at[index].get(mode='fill', fill_value=fill_value')`. **Note** that when doing reverse mode automatic differentiation which turns index updates into index retrievals and vice versa will not preserve the semantics of out of bounds indexing. Therefore, out-of-bounds indexing should be thought of as undefined behaviour in `jax`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0oYD9HHWZtK",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "np.arange(10)[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f27lT5R4WZtK",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Simple indexing: \", jnp.arange(10)[10])\n",
        "print(\"Using the ndarray at: \", jnp.arange(10).at[11].get())\n",
        "print(\"Using the ndarray at and fill values out-of-bounds with nan's: \",jnp.arange(10.0).at[11].get(mode='fill', fill_value=jnp.nan))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PIwyu1N-WZtL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Flax introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "a5qNMpleWZtL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Model parameters and initialization\n",
        "Parameters are not stored with the models themselves. You need to initialize parameters by calling the `init` function, using a PRNGKey and dummy input data. The parameter will have the shape of a nested dictionary whose leaves are `jax.numpy.array`'s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnXrgLw9WZtM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "import jax\n",
        "model = nn.Dense(features = 4)\n",
        "key1, key2 = random.split(random.PRNGKey(1), 2)\n",
        "minval, maxval = 0,10\n",
        "# generate a randomly initialized input\n",
        "x = random.randint(key1, (4, 10), minval, maxval)\n",
        "# initialiaze the parameters\n",
        "params = model.init({'params' : key2}, x)\n",
        "# Checking output shapes\n",
        "jax.tree_util.tree_map(lambda x: x.shape, params) \n",
        "# apply the model on the input data\n",
        "y = model.apply(params, x)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "h8p1J2A2WZtM",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Note the following about the above code block\n",
        "- the RNG has to be passed in the `model.init` function as the first argument if you are not explicitly using keyword and then the the input data\n",
        "- when applying the model on the input data additional RNGs as to be passed as a dictionary using the keyword `rngs = {'prng_name' : prng_key}` where the name is the same as you used when initializing the parameters\n",
        "- all the RNGs that is going to be used in the model has to be initialized in the `model.init`-function\n",
        "\n",
        "### Setting the parameters manually\n",
        "1. flatten the parameter dictionary\n",
        "2. edit the flattened parameters by setting new values to keys given by the flattened parameter dictionary\n",
        "3. unflatten the parameters\n",
        "4. freeze the parameters\n",
        "5. now the paremeters can be applied to the same model with different values.\n",
        "\n",
        "**Note** in the fully connected layers of the MLP the dimensions are flipped due to an implementation choice in `jax`, advice [here](https://flax.readthedocs.io/en/latest/advanced_topics/convert_pytorch_to_flax.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoZfSRduWZtM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from flax import traverse_util\n",
        "from flax.core import freeze\n",
        "# Get a flattened key-value list.\n",
        "flat_params = traverse_util.flatten_dict(params, sep='/')\n",
        "print(\"Flattened parameter tree: \\n\", jax.tree_util.tree_map(jnp.shape, flat_params) )\n",
        "# edit the parameters\n",
        "print('Before bias is assigned values : ', flat_params['params/bias'])\n",
        "flat_params['params/bias'] = jnp.ones(flat_params['params/bias'].shape)\n",
        "print('After bias is assigned values : ', flat_params['params/bias'])\n",
        "\n",
        "# Unflatten.\n",
        "unflat_params = traverse_util.unflatten_dict(flat_params, sep='/')\n",
        "# Refreeze.\n",
        "unflat_params = freeze(unflat_params)\n",
        "jax.tree_util.tree_map(jnp.shape, unflat_params)\n",
        "# apply to model\n",
        "y_new = model.apply(unflat_params, x)\n",
        "y_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exYLTsMkWZtN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# note that the new output is different from the old\n",
        "print(\"old - new output: \", y - y_new)\n",
        "\n",
        "# and that we still can apply the previous params to get the same result\n",
        "print(\"old - model applied on same old parameters : \", y - model.apply(params, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBDnJ183WZtN",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Train a character-level GPT on some text data\n",
        "Now that we have introduced `flax` and `jax`, let's move to the main part of this homework, using minGPT on some text data for autocompletion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu-WDjZPWZtO",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# First, some imports!\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "np.random.seed(182)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "t8mfd5gCWZtO",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 1. Attention is all we need!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Z77Nb9ceWZtO",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this section, we are going to be focusing specifically on the attention mechanism that is at the heart of its architecture. Attention is a critical component of many modern neural networks, and it plays a particularly important role in natural language processing tasks such as language modeling and machine translation. By completing this section of the homework, you will gain a deeper understanding of how attention works and how it can be used to improve the performance of language models.\n",
        "\n",
        "<br/>You will now implement the causal self attention for (min) GPT! You will implmement the code in the `model.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`.\n",
        "\n",
        "<br/>Below is a block diagram of the causal self-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://drive.google.com/file/d/1pNSBHCVGBtbdL5IVwBkMPMnTo_Pmz9vH/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import model\n",
        "\n",
        "# some tests here\n",
        "from tests import test_set_params_csa\n",
        "%run -i 'tests/test_set_params_csa.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EBEmsobnWZtP",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 2. Unpacking the MLP: Layer by Layer for Better Language Modeling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dU-SqUp9WZtP",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The multi-layer perceptron (MLP) in GPT, also known as the feedforward network, is an essential component that helps to improve the model's ability to learn from sequential data. While the self-attention mechanism in GPT allows the model to attend to different parts of the input sequence, the MLP is responsible for processing and transforming the attended features before they are fed into the next layer. This additional non-linearity helps to capture more complex patterns and dependencies between the input tokens, leading to better performance on a wide range of language modeling tasks.\n",
        "\n",
        "<br/> In this section, you are going to implement the MLP for (min) GPT! You will implmement the code in the `model.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`.\n",
        "<!--<br/> HINT: Read the documentation [here](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dense.html)-->\n",
        "\n",
        "<br/> Below is a block diagram of the MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://drive.google.com/file/d/1257WXCmvAF2O2-TSAXeiqKlDcW8w4ce8/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPL17W99WZtQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# some tests here\n",
        "from tests import test_set_params_mlp\n",
        "%run -i 'tests/test_mlp_params.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SUuKdcfrWZtR",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 3. From Text to Numbers: Understanding Encoding in Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nH3YRcCEWZtR",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The encoding section focuses on one of the most important aspects of natural language processing: converting textual data into numerical representations that can be understood and processed by machine learning models. In this section, we will explore the different types of encoding methods commonly used in NLP, including one-hot encoding, word embeddings, and more. By the end of this section, you should have a better understanding of how encoding works and why it is crucial for many language-based applications. You will implmement the code in the `encoding/bpe.py` file. Read the instructions in the docstring and then fill in the code in the places that says `#YOUR CODE HERE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CQ-b1-WWZtR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# some tests here\n",
        "from tests.test_encoding import TestEncoder\n",
        "TestEncoder().autograde()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_lEKsl6-WZtR",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 4. Crossing the Finish Line: Completing the MinGPT model</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "n2Y1Lk3YWZtS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We're almost there! In this section, you will complete the final parts of the minGPT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK3DjXz7WZtS"
      },
      "source": [
        "### 4.1 Transformer Block\n",
        "Complete the Transformer Block class in the `model.py` file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2snAQ9wWZtS"
      },
      "source": [
        "### 4.2 GPT\n",
        "Complete the GPT class in the `model.py` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGP7ZEYWZtT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's run some tests to see if your implementation is correct!\n",
        "from tests import test_set_params_all_blocks\n",
        "%run -i 'tests/test_set_params_all_blocks.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lGaGsIkfWZtT",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 5. Unleashing the power of minGPT: Training (and testing) the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XvT8XU-0WZtU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We will now train minGPT to be a character-level language model on some input text file. The input text file is an extract of text from German philosopher, Friedrich Wilhelm Nietzsche. <br/> To use minGPT as a character level language model, we first preprocess the input text by converting it into a sequence of characters. We then train the model to predict the next character in the sequence given the preceding characters as input. Once the model is trained, we can use it to generate new text by feeding it a starting sequence (context) and iteratively sampling new characters from the model's predicted distribution until the desired length of text is generated. By using minGPT as a character level language model, we can generate new text that is similar in style and content to the input data, making it a useful tool for tasks such as text generation, language modeling, and autocompletion tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd4Ya25bWZtU",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('The input data has %d characters. %d of these characters are unique. These characters include uppercase and lower case letters, as well as punctuations.'\n",
        "        % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)} # will be used for prediction/text generation task\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_block = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        encoded_txt = [self.stoi[char] for char in text_block]\n",
        "        x = torch.tensor(encoded_txt[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(encoded_txt[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data) - self.block_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uLRQj1JQWZtU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's now load the input Nietzsche text file and look at the composition of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3LWVMr7WZtU",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's load in the input data of Nietzsche text\n",
        "nietzsche_txt = open('./gpt_text_input/nietzsche.txt', 'r').read() \n",
        "dataset = TextDataset(nietzsche_txt, block_size = 64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XqBZEmCmWZtV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Go through the `train_config.py` file in the `train` directory. It contains the parameters we will use to train the model. You can play aroud with these parameters after you have trained the model for the first time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yqPyt_3ZWZtV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Initialize the model and the training instance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model will likely take between 20 and 30 minutes (on GPU), so go ahead and work on the written questions while this is running!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycsgp34-WZtW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from train.trainer import create_train_state, Trainer\n",
        "from model import GPT\n",
        "from jax import random\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, (0.9, 0.1))\n",
        "\n",
        "param_key, generation_key, dropout_key = random.split(random.PRNGKey(1), 3)\n",
        "\n",
        "init_rng = {\"params\": param_key, 'dropout' : dropout_key}\n",
        "\n",
        "model_config = {\n",
        "    \"n_layers\": 6,\n",
        "    \"n_head\": 6,\n",
        "    \"n_embd\": 32*6,\n",
        "    \"vocab_size\": dataset.vocab_size,\n",
        "    \"block_size\": dataset.block_size,\n",
        "    \"embd_pdrop\": 0.1\n",
        "}\n",
        "\n",
        "num_epochs = 4\n",
        "\n",
        "model = GPT(**model_config)\n",
        "\n",
        "state = create_train_state(model, init_rng, model_config, key=dropout_key)\n",
        "trainer = Trainer(train_dataset, test_dataset, train_state=state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "knO6LyF2WZtX",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_p6PJSgWZtX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for i in range(num_epochs):\n",
        "    trainer.run_trainer(epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kuZT0s51WZtX",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Plot the loss and accuracy on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyWtoKm-WZtY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_loss = trainer.metrics_history[\"test_loss\"]\n",
        "plt.plot(np.arange(0, len(test_loss), 1), test_loss, label=\"loss\")\n",
        "test_accuracy = trainer.metrics_history[\"test_accuracy\"]\n",
        "plt.plot(np.arange(0, len(test_accuracy), 1), test_accuracy, label=\"accuracy\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lradzA5denCV"
      },
      "source": [
        "#### Using the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKzDZyAXeop2"
      },
      "source": [
        "In this section we are going to explore how we can use the trained model. We will first start by defining some constants used in our predictions. The most important variables are **text**, which is our input, and **temperature**, which affects the randomness of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR7aN9fje5qo"
      },
      "outputs": [],
      "source": [
        "state = trainer.train_state\n",
        "\n",
        "model = GPT(**model_config)\n",
        "\n",
        "text = \"What is the meaning of life the universe and everything? It is \"\n",
        "# Ensure that the input is the correct length:\n",
        "text = text.rjust(model_config[\"block_size\"])\n",
        "text = text[:model_config[\"block_size\"]]\n",
        "\n",
        "temperature=0.4\n",
        "max_generated_tokens=128\n",
        "\n",
        "tokenized_input = np.array([[dataset.stoi[t] for t in text]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkWCLOErZkM1"
      },
      "source": [
        "Let's see explore which characters the model thinks are likely after this text input, and how different temperatures affect the probablities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsnhuFoeZiy3"
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "# Predict the next token. We are only interested in the last token in the sequence\n",
        "\n",
        "predictions = model.apply(state.params, tokenized_input, training=False)[0,-1]\n",
        "\n",
        "predictions_low_temperature = predictions/0.5\n",
        "predictions_medium_temperature = predictions\n",
        "predictions_high_temperature = predictions/2\n",
        "\n",
        "probabilities_low = nn.softmax(predictions_low_temperature)\n",
        "probabilities_medium = nn.softmax(predictions_medium_temperature)\n",
        "probabilities_high = nn.softmax(predictions_high_temperature)\n",
        "\n",
        "\n",
        "\n",
        "indexes = jnp.argsort(-predictions)\n",
        "\n",
        "labels = [dataset.itos[int(index)]for index in indexes] \n",
        "\n",
        "probabilities_low_sorted =  [float(probabilities_low[int(index)]) for index in indexes]\n",
        "probabilities_medium_sorted =  [float(probabilities_medium[int(index)]) for index in indexes]\n",
        "probabilities_high_sorted =  [float(probabilities_high[int(index)]) for index in indexes]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "\n",
        "axes[0].bar(labels[:20], probabilities_low_sorted[:20])\n",
        "axes[1].bar(labels[:20], probabilities_medium_sorted[:20])\n",
        "axes[2].bar(labels[:20], probabilities_high_sorted[:20])\n",
        "\n",
        "axes[0].set_title(\"temperature = 0.5\")\n",
        "axes[1].set_title(\"temperature = 1.0\")\n",
        "axes[2].set_title(\"temperature = 2.0\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn-nKj2ieUDZ"
      },
      "source": [
        "Now we will make the model predict one full word. Note that the random key is constant, so it will always generates the same word for the same input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd2a5f2_eGZJ"
      },
      "outputs": [],
      "source": [
        "# We want the model to stop generating after it reaches one of these characters\n",
        "stop_tokens = [dataset.stoi[\" \"], dataset.stoi[\"\\n\"],  dataset.stoi[\".\"],  dataset.stoi[\",\"]]\n",
        "print(\"Generated sequence: \")\n",
        "model.verbose_generate(state.params, tokenized_input, max_generated_tokens, key1, dataset.itos, temperature, stop_tokens=stop_tokens)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OLxq_eukWZtY",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Finally we will use the trained model to generate longer sequences. Try experimenting with different input texts and temperatures!\n",
        "\n",
        "Note that the model is only trained for a short while on a relatively small dataset, so the performance will not match other large language models you may be familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8MKEVzGWZtY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "state=trainer.train_state\n",
        "print(\"Generated sequence: \")\n",
        "model.verbose_generate(state.params, tokenized_input, max_generated_tokens, key1, dataset.itos, temperature)\n",
        "print() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijidvORrWZtZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0b5e19b0de7feeb6e6bb5f9738d975aa3f5dabb2cb545fec106b49f43b6978a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
